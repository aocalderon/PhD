\documentclass{article}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{hyperref}
\hypersetup{
colorlinks=false,
hidelinks
}


\title{Milestone 1}
\author{Andres Calderon \\ acald013@ucr.edu}

\begin{document}
\maketitle

\section{Introduction}
This report describes the initial steps in order to accomplish the final project in the course. The main goal of the project is to perform a reliability analysis of a machine learning algorithm (kNN).  During this first milestone it is intedend to provide a flexible implementation of the kNN algorithm, a mechanism to inject random errors in the calculation of the distance metric and a brief analisys of the impact of an unreliable distance calculation.

\section{A flexible implementation of kNN}
There are many open source implementations of the kNN algorithm under different programming languages.  This report uses the R Project for Statistical Computing\footnote{https://www.r-project.org/} platform using especifically the knnflex\footnote{\url{http://ftp.uni-bayreuth.de/math/statlib/R/CRAN/src/contrib/Descriptions/knnflex.html}} package. The knnflex package allows a more flexible implementation of the distance metric as well the oportunity to code custom functions for aggregations and tie handlers.  In addition, it uses the caret package to compute the confusion matrix and associated statistics for the model fit.

\subsection{A quick classification example}
The code in figure \ref{fig:code} illustrates the use of knnflex to classify a small random set of features.  In lines 5 to 10 it sets the number of instances and a random seed, create two attributes with random numbers (x1 and x2) and a binary class (y).  Lines 11 and 12 divede the data set in training and testing set (75\% and 25\% respectively). Line 17 call the kdd.dist function which will generate a distance matrix among all the instances in the data set. Line 18 perform the classification calling the knn.predict function.  It takes the training and testing datasets, the distance matrix, the number of neighbors to be taken into account and the aggregation method to pick the class between them.  

Finally, line 22 calls the confusionMatrix function to retrieve the accuracy and other statistics from the model (figure \ref{fig:cm}). Lines 14 and 20 plots the initial instances in the training set and the result of the classification for the instances in the testing set.  Figures show the results respectively.

\begin{figure}
 \centering
 \inputminted[
  fontsize=\footnotesize,
  tabsize=2,
  breaklines,
  xleftmargin=15mm,
  frame=single,
  linenos
  ]{r}{example.R}
 \caption{A quick code example}\label{fig:code}
\end{figure}

<<example1, echo=FALSE, message=FALSE>>=
require(knnflex)
require(caret)

n <- 200
set.seed(123)
x1 <- c(rnorm(n/2,mean=2.5),rnorm(n/2,mean=7.5))
x2 <- c(rnorm(n/2,mean=7.5),rnorm(n/2,mean=2.5))
x  <- cbind(x1,x2)
y <- c(rep(1,n/2),rep(0,n/2))
train <- sample(1:n,n*0.75)
test <- (1:n)[-train]
kdist <- knn.dist(x)
preds <- knn.predict(train,test,y,kdist)
@

\begin{figure}
 \centering
<<example2>>=
confusionMatrix(y[test],preds)
@
 \caption{Confusion matrix}\label{fig:cm}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{ex1}
  \caption{Instances in training set.}\label{fig:train}
\end{figure}

\begin{figure}
  \centering
  \includegraphics[width=0.6\textwidth]{ex2}
  \caption{Results for instances in testing set.}\label{fig:test}
\end{figure}

\end{document}