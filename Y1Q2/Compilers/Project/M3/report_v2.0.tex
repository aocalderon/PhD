\documentclass{article}

\usepackage{alltt}
\usepackage{graphicx}
\usepackage{minted}
\usepackage{xcolor}
\usepackage{placeins}
\usepackage[margin=3cm]{geometry}
\usepackage{booktabs}
\usepackage{subcaption}
\usepackage{amsmath}
\usepackage{hyperref}
\hypersetup{
colorlinks=false,
hidelinks
}
\setlength{\parindent}{2em}
\setlength{\parskip}{0.5em}

\title{Milestone 3}
\author{Andres Calderon \\ acald013@ucr.edu}
\begin{document}
\maketitle

\section{Formal reasoning.}
Table \ref{tab:desc} summarizes some of the attributes of the studied datasets. $dr$ refers to the default rate of the dataset and it is the number of elements of the most frequent value of the class.  It is taken as a reference for classifiers because any resulting accuracy should be better than the default rate, if not, it would be practically better to guess. $N$ refers to the number of available points for training the classifier. $C$ refers to the number of different classes in the dataset and $D$ refers to the number of dimensions or attributes. 

\begin{table}[h]
 \centering
 \begin{tabular}{rcccc}
  \toprule
	& \textbf{dr}	& \textbf{N}	& \textbf{C}	& \textbf{D} 	\\
  \midrule
Cancer	& 0.63	& 398	& 2	& 32 	\\
Iris	& 0.33	& 105	& 3	& 4 	\\
Seeds	& 0.33	& 147	& 3	& 7 	\\
Wine	& 0.40	& 125	& 3	& 13 	\\
Zoo	& 0.41	& 71	& 7	& 17 	\\
  \bottomrule
 \end{tabular}
 \caption{Description of the datasets.}\label{tab:desc}
\end{table}

Figure \ref{fig:a} shows the difference in the accuracy results of kNN when the probability of error in the distance computation increases. Figure \ref{fig:b} show the error rate of the same implementation by just subtracting the value of the original accuracy (without error injection) and the values obtained in figure \ref{fig:a}.  We can see that the increasing rate of error described by figure \ref{fig:b} can be modeled by an exponential function using the equation \ref{eq:1}. $\alpha$ will determine the highest point of the function and  $\beta$ will describe the slope and shape of the curve.

\begin{equation}\label{eq:1}
  Err(p) = \alpha \times e^{\beta \times p}
\end{equation}

\begin{figure}
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figures/accuracies.pdf}
\caption{Accuracies results with increasing error in distance computation.} \label{fig:a}
\end{subfigure}
\hspace*{\fill} % separation between the subfigures
\begin{subfigure}{0.48\textwidth}
\includegraphics[width=\linewidth]{figures/errors.pdf}
\caption{Error rate plot derived from figure \ref{fig:a}} \label{fig:b}
\end{subfigure}
\caption{Accuracy comparison and error rate for Iris dataset.} \label{fig:accerr}
\end{figure}


In our case, $\alpha$ represents the maximum error rate of the function and it relates to the default rate ($dr$) of the classifier.  Even if we introduce a lot of errors, the chance to pick up a correct answer is not below to the default rate.  So, we can model $\alpha$ as: 

\begin{equation}\label{eq:2}
  \alpha = (1-dr)
\end{equation}

As we already mention, $\beta$ describes the shape of the curve and how fast increase the error rate together with the value of $p$. In the way that kNN works, the classification of each new point will depend on the number of available points ($N$) in the search space.  Similarly, the probability to belong to a specific class ($C$) (and specifically to the correct one) depends on how many members of that class are close enough to the new point. For simplicity, we will consider that the number of available points $N$ will distribute equally on the number of classes $C$, so the number of available points for each class will be $\frac{N}{C}$.  Now, we will have to consider that this number is also affected by the default rate ($dr$).  At the moment when $\frac{N}{C}$ would be below of $dr$, the classifier will start guessing. So, if many points from the correct class are affected for the error injection, the probabilities to misclassified the new point will increase.  From this reasoning we can model $\beta$ as: 

\begin{equation}\label{eq:3}
  \beta = \frac{N}{C} \times (1-dr)
\end{equation}

Equation \ref{eq:4} illustrates the final result after replacing equations \ref{eq:2} and \ref{eq:3} in equation \ref{eq:1}.

\begin{equation}\label{eq:4}
  Err(p)=(1-dr) \times e^{\frac{N}{C} \times (1-dr) \times p}
\end{equation}


\section{Testing.}

We extend table \ref{tab:desc} by applying equations \ref{eq:2} and \ref{eq:3} to add columns $\alpha$ and $\beta$.  Figures from \ref{fig:cancer} to \ref{fig:zoo} show the corresponding fitting model by applying equation \ref{eq:4} respectively.

\begin{table}[h]
 \centering
 \begin{tabular}{rcccccc}
  \toprule
	& dr	& N	& C	& D 	& \boldmath$\alpha$	& \boldmath$\beta$	\\
  \midrule
Cancer	& 0.63	& 398	& 2	& 32 	& \textbf{0.37}	& \textbf{74.20}	\\
Iris	& 0.33	& 105	& 3	& 4 	& \textbf{0.67}	& \textbf{23.35}	\\
Seeds	& 0.33	& 147	& 3	& 7 	& \textbf{0.67}	& \textbf{32.68}	\\
Wine	& 0.40	& 125	& 3	& 13 	& \textbf{0.60}	& \textbf{24.97}	\\
Zoo	& 0.41	& 71	& 7	& 17 	& \textbf{0.59}	& \textbf{6.00}	\\







  \bottomrule
 \end{tabular}
 \caption{$\alpha$ and $\beta$ from values on table \ref{tab:desc}.}\label{tab:extend}
\end{table}

\begin{figure}
 \centering
 \includegraphics[width=0.9\textwidth]{./figures/cancer.pdf}
 \caption{Observed error rate compared to the fitted model for Cancer.}
 \label{fig:cancer}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.9\textwidth]{./figures/iris.pdf}
 \caption{Observed error rate compared to the fitted model for Iris.}
 \label{fig:iris}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.9\textwidth]{./figures/seeds.pdf}
 \caption{Observed error rate compared to the fitted model for Seeds.}
 \label{fig:seeds}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.9\textwidth]{./figures/wine.pdf}
 \caption{Observed error rate compared to the fitted model for Wine.}
 \label{fig:wine}
\end{figure}

\begin{figure}
 \centering
 \includegraphics[width=0.9\textwidth]{./figures/zoo.pdf}
 \caption{Observed error rate compared to the fitted model for Zoo.}
 \label{fig:zoo}
\end{figure}

\end{document}
