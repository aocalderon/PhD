\documentclass{article}

\usepackage{amsmath,amsfonts}
\usepackage{fullpage}
\usepackage{enumitem}

\usepackage{tikz}
\newcommand*\circled[1]{\tikz[baseline=(char.base)]{
            \node[shape=circle,draw,inner sep=2pt] (char) {#1};}}

\usepackage{minted}
\usepackage{xcolor}
\usepackage{booktabs}
\usepackage{amssymb}

\pagestyle{empty}

\renewcommand{\vec}[1]{\mathbf{#1}}

\title{Homework 2\\CS 210}
\author{Andres Calderon - SID 861243796}
\date{\today}
\begin{document}

\maketitle

\begin{center}
\begin{tabular}{|l|l|p{.4in}|}
\hline Question & Points & Score \\
\hline 1 & 10 & \\
\hline 2 & 10 & \\
\hline 3 & 10 & \\
\hline 4 & 10 & \\
\hline 5 & 10 & \\
\hline 6 & 10 & \\
\hline 7 & 10 & \\
\hline 8 & 10 & \\
\hline 9 & 10 & \\
\hline 10 & 10 & \\
\hline Total & 100 & \\
\hline 
\end{tabular}
\end{center}

\subsection*{Matrix algebra}
\begin{enumerate}
\setcounter{enumi}{0}
\item (Trefethen\&Bau 2.6) If $\vec{u}$ and $\vec{v}$ are $m-$vectors, the matrix $A = I+\vec{u}\vec{v}^T$ is known as a rank-one
  pertubation of the identity.  Show that if $A$ is nonsingular, then its inverse has the form $A^{-1} = I + \alpha
  \vec{u} \vec{v}^T$ for some scalar $\alpha$, and give an expression for $\alpha$.  For what $\vec{u}$ and $\vec{v}$ is
  $A$ singular?  If it is singular, what is null($A$)?\\
\\
Answer:\\
\\
\begin{tabular}{rl}
$I$ & $= AA^{-1}$ \\
& $= (I+\vec{u}\vec{v}^T)(I+\alpha\vec{u}\vec{v}^T)$ \\
& $= I+\alpha\vec{u}\vec{v}^T+\vec{u}\vec{v}^T+\alpha(\vec{u}\vec{v}^T)(\vec{u}\vec{v}^T)$\\
& $= I+\alpha\vec{u}\vec{v}^T+\vec{u}\vec{v}^T+\alpha\vec{u}(\vec{v}^T\vec{u})\vec{v}^T$\\
\end{tabular}

As $\vec{v}^T\vec{u}$ is a scalar, we can:

\begin{tabular}{rl}
$I$ & $= I+\alpha\vec{u}\vec{v}^T+\vec{u}\vec{v}^T+\alpha(\vec{v}^T\vec{u})(\vec{u}\vec{v}^T)$\\
& $= I+(\alpha+1+\alpha(\vec{v}^T\vec{u}))\vec{u}\vec{v}^T$\\
\end{tabular}

Since $\vec{u}\vec{v}^T\neq0$ (as A is supposed to be non-singular and its inverse must exist) an expression for $\alpha$ is:
$$\alpha+1+\alpha(\vec{v}^T\vec{u})=0$$
$$\alpha(1+\vec{v}^T\vec{u})=-1$$
$$\alpha=\frac{-1}{1+\vec{v}^T\vec{u}}$$

We can see that for any $\vec{u}$ and $\vec{v}$ where:
$$\vec{v}^T\vec{u}=-1$$
the previous expression cannot be defined and, in those cases, $A$ is singular.

To find the \textbf{null space} of A we have:
$$A\vec{x}=0$$
From the statement we have:
$$(I+\vec{u}\vec{v}^T)\vec{x}=0$$
If $\vec{x}=\vec{u}$, then:
$$(I+\vec{u}\vec{v}^T)\vec{u}=0$$
$$\vec{u}+\vec{u}(\vec{v}^T\vec{u})=0$$
If A is singular, we know $\vec{v}^T\vec{u}=-1$, so:
$$\vec{u}-\vec{u}=0$$
holds.  Therefore, $null(A)=span \{ \vec{u} \}$.

\item (Heath 2.8) Let $A$ and $B$ be any two $n \times n$ matrices.
\begin{enumerate}
\item Prove that $(AB)^T=B^TA^T$.\\
\\
Answer:\\
\\
Let denote the $i,j$ entry of $(AB)^T$ as $(AB)_{i,j}^T$.  Note that it is the same as the $j,i$ entry of $AB$. Note also that the $j,i$ entry of $AB$ is equal to the row $j$ of $A$ $dot$ the column $i$ of $B$.\\
Similarly, the $i,j$ entry of $B^TA^T$ is equal to the row $i$ of $B^T$ $dot$ the column $j$ of $A^T$, which is the same to say the column $i$ of $B$ $dot$ the row $j$ of $A$.\\
So, we can see that the $i,j$ entries of each side are the $dot$ product of the same two vectors.  Hence $(AB)^T=B^TA^T$.

\item If $A$ and $B$ are both non-singular, prove that $(AB)^{-1} = B^{-1}A^{-1}$.\\
\\
Answer:\\
\\
First, we have to assume that $A$ and $B$ are invertible.  So, $A^{-1}$ and $B^{-1}$ exist and:
$$AA^{-1}=A^{-1}A=I$$ and
$$BB^{-1}=B^{-1}B=I$$ 

Similarly, if $AB$ is invertible, $(AB)^{-1}$ exists and similar equalities should also hold:
$$AB(AB)^{-1}=(AB)^{-1}AB=I$$

As $(AB)^{-1} = B^{-1}A^{-1}$, then:
$$(AB)(B^{-1}A^{-1})=A(BB^{-1})A^{-1}$$
$$(AB)(B^{-1}A^{-1})=AIA^{-1}$$
$$(AB)(B^{-1}A^{-1})=AA^{-1}$$
$$(AB)(B^{-1}A^{-1})=I$$
and
$$(B^{-1}A^{-1})(AB)=B^{-1}(A^{-1}A)B$$
$$(B^{-1}A^{-1})(AB)=B^{-1}IB$$
$$(B^{-1}A^{-1})(AB)=B^{-1}B$$
$$(B^{-1}A^{-1})(AB)=I$$

Since $(AB)(B^{-1}A^{-1})=(B^{-1}A^{-1})(AB)=I$, then $(AB)^{-1} = B^{-1}A^{-1}$.

\end{enumerate}
\end{enumerate}

\subsection*{Vector and matrix norms}
\begin{enumerate}
\setcounter{enumi}{2}
\item Let $\vec{x} \in \mathbb{R}^n$. Two vector norms, $||\vec{x}||_a$ and $||\vec{x}||_b$, are \emph{equivalent} if
  $\exists \, c,d \in \mathbb{R}$ such that
$$c||\vec{x}||_b \leq ||\vec{x}||_a \leq d||\vec{x}||_b.$$
Matrix norm equivalence is defined analogously to vector norm equivalence, i.e., $||\cdot||_a$ and $||\cdot||_b$ are
equivalent if $\exists \, c,d$ s.t. $c||A||_b \leq ||A||_a \leq d||A||_b$.
  \begin{enumerate}
  \item Let $\vec{x} \in \mathbb{R}^n, \, A \in \mathbb{R}^{n \times n}$.  For each of the following, verify the inequality and give an example of a non-zero vector or matrix for which the
  bound is achieved (showing that the bound is tight):
    \begin{enumerate}
      \item $||\vec{x}||_\infty \leq ||\vec{x}||_2$
      $$ \max_i|\vec{x}_i| \leq \left(\sum_{i=1}^n{|\vec{x}_i|^2}\right)^\frac{1}{2}$$
      Let say $|x_r|$ is the maximum value in $\vec{x}$, then:
      $$ |x_r| \leq (\sum_{i=1}^n{|\vec{x}_i|^2})^\frac{1}{2}$$
      $$ |x_r|^2 \leq |x_1|^2+|x_2|^2+\cdots+|x_r|^2+\cdots+|x_n|^2$$
      $$ 0 \leq |x_1|^2+|x_2|^2+\cdots+|x_n|^2$$
      It is clear that any addition of squared absolute values is positive.\\
      i.e. $\vec{x}= \left(\begin{array}{c} 3 \\ 4 \\ \end{array} \right)$
      $$ ||\vec{x}||_\infty \leq ||\vec{x}||_2$$
      $$ 4 \leq 5$$
      \item $||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_\infty$
      $$ \left(\sum_{i=1}^n{|\vec{x}_i|^2}\right)^\frac{1}{2} \leq \sqrt{n}\max_i|\vec{x}_i| $$
      Again, let say $|x_r|$ is the maximum value in $\vec{x}$, then:
      $$ \left(\sum_{i=1}^n{|\vec{x}_i|^2}\right)^\frac{1}{2}  \leq \sqrt{n}|x_r| $$
      $$ \sum_{i=1}^n{|\vec{x}_i|^2} \leq n|x_r|^2 $$
      $$ |x_1|^2+|x_2|^2+\cdots+|x_r|^2+\cdots+|x_n|^2 \leq n|x_r|^2 $$
      Just in the case that all values of the left hand side are $x_r$ we will have an equality. For other cases, it is clear that the right hand side is greater.\\
      i.e. $\vec{x}= \left(\begin{array}{c} 2 \\ 2 \\ 2 \\ \end{array} \right)$
      $$ ||\vec{x}||_2 \leq \sqrt{n} ||\vec{x}||_\infty$$
      $$ \sqrt{4+4+4} \leq \sqrt{3}\times2 $$
      $$ \sqrt{12} \leq 2\sqrt{3} $$
      $$ \sqrt{4\times3} \leq 2\sqrt{3} $$
      $$ 2\sqrt{3} \leq 2\sqrt{3} $$

      \item $||A||_\infty \leq \sqrt{n} ||A||_2$
      \item $||A||_2 \leq \sqrt{n} ||A||_\infty$\\
      For items \texttt{iii} and \texttt{iv} the verification is not straightforward.  As sections 2.3.2 and 3.6.1 of the textbook state ``the matrix norm corresponding to the vector 2-norm is not so easy to compute''.  However, based on examples 2.4 and 3.17, we can give an example using the matrix:
      $$ A=\left[\begin{array}{ccc} 2&-1&1 \\ 1&0&1 \\ 3&-1&4 \\ \end{array} \right] $$
      where $||A||_\infty=8$ and $||A||_2=5,723$, so:
      $$||A||_\infty \leq \sqrt{n} ||A||_2$$
      $$8 \leq \sqrt{3}\times5,723$$
      $$8 \leq 9,91$$
      and
      $$||A||_2 \leq \sqrt{n} ||A||_\infty$$
      $$5,723 \leq \sqrt{3}\times8$$
      $$5,723 \leq 13,85$$
    \end{enumerate}
  This shows that $||\cdot||_\infty$ and $||\cdot||_2$ are equivalent, and that their induced matrix norms are equivalent.
  \item Prove that the equivalence of two vector norms implies the equivalence of their induced matrix norms.
  \end{enumerate}
\end{enumerate}

\subsection*{Sensitivity and conditioning}
\begin{enumerate}
\setcounter{enumi}{3}
\item (Heath 2.58) Suppose that the $n \times n$ matrix $A$ is perfectly well-conditioned, i.e., cond(A) = 1.  Which of the following matrices would then necessarily share this same
  property?
\begin{enumerate}
\item $cA$, where $c$ is any nonzero scalar \Large \checkmark \normalsize
\item $DA$, where $D$ is a nonsingular diagonal matrix
\item $PA$, where $P$ is any permutation matrix \Large \checkmark \normalsize
\item $BA$, where $B$ is any nonsingular matrix
\item $A^{-1}$, the inverse of $A$ \Large \checkmark \normalsize
\item $A^T$, the transpose of $A$
\end{enumerate}
\end{enumerate}

\subsection*{Linear Systems}
\begin{enumerate}
\setcounter{enumi}{4}
\item (Heath 2.4a) Show that the following matrix is singular.
$$
A = \left(\begin{array}{ccc} 1 & 1 & 0 \\ 1 & 2 & 1 \\ 1& 3 & 2 \end{array} \right)
$$

Answer:\\
Let's compute the determinant of $A$:\\
\\
\begin{tabular}{rl}
$det(A)$ & $ = 1[(2\times2)-(3\times1)]-1[(1\times2)-(3\times0)]+1[(1\times1)-(2\times0)]$ \\
& $ = 1-2+1$ \\
& $ = 0$
\end{tabular}

So, given $det(A)=0$, $A$ must be singular.
\item For each of the following statements, indicate whether the statement is true or false.
    \begin{description}
    \item[\circled{T}/F] If a matrix $A$ is singular, then the number of solutions to the linear system $A\vec{x} = \vec{b}$ depends on the particular choice of right-hand-side $\vec{b}$.
    \item[T/\circled{F}] If a matrix $A$ is nonsingular, then the number of solutions to the linear system $A\vec{x} = \vec{b}$ depends on the particular choice of right-hand-side $\vec{b}$.
    \item[T/\circled{F}] If a matrix has a very small determinant, then the matrix is nearly singular.
    \item[T/\circled{F}] If any matrix has a zero on its main diagonal, then it is necessarily singular.
    \end{description}
\item Can a system of linear equations $A\vec{x} = \vec{b}$ have exactly two solutions?  Explain your answer.\\
\\
Answer:\\
\\
No, according to section 2.2 of the textbook, a system of linear equations can have an unique solution, no solution at all or infinitely many solutions. Visually, it is easy to see that lines in a plane (or, more general, hyperplanes in higher dimensions) can only touch in one point (unique solution), be parallel and do not touch (no solution) or be the same line (hyperplane) and have multiple solutions.

Formally, let say $\vec{x_1}$ and $\vec{x_2}$ are the two unique solutions.  So, from $A\vec{x}=\vec{b}$ we have:
$$A\vec{x_1}=A\vec{x_2}=\vec{b}$$
Now, let assume $\frac{\vec{x_1}}{4}+\frac{3\vec{x_2}}{4}$ as a possible new solution.  So:\\
\\
\begin{tabular}{rl}
$A(\frac{\vec{x_1}}{4}+\frac{3\vec{x_1}}{4})$ & $= \vec{b}$ \\
$\frac{1}{4}(A\vec{x_1}+3A\vec{x_2})$ & $= \vec{b}$ \\
$\frac{1}{4}(\vec{b}+3\vec{b})$ & $= \vec{b}$ \\
$\frac{1}{4}(4\vec{b})$ & $= \vec{b}$ \\
$\vec{b}$ & $= \vec{b}$ \\
\end{tabular}

So, $\frac{\vec{x_1}}{4}+\frac{3\vec{x_2}}{4}$ is a third solution for the system.  Indeed, for any $\lambda$ in $(0,1)$:
$$\lambda\vec{x_1}+(1-\lambda)\vec{x_2}$$
should be a solution and the system must have infitinely many solutions.  For example, in the abovementioned example $\lambda=\frac{1}{4}$. 

\end{enumerate}

\subsection*{LU Factorization and Gaussian Eliminiation}

\begin{enumerate}
\setcounter{enumi}{7}
\item For each of the following statements, indicate whether the statement is true or false.
    \begin{description}
    \item[\circled{T}/F] If a triangular matrix has a zero on its main diagonal, then it is necessarily singular.
    \item[\circled{T}/F] The product of two upper triangular matrices is upper triangular.
    \item[T/\circled{F}] If a linear system is well-conditioned, then pivoting is unnecessary in Gaussian elimination.
    \item[\circled{T}/F] Once the LU factorization of a matrix has been computed to solve a linear system, then subsequent linear systems with the same matrix but different right-hand-side vectors can be solved without refactoring the matrix.
    \end{description}
%\item (T\&B 20.2) Suppose $A \in \mathbb{R}^{n \times n}$ has an $LU$ factorization.  Suppose that $A$ is banded with bandwidth $2p+1$, i.e., $a_{ij} = 0$ for $|i-j|>p$.  What can you say about the sparsity patterns of the factors $L$ and $U$ of $A$?  Explain.
\item Consider $LU$ factorization with partial pivoting of the matrix $A$ which computes
$$
M_{n-1} P_{n-1} \cdots M_3 P_3 M_2 P_2 M_1 P_1 A = U
$$
where $P_i$ is a row permutation matrix interchanging rows $i$ and $j>i$. 
  \begin{enumerate}
  \item Show that the matrix $P_3 P_2 M_1 P_2^{-1} P_3^{-1}$ has the same structure as the matrix $M_1$.\\
  \\
  Answer:\\
  \\
  Since $M_j=I-\vec{m_j}\vec{e_j^T}$, we can show that any $P_kM_jP_k^{-1}$ has the same structure as $M_j$ by:\\
\\
\begin{tabular}{rl}
$P_kM_jP_k^{-1}$ & $= P_k(I-\vec{m_j}\vec{e_j^T})P_k^{-1}$ \\
 & $= P_kP_k^{-1}-P_k\vec{m_j}\vec{e_j^T}P_k^{-1}$ \\
\end{tabular}
\\
\\
  As $\vec{e_j^T}P_k^{-1}=\vec{e_j^T}$, we have:\\
\\
\begin{tabular}{rl}
$P_kM_jP_k^{-1}$ & $= P_kP_k^{-1}-(P_k\vec{m_j})\vec{e_j^T}$ \\
 & $= I-(P_k\vec{m_j})\vec{e_j^T}$ \\
 & $= M_j^\prime$ \\
\end{tabular}
\\
\\
So, $M_j^\prime=I-(P_k\vec{m_j})\vec{e_j^T}$ has the same structure that $M_j=I-\vec{m_j}\vec{e_j^T}$. This concept can easily be applied to $P_3 P_2 M_1 P_2^{-1} P_3^{-1}$ to show it has the same structure as the matrix $M_1$.
 
  \item Explain how the above expression is transformed into the form $PA = LU$, where $P$ is a row permutation matrix.\\
  \\
  Answer:\\
  \\
  For sake of simplicity, let's take $n=3$, so:
  $$M_3 P_3 M_2 P_2 M_1 P_1 A = U$$
  We can insert identity matrices in the form of $I_2=P_3^{-1}P_3$ and $I_1=P_2^{-1}P_3^{-1}P_3P_2$ after $M_2$ and $M_1$ respectively, so:
  $$M_3 P_3 M_2 (P_3^{-1} P_3) P_2 M_1 (P_2^{-1} P_3^{-1} P_3 P_2) P_1 A = U$$
  or
  $$M_3 (P_3 M_2 P_3^{-1}) P_3 (P_2 M_1 P_2^{-1}) P_3^{-1} P_3 P_2 P_1 A = U$$
  As we already know, $P_kM_jP_k^{-1}=M_j^\prime$ has the same structure as $M_j$ for $k>j$. In addition, we can group the remaining permutations as $P$, so:
  $$M_3 M_2^\prime M_1^\prime P A = U$$
  As $M_3 M_2^\prime M_1^\prime = L^{-1}$, we have:
  $$L^{-1} P A = U$$
  $$P A = L U$$
  \end{enumerate}
\end{enumerate}

\subsection*{Cholesky Factorization}
\begin{enumerate}
\setcounter{enumi}{9}
\item (Heath 2.37) Suppose that the symmetric $(n+1) \times (n+1)$ matrix
$$
B = \begin{pmatrix}
\alpha & \vec{a}^T \\
\vec{a} & A
\end{pmatrix}
$$
is positive definite.
\begin{enumerate}
\item Show that the scalar $\alpha$ must be positive and the $n \times n$ matrix $A$ must be positive definite.\\
\\
Answer:\\
\\
Since $B$ is positive definite, then:
$$\vec{x}^TB\vec{x}>0$$
$$\vec{x}^T\begin{pmatrix}
\alpha & \vec{a}^T \\
\vec{a} & A
\end{pmatrix}\vec{x}>0$$
If $\vec{x}=\begin{pmatrix}1\\0\end{pmatrix}$, then:
$$\begin{pmatrix}1&0\end{pmatrix}
\begin{pmatrix}
\alpha & \vec{a}^T \\
\vec{a} & A
\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}>0
$$
$$\begin{pmatrix}
\alpha & \vec{a}^T \\
\end{pmatrix}\begin{pmatrix}1\\0\end{pmatrix}>0
$$
$$ \alpha > 0 $$

So, $\alpha$ is positive.\\
Similarly, if $\vec{x}=\begin{pmatrix} 0 \\ \vec{y} \end{pmatrix}$, where $\vec{y}$ is any non-zero vector, then:
$$\begin{pmatrix} 0 & \vec{y}^T \end{pmatrix}
\begin{pmatrix}
\alpha & \vec{a}^T \\
\vec{a} & A
\end{pmatrix}\begin{pmatrix} 0 \\ \vec{y} \end{pmatrix}>0
$$
$$\begin{pmatrix}
\vec{y}^T\vec{a} & \vec{y}^TA \\
\end{pmatrix}\begin{pmatrix} 0 \\ \vec{y} \end{pmatrix}>0
$$

$$ \vec{y}^TA\vec{y} > 0 $$

So, $A$ is positive definite.

\item What is the Cholesky factorization of $B$ in terms of $\alpha$, $\vec{a}$, and the Cholesky factorization of $A$?\\
\\
Answer:\\
\\
Following section 2.5.1 of the textbook, we have $B=LL^T$, we can express $LL^T$ in terms of $\alpha$ and $\vec{a}$:

$$
B = \begin{pmatrix}
\alpha & \vec{a}^T \\
\vec{a} & A
\end{pmatrix} =
\begin{pmatrix}
\sqrt{\alpha} & 0 \\
\frac{\vec{a}}{\sqrt{\alpha}} & L^\prime
\end{pmatrix}
\begin{pmatrix}
\sqrt{\alpha} & \frac{\vec{a}^T}{\sqrt{\alpha}} \\
0 & L^{\prime T}
\end{pmatrix}
$$

Where $$L^\prime L^{\prime T}=A-\frac{\vec{a}\vec{a}^T}{\sqrt{\alpha}}$$ is the Cholesky factorization of $A$.
\end{enumerate}
\end{enumerate}



\end{document}
