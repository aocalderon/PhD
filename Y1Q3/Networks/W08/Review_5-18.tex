\documentclass[a4paper,10pt]{scrartcl}
\usepackage[hmargin=2.5cm,vmargin=3cm]{geometry}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{hyperref}
\hypersetup{
colorlinks=false,
hidelinks
}

\setlength{\parindent}{2em}
\setlength{\parskip}{0.5em}

%opening
\title{Paper Review 5/13}
\author{Andres Calderon - SID:861243796}

\begin{document}
\maketitle
\thispagestyle{empty}

\section*{Diagnosing Network-Wide Traffic Anomalies (Lakhina et al., 2004)}
This paper describes a technique to detect, identify and measure traffic anomalies on a network.  It is based on Principal Component Analysis (PCA) of high-dimensional, noisy data.  The analysis is able to separate the subspace of the normal traffic and detect the anomalous component corresponding to the anomalies.

Although the evaluation of the method focuses on \textit{volume} anomalies, it is based on correlations of time-series from diverse links and its treatment as a spatial problem.  Therefore, the method can be extended to other types of anomalies.

Overall, the paper is interesting and easy to read.  Particularly, it is more related to my area of interest (data mining). On that sense, it is well-known that PCA is quite expensive in terms of execution time.  I would like to know if other techniques for time-series anomaly detection were considered.  Anytime clustering of time-series has shown very efficient results in the analysis of real-time complex problems.
% \begin{thebibliography}{9}
% \bibitem{github} 
% Andres Calderon.
% \textit{GitHub Personal Repository}, 2015. 
% \url{https://github.com/aocalderon/PhD/tree/master/Y1Q1/GPU/lab3}.
% \end{thebibliography}

\end{document}
