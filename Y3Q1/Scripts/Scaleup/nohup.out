acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 07:15:38,252 -> Starting session,1.67,0
2017-12-10 07:15:38,253 -> Setting variables,0.00,0
2017-12-10 07:15:42,535 -> Reading datasets,4.28,0
2017-12-10 07:15:42,551 -> Points partitions: 2
2017-12-10 07:15:42,561 -> Centers partitions: 2
2017-12-10 07:15:50,653 -> 01.Indexing points,8.10,78857,50.0,7
2017-12-10 07:15:59,030 -> 02.Indexing centers,8.37,502168,50.0,7
2017-12-10 07:15:59,045 -> 1024
2017-12-10 07:15:59,055 -> 1024
2017-12-10 07:16:22,069 -> 03.Joining datasets,23.01,502159,50.0,7
Done!!! Sun Dec 10 07:16:22 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 07:16:24,828 -> Starting session,1.51,0
2017-12-10 07:16:24,829 -> Setting variables,0.00,0
2017-12-10 07:16:29,268 -> Reading datasets,4.44,0
2017-12-10 07:16:29,281 -> Points partitions: 2
2017-12-10 07:16:29,290 -> Centers partitions: 2
2017-12-10 07:16:37,180 -> 01.Indexing points,7.85,78857,50.0,7
2017-12-10 07:16:45,424 -> 02.Indexing centers,8.24,502168,50.0,7
2017-12-10 07:16:45,438 -> 1024
2017-12-10 07:16:45,448 -> 1024
2017-12-10 07:17:07,480 -> 03.Joining datasets,22.03,502159,50.0,7
Done!!! Sun Dec 10 07:17:07 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 07:17:10,177 -> Starting session,1.63,0
2017-12-10 07:17:10,178 -> Setting variables,0.00,0
2017-12-10 07:17:14,648 -> Reading datasets,4.47,0
2017-12-10 07:17:14,664 -> Points partitions: 2
2017-12-10 07:17:14,674 -> Centers partitions: 2
2017-12-10 07:17:22,223 -> 01.Indexing points,7.51,78857,50.0,7
2017-12-10 07:17:30,696 -> 02.Indexing centers,8.47,502168,50.0,7
2017-12-10 07:17:30,707 -> 1024
2017-12-10 07:17:30,715 -> 1024
2017-12-10 07:17:53,303 -> 03.Joining datasets,22.59,502159,50.0,7
Done!!! Sun Dec 10 07:17:53 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 07:17:55,908 -> Starting session,1.38,0
2017-12-10 07:17:55,908 -> Setting variables,0.00,0
2017-12-10 07:18:00,426 -> Reading datasets,4.52,0
2017-12-10 07:18:00,443 -> Points partitions: 2
2017-12-10 07:18:00,456 -> Centers partitions: 2
2017-12-10 07:18:08,057 -> 01.Indexing points,7.55,78857,50.0,7
2017-12-10 07:18:16,701 -> 02.Indexing centers,8.64,502168,50.0,7
2017-12-10 07:18:16,711 -> 1024
2017-12-10 07:18:16,718 -> 1024
2017-12-10 07:18:39,864 -> 03.Joining datasets,23.15,502159,50.0,7
Done!!! Sun Dec 10 07:18:40 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 07:18:42,577 -> Starting session,1.53,0
2017-12-10 07:18:42,577 -> Setting variables,0.00,0
2017-12-10 07:18:46,928 -> Reading datasets,4.35,0
2017-12-10 07:18:46,942 -> Points partitions: 2
2017-12-10 07:18:46,951 -> Centers partitions: 2
2017-12-10 07:18:54,244 -> 01.Indexing points,7.25,78857,50.0,7
2017-12-10 07:19:02,333 -> 02.Indexing centers,8.09,502168,50.0,7
2017-12-10 07:19:02,346 -> 1024
2017-12-10 07:19:02,355 -> 1024
2017-12-10 07:19:24,595 -> 03.Joining datasets,22.24,502159,50.0,7
Done!!! Sun Dec 10 07:19:25 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 07:19:32,910 -> Starting session,1.63,0
2017-12-10 07:19:32,910 -> Setting variables,0.00,0
2017-12-10 07:19:37,759 -> Reading datasets,4.85,0
2017-12-10 07:19:37,771 -> Points partitions: 2
2017-12-10 07:19:37,779 -> Centers partitions: 2
2017-12-10 07:19:44,908 -> 01.Indexing points,7.07,78857,50.0,14
2017-12-10 07:19:53,264 -> 02.Indexing centers,8.35,502168,50.0,14
2017-12-10 07:19:53,276 -> 1024
2017-12-10 07:19:53,282 -> 1024
2017-12-10 07:20:10,927 -> 03.Joining datasets,17.64,502159,50.0,14
Done!!! Sun Dec 10 07:20:11 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 07:20:13,757 -> Starting session,1.56,0
2017-12-10 07:20:13,758 -> Setting variables,0.00,0
2017-12-10 07:20:18,628 -> Reading datasets,4.87,0
2017-12-10 07:20:18,642 -> Points partitions: 2
2017-12-10 07:20:18,653 -> Centers partitions: 2
2017-12-10 07:20:26,231 -> 01.Indexing points,7.54,78857,50.0,14
2017-12-10 07:20:34,095 -> 02.Indexing centers,7.86,502168,50.0,14
2017-12-10 07:20:34,102 -> 1024
2017-12-10 07:20:34,109 -> 1024
2017-12-10 07:20:53,011 -> 03.Joining datasets,18.90,502159,50.0,14
Done!!! Sun Dec 10 07:20:53 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 07:20:55,900 -> Starting session,1.76,0
2017-12-10 07:20:55,901 -> Setting variables,0.00,0
2017-12-10 07:21:00,955 -> Reading datasets,5.05,0
2017-12-10 07:21:00,966 -> Points partitions: 2
2017-12-10 07:21:00,976 -> Centers partitions: 2
2017-12-10 07:21:08,650 -> 01.Indexing points,7.63,78857,50.0,14
2017-12-10 07:21:17,167 -> 02.Indexing centers,8.52,502168,50.0,14
2017-12-10 07:21:17,178 -> 1024
2017-12-10 07:21:17,187 -> 1024
2017-12-10 07:21:37,209 -> 03.Joining datasets,20.02,502159,50.0,14
Done!!! Sun Dec 10 07:21:37 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 07:21:39,860 -> Starting session,1.55,0
2017-12-10 07:21:39,861 -> Setting variables,0.00,0
2017-12-10 07:21:44,903 -> Reading datasets,5.04,0
2017-12-10 07:21:44,914 -> Points partitions: 2
2017-12-10 07:21:44,922 -> Centers partitions: 2
2017-12-10 07:21:52,570 -> 01.Indexing points,7.61,78857,50.0,14
2017-12-10 07:22:00,630 -> 02.Indexing centers,8.06,502168,50.0,14
2017-12-10 07:22:00,642 -> 1024
2017-12-10 07:22:00,651 -> 1024
2017-12-10 07:22:18,754 -> 03.Joining datasets,18.10,502159,50.0,14
Done!!! Sun Dec 10 07:22:19 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 07:22:21,501 -> Starting session,1.54,0
2017-12-10 07:22:21,501 -> Setting variables,0.00,0
2017-12-10 07:22:26,364 -> Reading datasets,4.86,0
2017-12-10 07:22:26,376 -> Points partitions: 2
2017-12-10 07:22:26,383 -> Centers partitions: 2
2017-12-10 07:22:34,077 -> 01.Indexing points,7.66,78857,50.0,14
2017-12-10 07:22:42,424 -> 02.Indexing centers,8.35,502168,50.0,14
2017-12-10 07:22:42,433 -> 1024
2017-12-10 07:22:42,439 -> 1024
2017-12-10 07:22:59,762 -> 03.Joining datasets,17.43,502159,50.0,14
Done!!! Sun Dec 10 07:23:00 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 07:23:08,008 -> Starting session,1.68,0
2017-12-10 07:23:08,009 -> Setting variables,0.00,0
2017-12-10 07:23:14,056 -> Reading datasets,6.05,0
2017-12-10 07:23:14,070 -> Points partitions: 2
2017-12-10 07:23:14,081 -> Centers partitions: 2
2017-12-10 07:23:21,295 -> 01.Indexing points,7.17,78857,50.0,21
2017-12-10 07:23:28,827 -> 02.Indexing centers,7.53,502168,50.0,21
2017-12-10 07:23:28,835 -> 1024
2017-12-10 07:23:28,842 -> 1024
2017-12-10 07:23:42,378 -> 03.Joining datasets,13.54,502159,50.0,21
Done!!! Sun Dec 10 07:23:42 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 07:23:45,416 -> Starting session,1.82,0
2017-12-10 07:23:45,417 -> Setting variables,0.00,0
2017-12-10 07:23:50,352 -> Reading datasets,4.93,0
2017-12-10 07:23:50,370 -> Points partitions: 2
2017-12-10 07:23:50,383 -> Centers partitions: 2
2017-12-10 07:23:58,384 -> 01.Indexing points,7.96,78857,50.0,21
2017-12-10 07:24:06,580 -> 02.Indexing centers,8.19,502168,50.0,21
2017-12-10 07:24:06,587 -> 1024
2017-12-10 07:24:06,593 -> 1024
2017-12-10 07:24:20,555 -> 03.Joining datasets,13.96,502159,50.0,21
Done!!! Sun Dec 10 07:24:21 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 07:24:23,231 -> Starting session,1.52,0
2017-12-10 07:24:23,231 -> Setting variables,0.00,0
2017-12-10 07:24:28,280 -> Reading datasets,5.05,0
2017-12-10 07:24:28,290 -> Points partitions: 2
2017-12-10 07:24:28,298 -> Centers partitions: 2
2017-12-10 07:24:36,358 -> 01.Indexing points,8.02,78857,50.0,21
2017-12-10 07:24:43,918 -> 02.Indexing centers,7.56,502168,50.0,21
2017-12-10 07:24:43,926 -> 1024
2017-12-10 07:24:43,933 -> 1024
2017-12-10 07:24:57,902 -> 03.Joining datasets,13.97,502159,50.0,21
Done!!! Sun Dec 10 07:24:59 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 07:25:01,622 -> Starting session,1.63,0
2017-12-10 07:25:01,622 -> Setting variables,0.00,0
2017-12-10 07:25:06,367 -> Reading datasets,4.74,0
2017-12-10 07:25:06,383 -> Points partitions: 2
2017-12-10 07:25:06,395 -> Centers partitions: 2
2017-12-10 07:25:14,649 -> 01.Indexing points,8.21,78857,50.0,21
2017-12-10 07:25:22,352 -> 02.Indexing centers,7.70,502168,50.0,21
2017-12-10 07:25:22,363 -> 1024
2017-12-10 07:25:22,371 -> 1024
2017-12-10 07:25:38,288 -> 03.Joining datasets,15.92,502159,50.0,21
Done!!! Sun Dec 10 07:25:39 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 07:25:41,889 -> Starting session,1.56,0
2017-12-10 07:25:41,890 -> Setting variables,0.00,0
2017-12-10 07:25:47,594 -> Reading datasets,5.70,0
2017-12-10 07:25:47,613 -> Points partitions: 2
2017-12-10 07:25:47,625 -> Centers partitions: 2
2017-12-10 07:25:54,591 -> 01.Indexing points,6.91,78857,50.0,21
2017-12-10 07:26:01,987 -> 02.Indexing centers,7.39,502168,50.0,21
2017-12-10 07:26:01,996 -> 1024
2017-12-10 07:26:02,005 -> 1024
2017-12-10 07:26:16,058 -> 03.Joining datasets,14.05,502159,50.0,21
Done!!! Sun Dec 10 07:26:17 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 07:26:25,270 -> Starting session,1.80,0
2017-12-10 07:26:25,271 -> Setting variables,0.00,0
2017-12-10 07:26:31,500 -> Reading datasets,6.23,0
2017-12-10 07:26:31,513 -> Points partitions: 2
2017-12-10 07:26:31,520 -> Centers partitions: 2
2017-12-10 07:26:41,021 -> 01.Indexing points,9.46,78857,50.0,28
2017-12-10 07:26:50,689 -> 02.Indexing centers,9.67,502168,50.0,28
2017-12-10 07:26:50,707 -> 1024
2017-12-10 07:26:50,716 -> 1024
2017-12-10 07:27:04,288 -> 03.Joining datasets,13.57,502159,50.0,28
Done!!! Sun Dec 10 07:27:04 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 07:27:07,169 -> Starting session,1.59,0
2017-12-10 07:27:07,170 -> Setting variables,0.00,0
2017-12-10 07:27:14,214 -> Reading datasets,7.04,0
2017-12-10 07:27:14,231 -> Points partitions: 2
2017-12-10 07:27:14,242 -> Centers partitions: 2
2017-12-10 07:27:21,966 -> 01.Indexing points,7.68,78857,50.0,28
2017-12-10 07:27:32,239 -> 02.Indexing centers,10.27,502168,50.0,28
2017-12-10 07:27:32,249 -> 1024
2017-12-10 07:27:32,255 -> 1024
2017-12-10 07:27:45,185 -> 03.Joining datasets,12.93,502159,50.0,28
Done!!! Sun Dec 10 07:27:45 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 07:27:47,929 -> Starting session,1.61,0
2017-12-10 07:27:47,930 -> Setting variables,0.00,0
2017-12-10 07:27:55,150 -> Reading datasets,7.22,0
2017-12-10 07:27:55,170 -> Points partitions: 2
2017-12-10 07:27:55,183 -> Centers partitions: 2
2017-12-10 07:28:04,573 -> 01.Indexing points,9.33,78857,50.0,28
2017-12-10 07:28:15,688 -> 02.Indexing centers,11.11,502168,50.0,28
2017-12-10 07:28:15,699 -> 1024
2017-12-10 07:28:15,708 -> 1024
2017-12-10 07:28:28,387 -> 03.Joining datasets,12.72,502159,50.0,28
Done!!! Sun Dec 10 07:28:28 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 07:28:31,210 -> Starting session,1.56,0
2017-12-10 07:28:31,211 -> Setting variables,0.00,0
2017-12-10 07:28:37,621 -> Reading datasets,6.41,0
2017-12-10 07:28:37,638 -> Points partitions: 2
2017-12-10 07:28:37,649 -> Centers partitions: 2
2017-12-10 07:28:44,741 -> 01.Indexing points,7.04,78857,50.0,28
2017-12-10 07:28:53,096 -> 02.Indexing centers,8.35,502168,50.0,28
2017-12-10 07:28:53,109 -> 1024
2017-12-10 07:28:53,119 -> 1024
2017-12-10 07:29:05,726 -> 03.Joining datasets,12.61,502159,50.0,28
Done!!! Sun Dec 10 07:29:06 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 07:29:08,479 -> Starting session,1.54,0
2017-12-10 07:29:08,479 -> Setting variables,0.00,0
2017-12-10 07:29:15,661 -> Reading datasets,7.18,0
2017-12-10 07:29:15,684 -> Points partitions: 2
2017-12-10 07:29:15,696 -> Centers partitions: 2
2017-12-10 07:29:23,875 -> 01.Indexing points,8.13,78857,50.0,28
2017-12-10 07:29:32,057 -> 02.Indexing centers,8.18,502168,50.0,28
2017-12-10 07:29:32,070 -> 1024
2017-12-10 07:29:32,080 -> 1024
2017-12-10 07:29:45,579 -> 03.Joining datasets,13.50,502159,50.0,28
Done!!! Sun Dec 10 07:29:46 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 07:29:54,091 -> Starting session,1.57,0
2017-12-10 07:29:54,091 -> Setting variables,0.00,0
2017-12-10 07:29:58,493 -> Reading datasets,4.40,0
2017-12-10 07:29:58,508 -> Points partitions: 2
2017-12-10 07:29:58,517 -> Centers partitions: 2
2017-12-10 07:30:05,816 -> 01.Indexing points,7.26,59143,50.0,7
2017-12-10 07:30:13,399 -> 02.Indexing centers,7.58,376626,50.0,7
2017-12-10 07:30:13,410 -> 1024
2017-12-10 07:30:13,419 -> 1024
2017-12-10 07:30:33,944 -> 03.Joining datasets,20.52,376619,50.0,7
Done!!! Sun Dec 10 07:30:34 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 07:30:36,737 -> Starting session,1.54,0
2017-12-10 07:30:36,737 -> Setting variables,0.00,0
2017-12-10 07:30:41,000 -> Reading datasets,4.26,0
2017-12-10 07:30:41,013 -> Points partitions: 2
2017-12-10 07:30:41,022 -> Centers partitions: 2
2017-12-10 07:30:48,367 -> 01.Indexing points,7.30,59143,50.0,7
2017-12-10 07:30:55,404 -> 02.Indexing centers,7.04,376626,50.0,7
2017-12-10 07:30:55,416 -> 1024
2017-12-10 07:30:55,424 -> 1024
2017-12-10 07:31:15,734 -> 03.Joining datasets,20.31,376619,50.0,7
Done!!! Sun Dec 10 07:31:16 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 07:31:18,469 -> Starting session,1.54,0
2017-12-10 07:31:18,469 -> Setting variables,0.00,0
2017-12-10 07:31:22,564 -> Reading datasets,4.09,0
2017-12-10 07:31:22,575 -> Points partitions: 2
2017-12-10 07:31:22,582 -> Centers partitions: 2
2017-12-10 07:31:30,238 -> 01.Indexing points,7.62,59143,50.0,7
2017-12-10 07:31:37,407 -> 02.Indexing centers,7.17,376626,50.0,7
2017-12-10 07:31:37,421 -> 1024
2017-12-10 07:31:37,431 -> 1024
2017-12-10 07:31:57,720 -> 03.Joining datasets,20.29,376619,50.0,7
Done!!! Sun Dec 10 07:31:58 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 07:32:00,445 -> Starting session,1.55,0
2017-12-10 07:32:00,446 -> Setting variables,0.00,0
2017-12-10 07:32:04,706 -> Reading datasets,4.26,0
2017-12-10 07:32:04,723 -> Points partitions: 2
2017-12-10 07:32:04,733 -> Centers partitions: 2
2017-12-10 07:32:12,147 -> 01.Indexing points,7.37,59143,50.0,7
2017-12-10 07:32:19,246 -> 02.Indexing centers,7.10,376626,50.0,7
2017-12-10 07:32:19,259 -> 1024
2017-12-10 07:32:19,269 -> 1024
2017-12-10 07:32:39,713 -> 03.Joining datasets,20.44,376619,50.0,7
Done!!! Sun Dec 10 07:32:40 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 07:32:42,443 -> Starting session,1.54,0
2017-12-10 07:32:42,444 -> Setting variables,0.00,0
2017-12-10 07:32:46,747 -> Reading datasets,4.30,0
2017-12-10 07:32:46,760 -> Points partitions: 2
2017-12-10 07:32:46,768 -> Centers partitions: 2
2017-12-10 07:32:54,571 -> 01.Indexing points,7.76,59143,50.0,7
2017-12-10 07:33:01,887 -> 02.Indexing centers,7.31,376626,50.0,7
2017-12-10 07:33:01,896 -> 1024
2017-12-10 07:33:01,903 -> 1024
2017-12-10 07:33:22,857 -> 03.Joining datasets,20.95,376619,50.0,7
Done!!! Sun Dec 10 07:33:23 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 07:33:31,225 -> Starting session,1.66,0
2017-12-10 07:33:31,227 -> Setting variables,0.00,0
2017-12-10 07:33:36,220 -> Reading datasets,4.99,0
2017-12-10 07:33:36,230 -> Points partitions: 2
2017-12-10 07:33:36,241 -> Centers partitions: 2
2017-12-10 07:33:43,404 -> 01.Indexing points,7.12,59143,50.0,14
2017-12-10 07:33:50,527 -> 02.Indexing centers,7.20,376626,50.0,14
2017-12-10 07:33:50,537 -> 1024
2017-12-10 07:33:50,547 -> 1024
2017-12-10 07:34:06,643 -> 03.Joining datasets,16.10,376619,50.0,14
Done!!! Sun Dec 10 07:34:07 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 07:34:09,448 -> Starting session,1.69,0
2017-12-10 07:34:09,449 -> Setting variables,0.00,0
2017-12-10 07:34:14,264 -> Reading datasets,4.81,0
2017-12-10 07:34:14,276 -> Points partitions: 2
2017-12-10 07:34:14,283 -> Centers partitions: 2
2017-12-10 07:34:21,689 -> 01.Indexing points,7.37,59143,50.0,14
2017-12-10 07:34:28,907 -> 02.Indexing centers,7.22,376626,50.0,14
2017-12-10 07:34:28,918 -> 1024
2017-12-10 07:34:28,928 -> 1024
2017-12-10 07:34:47,397 -> 03.Joining datasets,18.47,376619,50.0,14
Done!!! Sun Dec 10 07:34:47 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 07:34:50,086 -> Starting session,1.58,0
2017-12-10 07:34:50,086 -> Setting variables,0.00,0
2017-12-10 07:34:54,979 -> Reading datasets,4.89,0
2017-12-10 07:34:54,990 -> Points partitions: 2
2017-12-10 07:34:54,998 -> Centers partitions: 2
2017-12-10 07:35:02,116 -> 01.Indexing points,7.08,59143,50.0,14
2017-12-10 07:35:09,176 -> 02.Indexing centers,7.06,376626,50.0,14
2017-12-10 07:35:09,186 -> 1024
2017-12-10 07:35:09,192 -> 1024
2017-12-10 07:35:26,235 -> 03.Joining datasets,17.04,376619,50.0,14
Done!!! Sun Dec 10 07:35:26 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 07:35:29,020 -> Starting session,1.57,0
2017-12-10 07:35:29,020 -> Setting variables,0.00,0
2017-12-10 07:35:34,036 -> Reading datasets,5.11,0
2017-12-10 07:35:34,046 -> Points partitions: 2
2017-12-10 07:35:34,054 -> Centers partitions: 2
2017-12-10 07:35:41,250 -> 01.Indexing points,7.15,59143,50.0,14
2017-12-10 07:35:48,270 -> 02.Indexing centers,7.02,376626,50.0,14
2017-12-10 07:35:48,280 -> 1024
2017-12-10 07:35:48,289 -> 1024
2017-12-10 07:36:05,377 -> 03.Joining datasets,17.09,376619,50.0,14
Done!!! Sun Dec 10 07:36:06 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 07:36:08,869 -> Starting session,1.48,0
2017-12-10 07:36:08,869 -> Setting variables,0.00,0
2017-12-10 07:36:13,702 -> Reading datasets,4.83,0
2017-12-10 07:36:13,716 -> Points partitions: 2
2017-12-10 07:36:13,726 -> Centers partitions: 2
2017-12-10 07:36:21,220 -> 01.Indexing points,7.46,59143,50.0,14
2017-12-10 07:36:28,382 -> 02.Indexing centers,7.16,376626,50.0,14
2017-12-10 07:36:28,394 -> 1024
2017-12-10 07:36:28,404 -> 1024
2017-12-10 07:36:45,810 -> 03.Joining datasets,17.41,376619,50.0,14
Done!!! Sun Dec 10 07:36:46 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 07:36:54,249 -> Starting session,1.67,0
2017-12-10 07:36:54,250 -> Setting variables,0.00,0
2017-12-10 07:36:59,093 -> Reading datasets,4.84,0
2017-12-10 07:36:59,106 -> Points partitions: 2
2017-12-10 07:36:59,118 -> Centers partitions: 2
2017-12-10 07:37:06,472 -> 01.Indexing points,7.31,59143,50.0,21
2017-12-10 07:37:12,882 -> 02.Indexing centers,6.41,376626,50.0,21
2017-12-10 07:37:12,890 -> 1024
2017-12-10 07:37:12,897 -> 1024
2017-12-10 07:37:27,772 -> 03.Joining datasets,14.88,376619,50.0,21
Done!!! Sun Dec 10 07:37:28 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 07:37:30,564 -> Starting session,1.58,0
2017-12-10 07:37:30,565 -> Setting variables,0.00,0
2017-12-10 07:37:36,610 -> Reading datasets,6.04,0
2017-12-10 07:37:36,634 -> Points partitions: 2
2017-12-10 07:37:36,646 -> Centers partitions: 2
2017-12-10 07:37:43,602 -> 01.Indexing points,6.90,59143,50.0,21
2017-12-10 07:37:50,011 -> 02.Indexing centers,6.41,376626,50.0,21
2017-12-10 07:37:50,018 -> 1024
2017-12-10 07:37:50,024 -> 1024
2017-12-10 07:38:02,236 -> 03.Joining datasets,12.21,376619,50.0,21
Done!!! Sun Dec 10 07:38:02 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 07:38:05,099 -> Starting session,1.66,0
2017-12-10 07:38:05,100 -> Setting variables,0.00,0
2017-12-10 07:38:10,080 -> Reading datasets,4.98,0
2017-12-10 07:38:10,094 -> Points partitions: 2
2017-12-10 07:38:10,105 -> Centers partitions: 2
2017-12-10 07:38:17,879 -> 01.Indexing points,7.73,59143,50.0,21
2017-12-10 07:38:24,528 -> 02.Indexing centers,6.65,376626,50.0,21
2017-12-10 07:38:24,540 -> 1024
2017-12-10 07:38:24,549 -> 1024
2017-12-10 07:38:36,781 -> 03.Joining datasets,12.23,376619,50.0,21
Done!!! Sun Dec 10 07:38:37 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 07:38:39,577 -> Starting session,1.49,0
2017-12-10 07:38:39,578 -> Setting variables,0.00,0
2017-12-10 07:38:44,415 -> Reading datasets,4.84,0
2017-12-10 07:38:44,426 -> Points partitions: 2
2017-12-10 07:38:44,433 -> Centers partitions: 2
2017-12-10 07:38:52,021 -> 01.Indexing points,7.55,59143,50.0,21
2017-12-10 07:38:58,821 -> 02.Indexing centers,6.80,376626,50.0,21
2017-12-10 07:38:58,829 -> 1024
2017-12-10 07:38:58,835 -> 1024
2017-12-10 07:39:12,466 -> 03.Joining datasets,13.63,376619,50.0,21
Done!!! Sun Dec 10 07:39:12 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 07:39:15,257 -> Starting session,1.57,0
2017-12-10 07:39:15,258 -> Setting variables,0.00,0
2017-12-10 07:39:20,123 -> Reading datasets,4.87,0
2017-12-10 07:39:20,134 -> Points partitions: 2
2017-12-10 07:39:20,142 -> Centers partitions: 2
2017-12-10 07:39:27,794 -> 01.Indexing points,7.61,59143,50.0,21
2017-12-10 07:39:33,807 -> 02.Indexing centers,6.01,376626,50.0,21
2017-12-10 07:39:33,815 -> 1024
2017-12-10 07:39:33,821 -> 1024
2017-12-10 07:39:47,552 -> 03.Joining datasets,13.73,376619,50.0,21
Done!!! Sun Dec 10 07:39:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 07:39:55,987 -> Starting session,1.74,0
2017-12-10 07:39:55,988 -> Setting variables,0.00,0
2017-12-10 07:40:02,896 -> Reading datasets,6.91,0
2017-12-10 07:40:02,914 -> Points partitions: 2
2017-12-10 07:40:02,926 -> Centers partitions: 2
2017-12-10 07:40:10,372 -> 01.Indexing points,7.39,59143,50.0,28
2017-12-10 07:40:17,049 -> 02.Indexing centers,6.68,376626,50.0,28
2017-12-10 07:40:17,057 -> 1024
2017-12-10 07:40:17,064 -> 1024
2017-12-10 07:40:28,719 -> 03.Joining datasets,11.66,376619,50.0,28
Done!!! Sun Dec 10 07:40:29 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 07:40:31,410 -> Starting session,1.47,0
2017-12-10 07:40:31,411 -> Setting variables,0.00,0
2017-12-10 07:40:38,487 -> Reading datasets,7.08,0
2017-12-10 07:40:38,507 -> Points partitions: 2
2017-12-10 07:40:38,519 -> Centers partitions: 2
2017-12-10 07:40:45,989 -> 01.Indexing points,7.50,59143,50.0,28
2017-12-10 07:40:55,393 -> 02.Indexing centers,9.40,376626,50.0,28
2017-12-10 07:40:55,401 -> 1024
2017-12-10 07:40:55,407 -> 1024
2017-12-10 07:41:07,644 -> 03.Joining datasets,12.24,376619,50.0,28
Done!!! Sun Dec 10 07:41:09 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 07:41:11,608 -> Starting session,1.76,0
2017-12-10 07:41:11,609 -> Setting variables,0.00,0
2017-12-10 07:41:18,864 -> Reading datasets,7.26,0
2017-12-10 07:41:18,878 -> Points partitions: 2
2017-12-10 07:41:18,885 -> Centers partitions: 2
2017-12-10 07:41:27,316 -> 01.Indexing points,8.39,59143,50.0,28
2017-12-10 07:41:35,893 -> 02.Indexing centers,8.58,376626,50.0,28
2017-12-10 07:41:35,901 -> 1024
2017-12-10 07:41:35,907 -> 1024
2017-12-10 07:41:49,041 -> 03.Joining datasets,13.13,376619,50.0,28
Done!!! Sun Dec 10 07:41:49 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 07:41:51,844 -> Starting session,1.59,0
2017-12-10 07:41:51,845 -> Setting variables,0.00,0
2017-12-10 07:41:58,176 -> Reading datasets,6.33,0
2017-12-10 07:41:58,190 -> Points partitions: 2
2017-12-10 07:41:58,200 -> Centers partitions: 2
2017-12-10 07:42:07,562 -> 01.Indexing points,9.32,59143,50.0,28
2017-12-10 07:42:16,152 -> 02.Indexing centers,8.59,376626,50.0,28
2017-12-10 07:42:16,166 -> 1024
2017-12-10 07:42:16,177 -> 1024
2017-12-10 07:42:28,148 -> 03.Joining datasets,11.97,376619,50.0,28
Done!!! Sun Dec 10 07:42:28 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 07:42:31,024 -> Starting session,1.67,0
2017-12-10 07:42:31,025 -> Setting variables,0.00,0
2017-12-10 07:42:38,406 -> Reading datasets,7.38,0
2017-12-10 07:42:38,422 -> Points partitions: 2
2017-12-10 07:42:38,434 -> Centers partitions: 2
2017-12-10 07:42:46,487 -> 01.Indexing points,8.01,59143,50.0,28
2017-12-10 07:42:53,253 -> 02.Indexing centers,6.76,376626,50.0,28
2017-12-10 07:42:53,261 -> 1024
2017-12-10 07:42:53,266 -> 1024
2017-12-10 07:43:05,208 -> 03.Joining datasets,11.94,376619,50.0,28
Done!!! Sun Dec 10 07:43:05 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 07:43:13,763 -> Starting session,1.56,0
2017-12-10 07:43:13,763 -> Setting variables,0.00,0
2017-12-10 07:43:18,049 -> Reading datasets,4.28,0
2017-12-10 07:43:18,062 -> Points partitions: 2
2017-12-10 07:43:18,070 -> Centers partitions: 2
2017-12-10 07:43:25,380 -> 01.Indexing points,7.27,39429,50.0,7
2017-12-10 07:43:31,595 -> 02.Indexing centers,6.21,251084,50.0,7
2017-12-10 07:43:31,604 -> 1024
2017-12-10 07:43:31,610 -> 1024
2017-12-10 07:43:49,916 -> 03.Joining datasets,18.31,251080,50.0,7
Done!!! Sun Dec 10 07:43:50 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 07:43:52,703 -> Starting session,1.57,0
2017-12-10 07:43:52,703 -> Setting variables,0.00,0
2017-12-10 07:43:56,878 -> Reading datasets,4.17,0
2017-12-10 07:43:56,889 -> Points partitions: 2
2017-12-10 07:43:56,896 -> Centers partitions: 2
2017-12-10 07:44:04,199 -> 01.Indexing points,7.27,39429,50.0,7
2017-12-10 07:44:10,308 -> 02.Indexing centers,6.11,251084,50.0,7
2017-12-10 07:44:10,323 -> 1024
2017-12-10 07:44:10,334 -> 1024
2017-12-10 07:44:28,999 -> 03.Joining datasets,18.66,251080,50.0,7
Done!!! Sun Dec 10 07:44:29 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 07:44:31,538 -> Starting session,1.35,0
2017-12-10 07:44:31,539 -> Setting variables,0.00,0
2017-12-10 07:44:36,071 -> Reading datasets,4.53,0
2017-12-10 07:44:36,082 -> Points partitions: 2
2017-12-10 07:44:36,090 -> Centers partitions: 2
2017-12-10 07:44:43,349 -> 01.Indexing points,7.22,39429,50.0,7
2017-12-10 07:44:49,780 -> 02.Indexing centers,6.43,251084,50.0,7
2017-12-10 07:44:49,795 -> 1024
2017-12-10 07:44:49,805 -> 1024
2017-12-10 07:45:07,886 -> 03.Joining datasets,18.08,251080,50.0,7
Done!!! Sun Dec 10 07:45:08 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 07:45:10,648 -> Starting session,1.54,0
2017-12-10 07:45:10,649 -> Setting variables,0.00,0
2017-12-10 07:45:14,956 -> Reading datasets,4.31,0
2017-12-10 07:45:14,970 -> Points partitions: 2
2017-12-10 07:45:14,981 -> Centers partitions: 2
2017-12-10 07:45:22,416 -> 01.Indexing points,7.39,39429,50.0,7
2017-12-10 07:45:28,822 -> 02.Indexing centers,6.40,251084,50.0,7
2017-12-10 07:45:28,834 -> 1024
2017-12-10 07:45:28,842 -> 1024
2017-12-10 07:45:46,844 -> 03.Joining datasets,18.00,251080,50.0,7
Done!!! Sun Dec 10 07:45:47 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 07:45:49,373 -> Starting session,1.47,0
2017-12-10 07:45:49,373 -> Setting variables,0.00,0
2017-12-10 07:45:53,872 -> Reading datasets,4.50,0
2017-12-10 07:45:53,883 -> Points partitions: 2
2017-12-10 07:45:53,891 -> Centers partitions: 2
2017-12-10 07:46:01,377 -> 01.Indexing points,7.45,39429,50.0,7
2017-12-10 07:46:07,711 -> 02.Indexing centers,6.33,251084,50.0,7
2017-12-10 07:46:07,724 -> 1024
2017-12-10 07:46:07,733 -> 1024
2017-12-10 07:46:26,081 -> 03.Joining datasets,18.35,251080,50.0,7
Done!!! Sun Dec 10 07:46:26 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 07:46:34,459 -> Starting session,1.62,0
2017-12-10 07:46:34,461 -> Setting variables,0.00,0
2017-12-10 07:46:39,644 -> Reading datasets,5.18,0
2017-12-10 07:46:39,661 -> Points partitions: 2
2017-12-10 07:46:39,671 -> Centers partitions: 2
2017-12-10 07:46:46,403 -> 01.Indexing points,6.69,39429,50.0,14
2017-12-10 07:46:52,712 -> 02.Indexing centers,6.31,251084,50.0,14
2017-12-10 07:46:52,720 -> 1024
2017-12-10 07:46:52,726 -> 1024
2017-12-10 07:47:07,672 -> 03.Joining datasets,14.95,251080,50.0,14
Done!!! Sun Dec 10 07:47:08 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 07:47:10,431 -> Starting session,1.57,0
2017-12-10 07:47:10,431 -> Setting variables,0.00,0
2017-12-10 07:47:15,262 -> Reading datasets,4.83,0
2017-12-10 07:47:15,279 -> Points partitions: 2
2017-12-10 07:47:15,288 -> Centers partitions: 2
2017-12-10 07:47:22,560 -> 01.Indexing points,7.24,39429,50.0,14
2017-12-10 07:47:28,867 -> 02.Indexing centers,6.31,251084,50.0,14
2017-12-10 07:47:28,878 -> 1024
2017-12-10 07:47:28,887 -> 1024
2017-12-10 07:47:45,031 -> 03.Joining datasets,16.14,251080,50.0,14
Done!!! Sun Dec 10 07:47:45 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 07:47:48,013 -> Starting session,1.63,0
2017-12-10 07:47:48,014 -> Setting variables,0.00,0
2017-12-10 07:47:52,744 -> Reading datasets,4.73,0
2017-12-10 07:47:52,756 -> Points partitions: 2
2017-12-10 07:47:52,763 -> Centers partitions: 2
2017-12-10 07:47:59,638 -> 01.Indexing points,6.84,39429,50.0,14
2017-12-10 07:48:06,112 -> 02.Indexing centers,6.47,251084,50.0,14
2017-12-10 07:48:06,120 -> 1024
2017-12-10 07:48:06,126 -> 1024
2017-12-10 07:48:21,417 -> 03.Joining datasets,15.29,251080,50.0,14
Done!!! Sun Dec 10 07:48:21 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 07:48:24,265 -> Starting session,1.62,0
2017-12-10 07:48:24,266 -> Setting variables,0.00,0
2017-12-10 07:48:29,102 -> Reading datasets,4.84,0
2017-12-10 07:48:29,112 -> Points partitions: 2
2017-12-10 07:48:29,123 -> Centers partitions: 2
2017-12-10 07:48:36,692 -> 01.Indexing points,7.53,39429,50.0,14
2017-12-10 07:48:43,340 -> 02.Indexing centers,6.65,251084,50.0,14
2017-12-10 07:48:43,352 -> 1024
2017-12-10 07:48:43,361 -> 1024
2017-12-10 07:48:59,511 -> 03.Joining datasets,16.15,251080,50.0,14
Done!!! Sun Dec 10 07:48:59 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 07:49:02,271 -> Starting session,1.61,0
2017-12-10 07:49:02,272 -> Setting variables,0.00,0
2017-12-10 07:49:06,986 -> Reading datasets,4.71,0
2017-12-10 07:49:06,997 -> Points partitions: 2
2017-12-10 07:49:07,005 -> Centers partitions: 2
2017-12-10 07:49:14,071 -> 01.Indexing points,7.03,39429,50.0,14
2017-12-10 07:49:20,432 -> 02.Indexing centers,6.36,251084,50.0,14
2017-12-10 07:49:20,444 -> 1024
2017-12-10 07:49:20,451 -> 1024
2017-12-10 07:49:35,815 -> 03.Joining datasets,15.36,251080,50.0,14
Done!!! Sun Dec 10 07:49:36 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 07:49:44,272 -> Starting session,1.69,0
2017-12-10 07:49:44,273 -> Setting variables,0.00,0
2017-12-10 07:49:49,873 -> Reading datasets,5.60,0
2017-12-10 07:49:49,889 -> Points partitions: 2
2017-12-10 07:49:49,900 -> Centers partitions: 2
2017-12-10 07:49:56,227 -> 01.Indexing points,6.28,39429,50.0,21
2017-12-10 07:50:02,130 -> 02.Indexing centers,5.47,251084,50.0,21
2017-12-10 07:50:02,138 -> 1024
2017-12-10 07:50:02,144 -> 1024
2017-12-10 07:50:14,163 -> 03.Joining datasets,12.02,251080,50.0,21
Done!!! Sun Dec 10 07:50:14 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 07:50:17,101 -> Starting session,1.67,0
2017-12-10 07:50:17,101 -> Setting variables,0.00,0
2017-12-10 07:50:22,916 -> Reading datasets,5.81,0
2017-12-10 07:50:22,927 -> Points partitions: 2
2017-12-10 07:50:22,940 -> Centers partitions: 2
2017-12-10 07:50:29,243 -> 01.Indexing points,6.26,39429,50.0,21
2017-12-10 07:50:34,749 -> 02.Indexing centers,5.51,251084,50.0,21
2017-12-10 07:50:34,758 -> 1024
2017-12-10 07:50:34,764 -> 1024
2017-12-10 07:50:46,710 -> 03.Joining datasets,11.95,251080,50.0,21
Done!!! Sun Dec 10 07:50:48 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 07:50:50,481 -> Starting session,1.72,0
2017-12-10 07:50:50,481 -> Setting variables,0.00,0
2017-12-10 07:50:56,192 -> Reading datasets,5.71,0
2017-12-10 07:50:56,216 -> Points partitions: 2
2017-12-10 07:50:56,232 -> Centers partitions: 2
2017-12-10 07:51:02,817 -> 01.Indexing points,6.53,39429,50.0,21
2017-12-10 07:51:08,538 -> 02.Indexing centers,5.72,251084,50.0,21
2017-12-10 07:51:08,545 -> 1024
2017-12-10 07:51:08,552 -> 1024
2017-12-10 07:51:20,079 -> 03.Joining datasets,11.53,251080,50.0,21
Done!!! Sun Dec 10 07:51:21 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 07:51:23,686 -> Starting session,1.58,0
2017-12-10 07:51:23,686 -> Setting variables,0.00,0
2017-12-10 07:51:28,588 -> Reading datasets,4.90,0
2017-12-10 07:51:28,604 -> Points partitions: 2
2017-12-10 07:51:28,616 -> Centers partitions: 2
2017-12-10 07:51:36,072 -> 01.Indexing points,7.41,39429,50.0,21
2017-12-10 07:51:41,966 -> 02.Indexing centers,5.89,251084,50.0,21
2017-12-10 07:51:41,978 -> 1024
2017-12-10 07:51:41,985 -> 1024
2017-12-10 07:51:53,110 -> 03.Joining datasets,11.12,251080,50.0,21
Done!!! Sun Dec 10 07:51:54 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 07:51:56,578 -> Starting session,1.57,0
2017-12-10 07:51:56,579 -> Setting variables,0.00,0
2017-12-10 07:52:01,353 -> Reading datasets,4.77,0
2017-12-10 07:52:01,367 -> Points partitions: 2
2017-12-10 07:52:01,376 -> Centers partitions: 2
2017-12-10 07:52:08,407 -> 01.Indexing points,6.98,39429,50.0,21
2017-12-10 07:52:14,091 -> 02.Indexing centers,5.68,251084,50.0,21
2017-12-10 07:52:14,102 -> 1024
2017-12-10 07:52:14,110 -> 1024
2017-12-10 07:52:25,676 -> 03.Joining datasets,11.57,251080,50.0,21
Done!!! Sun Dec 10 07:52:26 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 07:52:34,968 -> Starting session,1.73,0
2017-12-10 07:52:34,970 -> Setting variables,0.00,0
2017-12-10 07:52:42,221 -> Reading datasets,7.25,0
2017-12-10 07:52:42,241 -> Points partitions: 2
2017-12-10 07:52:42,253 -> Centers partitions: 2
2017-12-10 07:52:48,438 -> 01.Indexing points,6.13,39429,50.0,28
2017-12-10 07:52:53,890 -> 02.Indexing centers,5.45,251084,50.0,28
2017-12-10 07:52:53,897 -> 1024
2017-12-10 07:52:53,903 -> 1024
2017-12-10 07:53:04,848 -> 03.Joining datasets,10.94,251080,50.0,28
Done!!! Sun Dec 10 07:53:06 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 07:53:08,402 -> Starting session,1.52,0
2017-12-10 07:53:08,403 -> Setting variables,0.00,0
2017-12-10 07:53:15,411 -> Reading datasets,7.01,0
2017-12-10 07:53:15,429 -> Points partitions: 2
2017-12-10 07:53:15,441 -> Centers partitions: 2
2017-12-10 07:53:22,194 -> 01.Indexing points,6.70,39429,50.0,28
2017-12-10 07:53:29,792 -> 02.Indexing centers,7.60,251084,50.0,28
2017-12-10 07:53:29,800 -> 1024
2017-12-10 07:53:29,805 -> 1024
2017-12-10 07:53:40,999 -> 03.Joining datasets,11.19,251080,50.0,28
Done!!! Sun Dec 10 07:53:41 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 07:53:43,793 -> Starting session,1.53,0
2017-12-10 07:53:43,794 -> Setting variables,0.00,0
2017-12-10 07:53:49,465 -> Reading datasets,5.67,0
2017-12-10 07:53:49,482 -> Points partitions: 2
2017-12-10 07:53:49,556 -> Centers partitions: 2
2017-12-10 07:53:55,838 -> 01.Indexing points,6.23,39429,50.0,28
2017-12-10 07:54:02,666 -> 02.Indexing centers,6.83,251084,50.0,28
2017-12-10 07:54:02,676 -> 1024
2017-12-10 07:54:02,685 -> 1024
2017-12-10 07:54:13,850 -> 03.Joining datasets,11.16,251080,50.0,28
Done!!! Sun Dec 10 07:54:14 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 07:54:16,761 -> Starting session,1.80,0
2017-12-10 07:54:16,762 -> Setting variables,0.00,0
2017-12-10 07:54:23,935 -> Reading datasets,7.17,0
2017-12-10 07:54:23,945 -> Points partitions: 2
2017-12-10 07:54:23,955 -> Centers partitions: 2
2017-12-10 07:54:30,311 -> 01.Indexing points,6.32,39429,50.0,28
2017-12-10 07:54:37,414 -> 02.Indexing centers,7.19,251084,50.0,28
2017-12-10 07:54:37,424 -> 1024
2017-12-10 07:54:37,433 -> 1024
2017-12-10 07:54:48,553 -> 03.Joining datasets,11.12,251080,50.0,28
Done!!! Sun Dec 10 07:54:49 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 07:54:51,338 -> Starting session,1.57,0
2017-12-10 07:54:51,338 -> Setting variables,0.00,0
2017-12-10 07:54:58,366 -> Reading datasets,7.03,0
2017-12-10 07:54:58,379 -> Points partitions: 2
2017-12-10 07:54:58,387 -> Centers partitions: 2
2017-12-10 07:55:04,643 -> 01.Indexing points,6.22,39429,50.0,28
2017-12-10 07:55:10,535 -> 02.Indexing centers,5.89,251084,50.0,28
2017-12-10 07:55:10,547 -> 1024
2017-12-10 07:55:10,556 -> 1024
2017-12-10 07:55:22,829 -> 03.Joining datasets,12.27,251080,50.0,28
Done!!! Sun Dec 10 07:55:24 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 07:55:32,081 -> Starting session,1.53,0
2017-12-10 07:55:32,081 -> Setting variables,0.00,0
2017-12-10 07:55:36,164 -> Reading datasets,4.08,0
2017-12-10 07:55:36,176 -> Points partitions: 2
2017-12-10 07:55:36,183 -> Centers partitions: 2
2017-12-10 07:55:42,752 -> 01.Indexing points,6.53,19715,50.0,7
2017-12-10 07:55:48,034 -> 02.Indexing centers,5.28,125542,50.0,7
2017-12-10 07:55:48,047 -> 992
2017-12-10 07:55:48,059 -> 1024
2017-12-10 07:56:02,863 -> 03.Joining datasets,14.80,125540,50.0,7
Done!!! Sun Dec 10 07:56:03 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 07:56:05,584 -> Starting session,1.53,0
2017-12-10 07:56:05,585 -> Setting variables,0.00,0
2017-12-10 07:56:09,619 -> Reading datasets,4.03,0
2017-12-10 07:56:09,638 -> Points partitions: 2
2017-12-10 07:56:09,651 -> Centers partitions: 2
2017-12-10 07:56:16,584 -> 01.Indexing points,6.89,19715,50.0,7
2017-12-10 07:56:21,819 -> 02.Indexing centers,5.23,125542,50.0,7
2017-12-10 07:56:21,829 -> 992
2017-12-10 07:56:21,837 -> 1024
2017-12-10 07:56:37,393 -> 03.Joining datasets,15.56,125540,50.0,7
Done!!! Sun Dec 10 07:56:37 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 07:56:40,061 -> Starting session,1.50,0
2017-12-10 07:56:40,061 -> Setting variables,0.00,0
2017-12-10 07:56:44,289 -> Reading datasets,4.23,0
2017-12-10 07:56:44,302 -> Points partitions: 2
2017-12-10 07:56:44,311 -> Centers partitions: 2
2017-12-10 07:56:51,006 -> 01.Indexing points,6.66,19715,50.0,7
2017-12-10 07:56:56,299 -> 02.Indexing centers,5.29,125542,50.0,7
2017-12-10 07:56:56,312 -> 992
2017-12-10 07:56:56,322 -> 1024
2017-12-10 07:57:11,791 -> 03.Joining datasets,15.47,125540,50.0,7
Done!!! Sun Dec 10 07:57:12 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 07:57:14,631 -> Starting session,1.61,0
2017-12-10 07:57:14,632 -> Setting variables,0.00,0
2017-12-10 07:57:18,764 -> Reading datasets,4.13,0
2017-12-10 07:57:18,777 -> Points partitions: 2
2017-12-10 07:57:18,788 -> Centers partitions: 2
2017-12-10 07:57:25,243 -> 01.Indexing points,6.41,19715,50.0,7
2017-12-10 07:57:30,304 -> 02.Indexing centers,5.06,125542,50.0,7
2017-12-10 07:57:30,314 -> 992
2017-12-10 07:57:30,321 -> 1024
2017-12-10 07:57:45,362 -> 03.Joining datasets,15.04,125540,50.0,7
Done!!! Sun Dec 10 07:57:45 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 07:57:47,933 -> Starting session,1.38,0
2017-12-10 07:57:47,934 -> Setting variables,0.00,0
2017-12-10 07:57:51,984 -> Reading datasets,4.05,0
2017-12-10 07:57:51,998 -> Points partitions: 2
2017-12-10 07:57:52,007 -> Centers partitions: 2
2017-12-10 07:57:58,910 -> 01.Indexing points,6.86,19715,50.0,7
2017-12-10 07:58:04,394 -> 02.Indexing centers,5.48,125542,50.0,7
2017-12-10 07:58:04,403 -> 992
2017-12-10 07:58:04,409 -> 1024
2017-12-10 07:58:19,332 -> 03.Joining datasets,14.98,125540,50.0,7
Done!!! Sun Dec 10 07:58:19 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 07:58:27,668 -> Starting session,1.66,0
2017-12-10 07:58:27,670 -> Setting variables,0.00,0
2017-12-10 07:58:32,317 -> Reading datasets,4.65,0
2017-12-10 07:58:32,327 -> Points partitions: 2
2017-12-10 07:58:32,335 -> Centers partitions: 2
2017-12-10 07:58:39,375 -> 01.Indexing points,7.00,19715,50.0,14
2017-12-10 07:58:44,807 -> 02.Indexing centers,5.43,125542,50.0,14
2017-12-10 07:58:44,816 -> 992
2017-12-10 07:58:44,822 -> 1024
2017-12-10 07:58:58,121 -> 03.Joining datasets,13.30,125540,50.0,14
Done!!! Sun Dec 10 07:58:58 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 07:59:01,018 -> Starting session,1.71,0
2017-12-10 07:59:01,019 -> Setting variables,0.00,0
2017-12-10 07:59:05,665 -> Reading datasets,4.65,0
2017-12-10 07:59:05,676 -> Points partitions: 2
2017-12-10 07:59:05,683 -> Centers partitions: 2
2017-12-10 07:59:12,181 -> 01.Indexing points,6.46,19715,50.0,14
2017-12-10 07:59:17,050 -> 02.Indexing centers,4.87,125542,50.0,14
2017-12-10 07:59:17,058 -> 992
2017-12-10 07:59:17,063 -> 1024
2017-12-10 07:59:29,662 -> 03.Joining datasets,12.60,125540,50.0,14
Done!!! Sun Dec 10 07:59:30 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 07:59:32,445 -> Starting session,1.59,0
2017-12-10 07:59:32,445 -> Setting variables,0.00,0
2017-12-10 07:59:37,040 -> Reading datasets,4.59,0
2017-12-10 07:59:37,054 -> Points partitions: 2
2017-12-10 07:59:37,065 -> Centers partitions: 2
2017-12-10 07:59:43,873 -> 01.Indexing points,6.76,19715,50.0,14
2017-12-10 07:59:49,062 -> 02.Indexing centers,5.19,125542,50.0,14
2017-12-10 07:59:49,074 -> 992
2017-12-10 07:59:49,082 -> 1024
2017-12-10 08:00:01,110 -> 03.Joining datasets,12.03,125540,50.0,14
Done!!! Sun Dec 10 08:00:01 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 08:00:03,883 -> Starting session,1.58,0
2017-12-10 08:00:03,884 -> Setting variables,0.00,0
2017-12-10 08:00:08,529 -> Reading datasets,4.64,0
2017-12-10 08:00:08,544 -> Points partitions: 2
2017-12-10 08:00:08,554 -> Centers partitions: 2
2017-12-10 08:00:15,463 -> 01.Indexing points,6.87,19715,50.0,14
2017-12-10 08:00:20,689 -> 02.Indexing centers,5.22,125542,50.0,14
2017-12-10 08:00:20,701 -> 992
2017-12-10 08:00:20,709 -> 1024
2017-12-10 08:00:32,981 -> 03.Joining datasets,12.27,125540,50.0,14
Done!!! Sun Dec 10 08:00:33 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 08:00:35,737 -> Starting session,1.53,0
2017-12-10 08:00:35,737 -> Setting variables,0.00,0
2017-12-10 08:00:40,172 -> Reading datasets,4.44,0
2017-12-10 08:00:40,186 -> Points partitions: 2
2017-12-10 08:00:40,193 -> Centers partitions: 2
2017-12-10 08:00:47,202 -> 01.Indexing points,6.97,19715,50.0,14
2017-12-10 08:00:52,462 -> 02.Indexing centers,5.26,125542,50.0,14
2017-12-10 08:00:52,472 -> 992
2017-12-10 08:00:52,479 -> 1024
2017-12-10 08:01:05,028 -> 03.Joining datasets,12.67,125540,50.0,14
Done!!! Sun Dec 10 08:01:06 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 08:01:14,171 -> Starting session,1.62,0
2017-12-10 08:01:14,173 -> Setting variables,0.00,0
2017-12-10 08:01:18,930 -> Reading datasets,4.76,0
2017-12-10 08:01:18,941 -> Points partitions: 2
2017-12-10 08:01:18,951 -> Centers partitions: 2
2017-12-10 08:01:26,630 -> 01.Indexing points,7.64,19715,50.0,21
2017-12-10 08:01:31,827 -> 02.Indexing centers,5.20,125542,50.0,21
2017-12-10 08:01:31,836 -> 992
2017-12-10 08:01:31,845 -> 1024
2017-12-10 08:01:43,474 -> 03.Joining datasets,11.63,125540,50.0,21
Done!!! Sun Dec 10 08:01:43 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 08:01:46,154 -> Starting session,1.57,0
2017-12-10 08:01:46,155 -> Setting variables,0.00,0
2017-12-10 08:01:51,880 -> Reading datasets,5.73,0
2017-12-10 08:01:51,897 -> Points partitions: 2
2017-12-10 08:01:51,907 -> Centers partitions: 2
2017-12-10 08:01:56,913 -> 01.Indexing points,4.96,19715,50.0,21
2017-12-10 08:02:02,339 -> 02.Indexing centers,5.42,125542,50.0,21
2017-12-10 08:02:02,351 -> 992
2017-12-10 08:02:02,360 -> 1024
2017-12-10 08:02:12,406 -> 03.Joining datasets,10.05,125540,50.0,21
Done!!! Sun Dec 10 08:02:12 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 08:02:15,077 -> Starting session,1.56,0
2017-12-10 08:02:15,077 -> Setting variables,0.00,0
2017-12-10 08:02:19,600 -> Reading datasets,4.52,0
2017-12-10 08:02:19,613 -> Points partitions: 2
2017-12-10 08:02:19,621 -> Centers partitions: 2
2017-12-10 08:02:27,591 -> 01.Indexing points,7.93,19715,50.0,21
2017-12-10 08:02:32,445 -> 02.Indexing centers,4.85,125542,50.0,21
2017-12-10 08:02:32,453 -> 992
2017-12-10 08:02:32,459 -> 1024
2017-12-10 08:02:43,329 -> 03.Joining datasets,10.87,125540,50.0,21
Done!!! Sun Dec 10 08:02:44 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 08:02:46,967 -> Starting session,1.62,0
2017-12-10 08:02:46,968 -> Setting variables,0.00,0
2017-12-10 08:02:51,500 -> Reading datasets,4.53,0
2017-12-10 08:02:51,510 -> Points partitions: 2
2017-12-10 08:02:51,521 -> Centers partitions: 2
2017-12-10 08:02:57,595 -> 01.Indexing points,6.03,19715,50.0,21
2017-12-10 08:03:02,135 -> 02.Indexing centers,4.54,125542,50.0,21
2017-12-10 08:03:02,145 -> 992
2017-12-10 08:03:02,153 -> 1024
2017-12-10 08:03:13,009 -> 03.Joining datasets,10.86,125540,50.0,21
Done!!! Sun Dec 10 08:03:13 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 08:03:15,710 -> Starting session,1.48,0
2017-12-10 08:03:15,710 -> Setting variables,0.00,0
2017-12-10 08:03:21,485 -> Reading datasets,5.77,0
2017-12-10 08:03:21,497 -> Points partitions: 2
2017-12-10 08:03:21,506 -> Centers partitions: 2
2017-12-10 08:03:28,474 -> 01.Indexing points,6.92,19715,50.0,21
2017-12-10 08:03:33,301 -> 02.Indexing centers,4.83,125542,50.0,21
2017-12-10 08:03:33,313 -> 992
2017-12-10 08:03:33,322 -> 1024
2017-12-10 08:03:44,776 -> 03.Joining datasets,11.45,125540,50.0,21
Done!!! Sun Dec 10 08:03:45 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 08:03:53,180 -> Starting session,1.74,0
2017-12-10 08:03:53,181 -> Setting variables,0.00,0
2017-12-10 08:04:00,074 -> Reading datasets,6.89,0
2017-12-10 08:04:00,090 -> Points partitions: 2
2017-12-10 08:04:00,101 -> Centers partitions: 2
2017-12-10 08:04:08,359 -> 01.Indexing points,8.20,19715,50.0,28
2017-12-10 08:04:13,800 -> 02.Indexing centers,5.44,125542,50.0,28
2017-12-10 08:04:13,810 -> 992
2017-12-10 08:04:13,816 -> 1024
2017-12-10 08:04:25,306 -> 03.Joining datasets,11.49,125540,50.0,28
Done!!! Sun Dec 10 08:04:25 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 08:04:28,043 -> Starting session,1.59,0
2017-12-10 08:04:28,044 -> Setting variables,0.00,0
2017-12-10 08:04:35,207 -> Reading datasets,7.16,0
2017-12-10 08:04:35,230 -> Points partitions: 2
2017-12-10 08:04:35,243 -> Centers partitions: 2
2017-12-10 08:04:43,141 -> 01.Indexing points,7.85,19715,50.0,28
2017-12-10 08:04:48,691 -> 02.Indexing centers,5.55,125542,50.0,28
2017-12-10 08:04:48,701 -> 992
2017-12-10 08:04:48,710 -> 1024
2017-12-10 08:05:01,288 -> 03.Joining datasets,12.58,125540,50.0,28
Done!!! Sun Dec 10 08:05:01 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 08:05:04,045 -> Starting session,1.64,0
2017-12-10 08:05:04,045 -> Setting variables,0.00,0
2017-12-10 08:05:10,053 -> Reading datasets,6.01,0
2017-12-10 08:05:10,070 -> Points partitions: 2
2017-12-10 08:05:10,081 -> Centers partitions: 2
2017-12-10 08:05:19,147 -> 01.Indexing points,9.02,19715,50.0,28
2017-12-10 08:05:24,117 -> 02.Indexing centers,4.97,125542,50.0,28
2017-12-10 08:05:24,126 -> 992
2017-12-10 08:05:24,132 -> 1024
2017-12-10 08:05:35,334 -> 03.Joining datasets,11.20,125540,50.0,28
Done!!! Sun Dec 10 08:05:35 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 08:05:38,206 -> Starting session,1.64,0
2017-12-10 08:05:38,207 -> Setting variables,0.00,0
2017-12-10 08:05:42,942 -> Reading datasets,4.74,0
2017-12-10 08:05:42,957 -> Points partitions: 2
2017-12-10 08:05:42,967 -> Centers partitions: 2
2017-12-10 08:05:52,212 -> 01.Indexing points,9.20,19715,50.0,28
2017-12-10 08:05:57,864 -> 02.Indexing centers,5.65,125542,50.0,28
2017-12-10 08:05:57,875 -> 992
2017-12-10 08:05:57,883 -> 1024
2017-12-10 08:06:10,548 -> 03.Joining datasets,12.66,125540,50.0,28
Done!!! Sun Dec 10 08:06:11 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 08:06:13,288 -> Starting session,1.51,0
2017-12-10 08:06:13,288 -> Setting variables,0.00,0
2017-12-10 08:06:19,982 -> Reading datasets,6.69,0
2017-12-10 08:06:20,004 -> Points partitions: 2
2017-12-10 08:06:20,021 -> Centers partitions: 2
2017-12-10 08:06:27,197 -> 01.Indexing points,7.12,19715,50.0,28
2017-12-10 08:06:32,511 -> 02.Indexing centers,5.31,125542,50.0,28
2017-12-10 08:06:32,523 -> 992
2017-12-10 08:06:32,532 -> 1024
2017-12-10 08:06:43,821 -> 03.Joining datasets,11.29,125540,50.0,28
Done!!! Sun Dec 10 08:06:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 08:06:52,295 -> Starting session,1.50,0
2017-12-10 08:06:52,296 -> Setting variables,0.00,0
2017-12-10 08:06:56,588 -> Reading datasets,4.29,0
2017-12-10 08:06:56,602 -> Points partitions: 2
2017-12-10 08:06:56,611 -> Centers partitions: 2
2017-12-10 08:07:04,445 -> 01.Indexing points,7.79,78857,40.0,7
2017-12-10 08:07:12,027 -> 02.Indexing centers,7.58,395352,40.0,7
2017-12-10 08:07:12,040 -> 1024
2017-12-10 08:07:12,049 -> 1024
2017-12-10 08:07:33,747 -> 03.Joining datasets,21.70,395346,40.0,7
Done!!! Sun Dec 10 08:07:34 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 08:07:36,457 -> Starting session,1.55,0
2017-12-10 08:07:36,458 -> Setting variables,0.00,0
2017-12-10 08:07:40,841 -> Reading datasets,4.38,0
2017-12-10 08:07:40,859 -> Points partitions: 2
2017-12-10 08:07:40,869 -> Centers partitions: 2
2017-12-10 08:07:48,500 -> 01.Indexing points,7.59,78857,40.0,7
2017-12-10 08:07:55,968 -> 02.Indexing centers,7.47,395352,40.0,7
2017-12-10 08:07:55,981 -> 1024
2017-12-10 08:07:55,990 -> 1024
2017-12-10 08:08:17,418 -> 03.Joining datasets,21.43,395346,40.0,7
Done!!! Sun Dec 10 08:08:17 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 08:08:20,172 -> Starting session,1.56,0
2017-12-10 08:08:20,173 -> Setting variables,0.00,0
2017-12-10 08:08:24,398 -> Reading datasets,4.23,0
2017-12-10 08:08:24,413 -> Points partitions: 2
2017-12-10 08:08:24,422 -> Centers partitions: 2
2017-12-10 08:08:31,681 -> 01.Indexing points,7.22,78857,40.0,7
2017-12-10 08:08:38,811 -> 02.Indexing centers,7.13,395352,40.0,7
2017-12-10 08:08:38,822 -> 1024
2017-12-10 08:08:38,831 -> 1024
2017-12-10 08:08:58,885 -> 03.Joining datasets,20.05,395346,40.0,7
Done!!! Sun Dec 10 08:08:59 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 08:09:01,656 -> Starting session,1.55,0
2017-12-10 08:09:01,656 -> Setting variables,0.00,0
2017-12-10 08:09:06,025 -> Reading datasets,4.37,0
2017-12-10 08:09:06,039 -> Points partitions: 2
2017-12-10 08:09:06,049 -> Centers partitions: 2
2017-12-10 08:09:14,016 -> 01.Indexing points,7.92,78857,40.0,7
2017-12-10 08:09:21,317 -> 02.Indexing centers,7.30,395352,40.0,7
2017-12-10 08:09:21,329 -> 1024
2017-12-10 08:09:21,336 -> 1024
2017-12-10 08:09:43,356 -> 03.Joining datasets,22.02,395346,40.0,7
Done!!! Sun Dec 10 08:09:43 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 08:09:46,044 -> Starting session,1.50,0
2017-12-10 08:09:46,044 -> Setting variables,0.00,0
2017-12-10 08:09:50,325 -> Reading datasets,4.28,0
2017-12-10 08:09:50,342 -> Points partitions: 2
2017-12-10 08:09:50,355 -> Centers partitions: 2
2017-12-10 08:09:58,428 -> 01.Indexing points,8.03,78857,40.0,7
2017-12-10 08:10:05,692 -> 02.Indexing centers,7.26,395352,40.0,7
2017-12-10 08:10:05,703 -> 1024
2017-12-10 08:10:05,711 -> 1024
2017-12-10 08:10:27,721 -> 03.Joining datasets,22.01,395346,40.0,7
Done!!! Sun Dec 10 08:10:28 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 08:10:36,053 -> Starting session,1.60,0
2017-12-10 08:10:36,054 -> Setting variables,0.00,0
2017-12-10 08:10:41,159 -> Reading datasets,5.10,0
2017-12-10 08:10:41,173 -> Points partitions: 2
2017-12-10 08:10:41,183 -> Centers partitions: 2
2017-12-10 08:10:49,131 -> 01.Indexing points,7.91,78857,40.0,14
2017-12-10 08:10:56,498 -> 02.Indexing centers,7.37,395352,40.0,14
2017-12-10 08:10:56,505 -> 1024
2017-12-10 08:10:56,511 -> 1024
2017-12-10 08:11:13,897 -> 03.Joining datasets,17.39,395346,40.0,14
Done!!! Sun Dec 10 08:11:14 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 08:11:16,746 -> Starting session,1.73,0
2017-12-10 08:11:16,749 -> Setting variables,0.00,0
2017-12-10 08:11:21,586 -> Reading datasets,4.84,0
2017-12-10 08:11:21,597 -> Points partitions: 2
2017-12-10 08:11:21,605 -> Centers partitions: 2
2017-12-10 08:11:29,189 -> 01.Indexing points,7.55,78857,40.0,14
2017-12-10 08:11:36,428 -> 02.Indexing centers,7.24,395352,40.0,14
2017-12-10 08:11:36,436 -> 1024
2017-12-10 08:11:36,443 -> 1024
2017-12-10 08:11:53,805 -> 03.Joining datasets,17.45,395346,40.0,14
Done!!! Sun Dec 10 08:11:54 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 08:11:56,658 -> Starting session,1.69,0
2017-12-10 08:11:56,659 -> Setting variables,0.00,0
2017-12-10 08:12:01,540 -> Reading datasets,4.88,0
2017-12-10 08:12:01,553 -> Points partitions: 2
2017-12-10 08:12:01,562 -> Centers partitions: 2
2017-12-10 08:12:09,395 -> 01.Indexing points,7.80,78857,40.0,14
2017-12-10 08:12:17,026 -> 02.Indexing centers,7.63,395352,40.0,14
2017-12-10 08:12:17,033 -> 1024
2017-12-10 08:12:17,039 -> 1024
2017-12-10 08:12:34,128 -> 03.Joining datasets,17.09,395346,40.0,14
Done!!! Sun Dec 10 08:12:34 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 08:12:36,905 -> Starting session,1.54,0
2017-12-10 08:12:36,906 -> Setting variables,0.00,0
2017-12-10 08:12:41,946 -> Reading datasets,5.04,0
2017-12-10 08:12:41,956 -> Points partitions: 2
2017-12-10 08:12:41,963 -> Centers partitions: 2
2017-12-10 08:12:49,586 -> 01.Indexing points,7.58,78857,40.0,14
2017-12-10 08:12:57,099 -> 02.Indexing centers,7.51,395352,40.0,14
2017-12-10 08:12:57,106 -> 1024
2017-12-10 08:12:57,112 -> 1024
2017-12-10 08:13:13,665 -> 03.Joining datasets,16.55,395346,40.0,14
Done!!! Sun Dec 10 08:13:14 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 08:13:16,428 -> Starting session,1.49,0
2017-12-10 08:13:16,429 -> Setting variables,0.00,0
2017-12-10 08:13:21,502 -> Reading datasets,5.07,0
2017-12-10 08:13:21,520 -> Points partitions: 2
2017-12-10 08:13:21,530 -> Centers partitions: 2
2017-12-10 08:13:29,015 -> 01.Indexing points,7.44,78857,40.0,14
2017-12-10 08:13:36,592 -> 02.Indexing centers,7.58,395352,40.0,14
2017-12-10 08:13:36,603 -> 1024
2017-12-10 08:13:36,609 -> 1024
2017-12-10 08:13:53,285 -> 03.Joining datasets,16.68,395346,40.0,14
Done!!! Sun Dec 10 08:13:53 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 08:14:01,667 -> Starting session,1.69,0
2017-12-10 08:14:01,667 -> Setting variables,0.00,0
2017-12-10 08:14:07,841 -> Reading datasets,6.17,0
2017-12-10 08:14:07,853 -> Points partitions: 2
2017-12-10 08:14:07,860 -> Centers partitions: 2
2017-12-10 08:14:14,964 -> 01.Indexing points,7.07,78857,40.0,21
2017-12-10 08:14:21,406 -> 02.Indexing centers,6.44,395352,40.0,21
2017-12-10 08:14:21,419 -> 1024
2017-12-10 08:14:21,428 -> 1024
2017-12-10 08:14:34,308 -> 03.Joining datasets,12.88,395346,40.0,21
Done!!! Sun Dec 10 08:14:35 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 08:14:37,913 -> Starting session,1.58,0
2017-12-10 08:14:37,914 -> Setting variables,0.00,0
2017-12-10 08:14:42,844 -> Reading datasets,4.93,0
2017-12-10 08:14:42,857 -> Points partitions: 2
2017-12-10 08:14:42,869 -> Centers partitions: 2
2017-12-10 08:14:50,584 -> 01.Indexing points,7.67,78857,40.0,21
2017-12-10 08:14:57,123 -> 02.Indexing centers,6.54,395352,40.0,21
2017-12-10 08:14:57,134 -> 1024
2017-12-10 08:14:57,142 -> 1024
2017-12-10 08:15:10,629 -> 03.Joining datasets,13.49,395346,40.0,21
Done!!! Sun Dec 10 08:15:11 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 08:15:13,603 -> Starting session,1.79,0
2017-12-10 08:15:13,604 -> Setting variables,0.00,0
2017-12-10 08:15:19,411 -> Reading datasets,5.81,0
2017-12-10 08:15:19,432 -> Points partitions: 2
2017-12-10 08:15:19,445 -> Centers partitions: 2
2017-12-10 08:15:26,453 -> 01.Indexing points,6.95,78857,40.0,21
2017-12-10 08:15:33,038 -> 02.Indexing centers,6.62,395352,40.0,21
2017-12-10 08:15:33,046 -> 1024
2017-12-10 08:15:33,053 -> 1024
2017-12-10 08:15:45,295 -> 03.Joining datasets,12.24,395346,40.0,21
Done!!! Sun Dec 10 08:15:45 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 08:15:48,144 -> Starting session,1.73,0
2017-12-10 08:15:48,145 -> Setting variables,0.00,0
2017-12-10 08:15:53,003 -> Reading datasets,4.86,0
2017-12-10 08:15:53,022 -> Points partitions: 2
2017-12-10 08:15:53,034 -> Centers partitions: 2
2017-12-10 08:16:00,976 -> 01.Indexing points,7.89,78857,40.0,21
2017-12-10 08:16:07,566 -> 02.Indexing centers,6.59,395352,40.0,21
2017-12-10 08:16:07,576 -> 1024
2017-12-10 08:16:07,585 -> 1024
2017-12-10 08:16:20,115 -> 03.Joining datasets,12.53,395346,40.0,21
Done!!! Sun Dec 10 08:16:20 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 08:16:22,932 -> Starting session,1.61,0
2017-12-10 08:16:22,932 -> Setting variables,0.00,0
2017-12-10 08:16:28,812 -> Reading datasets,5.88,0
2017-12-10 08:16:28,824 -> Points partitions: 2
2017-12-10 08:16:28,835 -> Centers partitions: 2
2017-12-10 08:16:35,941 -> 01.Indexing points,7.06,78857,40.0,21
2017-12-10 08:16:42,457 -> 02.Indexing centers,6.51,395352,40.0,21
2017-12-10 08:16:42,465 -> 1024
2017-12-10 08:16:42,471 -> 1024
2017-12-10 08:16:54,915 -> 03.Joining datasets,12.44,395346,40.0,21
Done!!! Sun Dec 10 08:16:55 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 08:17:03,368 -> Starting session,1.73,0
2017-12-10 08:17:03,369 -> Setting variables,0.00,0
2017-12-10 08:17:10,545 -> Reading datasets,7.18,0
2017-12-10 08:17:10,565 -> Points partitions: 2
2017-12-10 08:17:10,578 -> Centers partitions: 2
2017-12-10 08:17:17,792 -> 01.Indexing points,7.16,78857,40.0,28
2017-12-10 08:17:24,765 -> 02.Indexing centers,6.97,395352,40.0,28
2017-12-10 08:17:24,778 -> 1024
2017-12-10 08:17:24,788 -> 1024
2017-12-10 08:17:36,267 -> 03.Joining datasets,11.48,395346,40.0,28
Done!!! Sun Dec 10 08:17:37 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 08:17:39,972 -> Starting session,1.62,0
2017-12-10 08:17:39,973 -> Setting variables,0.00,0
2017-12-10 08:17:47,079 -> Reading datasets,7.11,0
2017-12-10 08:17:47,098 -> Points partitions: 2
2017-12-10 08:17:47,110 -> Centers partitions: 2
2017-12-10 08:17:55,563 -> 01.Indexing points,8.40,78857,40.0,28
2017-12-10 08:18:02,196 -> 02.Indexing centers,6.63,395352,40.0,28
2017-12-10 08:18:02,203 -> 1024
2017-12-10 08:18:02,209 -> 1024
2017-12-10 08:18:15,817 -> 03.Joining datasets,13.61,395346,40.0,28
Done!!! Sun Dec 10 08:18:16 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 08:18:18,735 -> Starting session,1.82,0
2017-12-10 08:18:18,736 -> Setting variables,0.00,0
2017-12-10 08:18:25,989 -> Reading datasets,7.25,0
2017-12-10 08:18:26,007 -> Points partitions: 2
2017-12-10 08:18:26,017 -> Centers partitions: 2
2017-12-10 08:18:34,030 -> 01.Indexing points,7.92,78857,40.0,28
2017-12-10 08:18:43,365 -> 02.Indexing centers,9.33,395352,40.0,28
2017-12-10 08:18:43,372 -> 1024
2017-12-10 08:18:43,378 -> 1024
2017-12-10 08:18:56,175 -> 03.Joining datasets,12.80,395346,40.0,28
Done!!! Sun Dec 10 08:18:56 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 08:18:58,971 -> Starting session,1.63,0
2017-12-10 08:18:58,972 -> Setting variables,0.00,0
2017-12-10 08:19:04,723 -> Reading datasets,5.75,0
2017-12-10 08:19:04,749 -> Points partitions: 2
2017-12-10 08:19:04,767 -> Centers partitions: 2
2017-12-10 08:19:14,236 -> 01.Indexing points,9.41,78857,40.0,28
2017-12-10 08:19:22,394 -> 02.Indexing centers,8.16,395352,40.0,28
2017-12-10 08:19:22,402 -> 1024
2017-12-10 08:19:22,408 -> 1024
2017-12-10 08:19:35,647 -> 03.Joining datasets,13.24,395346,40.0,28
Done!!! Sun Dec 10 08:19:36 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 08:19:38,528 -> Starting session,1.79,0
2017-12-10 08:19:38,529 -> Setting variables,0.00,0
2017-12-10 08:19:45,819 -> Reading datasets,7.29,0
2017-12-10 08:19:45,839 -> Points partitions: 2
2017-12-10 08:19:45,852 -> Centers partitions: 2
2017-12-10 08:19:53,450 -> 01.Indexing points,7.54,78857,40.0,28
2017-12-10 08:20:03,702 -> 02.Indexing centers,10.25,395352,40.0,28
2017-12-10 08:20:03,711 -> 1024
2017-12-10 08:20:03,717 -> 1024
2017-12-10 08:20:15,166 -> 03.Joining datasets,11.45,395346,40.0,28
Done!!! Sun Dec 10 08:20:16 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 08:20:24,406 -> Starting session,1.63,0
2017-12-10 08:20:24,406 -> Setting variables,0.00,0
2017-12-10 08:20:28,624 -> Reading datasets,4.22,0
2017-12-10 08:20:28,638 -> Points partitions: 2
2017-12-10 08:20:28,647 -> Centers partitions: 2
2017-12-10 08:20:36,210 -> 01.Indexing points,7.52,59143,40.0,7
2017-12-10 08:20:42,830 -> 02.Indexing centers,6.62,296514,40.0,7
2017-12-10 08:20:42,838 -> 1024
2017-12-10 08:20:42,843 -> 1024
2017-12-10 08:21:02,971 -> 03.Joining datasets,20.13,296510,40.0,7
Done!!! Sun Dec 10 08:21:03 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 08:21:05,694 -> Starting session,1.53,0
2017-12-10 08:21:05,695 -> Setting variables,0.00,0
2017-12-10 08:21:09,852 -> Reading datasets,4.16,0
2017-12-10 08:21:09,865 -> Points partitions: 2
2017-12-10 08:21:09,873 -> Centers partitions: 2
2017-12-10 08:21:16,888 -> 01.Indexing points,6.97,59143,40.0,7
2017-12-10 08:21:23,129 -> 02.Indexing centers,6.24,296514,40.0,7
2017-12-10 08:21:23,142 -> 1024
2017-12-10 08:21:23,150 -> 1024
2017-12-10 08:21:42,713 -> 03.Joining datasets,19.56,296510,40.0,7
Done!!! Sun Dec 10 08:21:43 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 08:21:45,452 -> Starting session,1.52,0
2017-12-10 08:21:45,452 -> Setting variables,0.00,0
2017-12-10 08:21:49,598 -> Reading datasets,4.15,0
2017-12-10 08:21:49,611 -> Points partitions: 2
2017-12-10 08:21:49,619 -> Centers partitions: 2
2017-12-10 08:21:56,774 -> 01.Indexing points,7.11,59143,40.0,7
2017-12-10 08:22:03,111 -> 02.Indexing centers,6.34,296514,40.0,7
2017-12-10 08:22:03,123 -> 1024
2017-12-10 08:22:03,132 -> 1024
2017-12-10 08:22:22,203 -> 03.Joining datasets,19.07,296510,40.0,7
Done!!! Sun Dec 10 08:22:22 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 08:22:24,848 -> Starting session,1.54,0
2017-12-10 08:22:24,849 -> Setting variables,0.00,0
2017-12-10 08:22:29,167 -> Reading datasets,4.32,0
2017-12-10 08:22:29,179 -> Points partitions: 2
2017-12-10 08:22:29,187 -> Centers partitions: 2
2017-12-10 08:22:36,645 -> 01.Indexing points,7.42,59143,40.0,7
2017-12-10 08:22:43,041 -> 02.Indexing centers,6.40,296514,40.0,7
2017-12-10 08:22:43,054 -> 1024
2017-12-10 08:22:43,063 -> 1024
2017-12-10 08:23:03,084 -> 03.Joining datasets,20.02,296510,40.0,7
Done!!! Sun Dec 10 08:23:03 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 08:23:05,657 -> Starting session,1.49,0
2017-12-10 08:23:05,657 -> Setting variables,0.00,0
2017-12-10 08:23:09,781 -> Reading datasets,4.12,0
2017-12-10 08:23:09,794 -> Points partitions: 2
2017-12-10 08:23:09,956 -> Centers partitions: 2
2017-12-10 08:23:17,123 -> 01.Indexing points,7.12,59143,40.0,7
2017-12-10 08:23:23,630 -> 02.Indexing centers,6.50,296514,40.0,7
2017-12-10 08:23:23,639 -> 1024
2017-12-10 08:23:23,647 -> 1024
2017-12-10 08:23:42,901 -> 03.Joining datasets,19.25,296510,40.0,7
Done!!! Sun Dec 10 08:23:43 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 08:23:51,320 -> Starting session,1.82,0
2017-12-10 08:23:51,322 -> Setting variables,0.00,0
2017-12-10 08:23:56,233 -> Reading datasets,4.91,0
2017-12-10 08:23:56,246 -> Points partitions: 2
2017-12-10 08:23:56,257 -> Centers partitions: 2
2017-12-10 08:24:03,849 -> 01.Indexing points,7.55,59143,40.0,14
2017-12-10 08:24:10,202 -> 02.Indexing centers,6.35,296514,40.0,14
2017-12-10 08:24:10,212 -> 1024
2017-12-10 08:24:10,218 -> 1024
2017-12-10 08:24:26,216 -> 03.Joining datasets,16.00,296510,40.0,14
Done!!! Sun Dec 10 08:24:26 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 08:24:29,090 -> Starting session,1.64,0
2017-12-10 08:24:29,090 -> Setting variables,0.00,0
2017-12-10 08:24:33,906 -> Reading datasets,4.82,0
2017-12-10 08:24:33,920 -> Points partitions: 2
2017-12-10 08:24:33,930 -> Centers partitions: 2
2017-12-10 08:24:41,296 -> 01.Indexing points,7.44,59143,40.0,14
2017-12-10 08:24:47,409 -> 02.Indexing centers,6.11,296514,40.0,14
2017-12-10 08:24:47,421 -> 1024
2017-12-10 08:24:47,429 -> 1024
2017-12-10 08:25:04,326 -> 03.Joining datasets,16.90,296510,40.0,14
Done!!! Sun Dec 10 08:25:04 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 08:25:07,314 -> Starting session,1.74,0
2017-12-10 08:25:07,315 -> Setting variables,0.00,0
2017-12-10 08:25:12,205 -> Reading datasets,4.89,0
2017-12-10 08:25:12,219 -> Points partitions: 2
2017-12-10 08:25:12,229 -> Centers partitions: 2
2017-12-10 08:25:19,781 -> 01.Indexing points,7.51,59143,40.0,14
2017-12-10 08:25:26,318 -> 02.Indexing centers,6.54,296514,40.0,14
2017-12-10 08:25:26,326 -> 1024
2017-12-10 08:25:26,333 -> 1024
2017-12-10 08:25:42,075 -> 03.Joining datasets,15.74,296510,40.0,14
Done!!! Sun Dec 10 08:25:42 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 08:25:44,745 -> Starting session,1.57,0
2017-12-10 08:25:44,745 -> Setting variables,0.00,0
2017-12-10 08:25:49,648 -> Reading datasets,4.90,0
2017-12-10 08:25:49,659 -> Points partitions: 2
2017-12-10 08:25:49,666 -> Centers partitions: 2
2017-12-10 08:25:57,159 -> 01.Indexing points,7.45,59143,40.0,14
2017-12-10 08:26:03,805 -> 02.Indexing centers,6.64,296514,40.0,14
2017-12-10 08:26:03,812 -> 1024
2017-12-10 08:26:03,818 -> 1024
2017-12-10 08:26:19,644 -> 03.Joining datasets,15.83,296510,40.0,14
Done!!! Sun Dec 10 08:26:20 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 08:26:23,150 -> Starting session,1.61,0
2017-12-10 08:26:23,151 -> Setting variables,0.00,0
2017-12-10 08:26:28,093 -> Reading datasets,4.94,0
2017-12-10 08:26:28,103 -> Points partitions: 2
2017-12-10 08:26:28,110 -> Centers partitions: 2
2017-12-10 08:26:35,421 -> 01.Indexing points,7.27,59143,40.0,14
2017-12-10 08:26:41,740 -> 02.Indexing centers,6.32,296514,40.0,14
2017-12-10 08:26:41,749 -> 1024
2017-12-10 08:26:41,756 -> 1024
2017-12-10 08:26:56,947 -> 03.Joining datasets,15.19,296510,40.0,14
Done!!! Sun Dec 10 08:26:57 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 08:27:05,213 -> Starting session,1.69,0
2017-12-10 08:27:05,214 -> Setting variables,0.00,0
2017-12-10 08:27:10,199 -> Reading datasets,4.99,0
2017-12-10 08:27:10,212 -> Points partitions: 2
2017-12-10 08:27:10,223 -> Centers partitions: 2
2017-12-10 08:27:17,519 -> 01.Indexing points,7.25,59143,40.0,21
2017-12-10 08:27:23,545 -> 02.Indexing centers,6.03,296514,40.0,21
2017-12-10 08:27:23,554 -> 1024
2017-12-10 08:27:23,561 -> 1024
2017-12-10 08:27:37,017 -> 03.Joining datasets,13.52,296510,40.0,21
Done!!! Sun Dec 10 08:27:38 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 08:27:40,653 -> Starting session,1.64,0
2017-12-10 08:27:40,653 -> Setting variables,0.00,0
2017-12-10 08:27:46,534 -> Reading datasets,5.88,0
2017-12-10 08:27:46,553 -> Points partitions: 2
2017-12-10 08:27:46,566 -> Centers partitions: 2
2017-12-10 08:27:53,805 -> 01.Indexing points,7.18,59143,40.0,21
2017-12-10 08:27:59,913 -> 02.Indexing centers,6.11,296514,40.0,21
2017-12-10 08:27:59,920 -> 1024
2017-12-10 08:27:59,926 -> 1024
2017-12-10 08:28:13,175 -> 03.Joining datasets,13.25,296510,40.0,21
Done!!! Sun Dec 10 08:28:13 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 08:28:16,152 -> Starting session,1.64,0
2017-12-10 08:28:16,152 -> Setting variables,0.00,0
2017-12-10 08:28:22,018 -> Reading datasets,5.87,0
2017-12-10 08:28:22,038 -> Points partitions: 2
2017-12-10 08:28:22,050 -> Centers partitions: 2
2017-12-10 08:28:28,611 -> 01.Indexing points,6.51,59143,40.0,21
2017-12-10 08:28:34,754 -> 02.Indexing centers,6.14,296514,40.0,21
2017-12-10 08:28:34,763 -> 1024
2017-12-10 08:28:34,769 -> 1024
2017-12-10 08:28:47,280 -> 03.Joining datasets,12.51,296510,40.0,21
Done!!! Sun Dec 10 08:28:47 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 08:28:50,103 -> Starting session,1.57,0
2017-12-10 08:28:50,104 -> Setting variables,0.00,0
2017-12-10 08:28:54,869 -> Reading datasets,4.77,0
2017-12-10 08:28:54,879 -> Points partitions: 2
2017-12-10 08:28:54,886 -> Centers partitions: 2
2017-12-10 08:29:02,151 -> 01.Indexing points,7.22,59143,40.0,21
2017-12-10 08:29:08,086 -> 02.Indexing centers,5.93,296514,40.0,21
2017-12-10 08:29:08,094 -> 1024
2017-12-10 08:29:08,102 -> 1024
2017-12-10 08:29:20,922 -> 03.Joining datasets,12.82,296510,40.0,21
Done!!! Sun Dec 10 08:29:22 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 08:29:24,679 -> Starting session,1.73,0
2017-12-10 08:29:24,680 -> Setting variables,0.00,0
2017-12-10 08:29:30,665 -> Reading datasets,5.99,0
2017-12-10 08:29:30,689 -> Points partitions: 2
2017-12-10 08:29:30,707 -> Centers partitions: 2
2017-12-10 08:29:37,658 -> 01.Indexing points,6.89,59143,40.0,21
2017-12-10 08:29:43,557 -> 02.Indexing centers,5.90,296514,40.0,21
2017-12-10 08:29:43,566 -> 1024
2017-12-10 08:29:43,572 -> 1024
2017-12-10 08:29:55,453 -> 03.Joining datasets,11.88,296510,40.0,21
Done!!! Sun Dec 10 08:29:55 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 08:30:04,015 -> Starting session,1.82,0
2017-12-10 08:30:04,015 -> Setting variables,0.00,0
2017-12-10 08:30:11,174 -> Reading datasets,7.16,0
2017-12-10 08:30:11,193 -> Points partitions: 2
2017-12-10 08:30:11,204 -> Centers partitions: 2
2017-12-10 08:30:19,204 -> 01.Indexing points,7.95,59143,40.0,28
2017-12-10 08:30:26,003 -> 02.Indexing centers,6.80,296514,40.0,28
2017-12-10 08:30:26,011 -> 1024
2017-12-10 08:30:26,017 -> 1024
2017-12-10 08:30:36,250 -> 03.Joining datasets,10.23,296510,40.0,28
Done!!! Sun Dec 10 08:30:36 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 08:30:38,933 -> Starting session,1.55,0
2017-12-10 08:30:38,933 -> Setting variables,0.00,0
2017-12-10 08:30:44,739 -> Reading datasets,5.81,0
2017-12-10 08:30:44,756 -> Points partitions: 2
2017-12-10 08:30:44,769 -> Centers partitions: 2
2017-12-10 08:30:53,300 -> 01.Indexing points,8.49,59143,40.0,28
2017-12-10 08:31:01,365 -> 02.Indexing centers,8.06,296514,40.0,28
2017-12-10 08:31:01,372 -> 1024
2017-12-10 08:31:01,379 -> 1024
2017-12-10 08:31:12,978 -> 03.Joining datasets,11.60,296510,40.0,28
Done!!! Sun Dec 10 08:31:13 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 08:31:15,697 -> Starting session,1.51,0
2017-12-10 08:31:15,697 -> Setting variables,0.00,0
2017-12-10 08:31:22,900 -> Reading datasets,7.20,0
2017-12-10 08:31:22,919 -> Points partitions: 2
2017-12-10 08:31:22,931 -> Centers partitions: 2
2017-12-10 08:31:30,889 -> 01.Indexing points,7.91,59143,40.0,28
2017-12-10 08:31:38,420 -> 02.Indexing centers,7.53,296514,40.0,28
2017-12-10 08:31:38,431 -> 1024
2017-12-10 08:31:38,438 -> 1024
2017-12-10 08:31:49,975 -> 03.Joining datasets,11.54,296510,40.0,28
Done!!! Sun Dec 10 08:31:50 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 08:31:52,726 -> Starting session,1.64,0
2017-12-10 08:31:52,727 -> Setting variables,0.00,0
2017-12-10 08:32:00,029 -> Reading datasets,7.38,0
2017-12-10 08:32:00,051 -> Points partitions: 2
2017-12-10 08:32:00,064 -> Centers partitions: 2
2017-12-10 08:32:06,797 -> 01.Indexing points,6.69,59143,40.0,28
2017-12-10 08:32:12,628 -> 02.Indexing centers,5.83,296514,40.0,28
2017-12-10 08:32:12,644 -> 1024
2017-12-10 08:32:12,656 -> 1024
2017-12-10 08:32:24,175 -> 03.Joining datasets,11.52,296510,40.0,28
Done!!! Sun Dec 10 08:32:24 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 08:32:26,838 -> Starting session,1.56,0
2017-12-10 08:32:26,839 -> Setting variables,0.00,0
2017-12-10 08:32:33,889 -> Reading datasets,7.05,0
2017-12-10 08:32:33,910 -> Points partitions: 2
2017-12-10 08:32:33,922 -> Centers partitions: 2
2017-12-10 08:32:42,323 -> 01.Indexing points,8.35,59143,40.0,28
2017-12-10 08:32:50,162 -> 02.Indexing centers,7.84,296514,40.0,28
2017-12-10 08:32:50,172 -> 1024
2017-12-10 08:32:50,179 -> 1024
2017-12-10 08:33:01,304 -> 03.Joining datasets,11.12,296510,40.0,28
Done!!! Sun Dec 10 08:33:02 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 08:33:10,397 -> Starting session,1.49,0
2017-12-10 08:33:10,397 -> Setting variables,0.00,0
2017-12-10 08:33:14,715 -> Reading datasets,4.32,0
2017-12-10 08:33:14,726 -> Points partitions: 2
2017-12-10 08:33:14,733 -> Centers partitions: 2
2017-12-10 08:33:22,165 -> 01.Indexing points,7.39,39429,40.0,7
2017-12-10 08:33:28,438 -> 02.Indexing centers,6.27,197676,40.0,7
2017-12-10 08:33:28,450 -> 1024
2017-12-10 08:33:28,459 -> 1024
2017-12-10 08:33:46,429 -> 03.Joining datasets,17.97,197673,40.0,7
Done!!! Sun Dec 10 08:33:46 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 08:33:49,187 -> Starting session,1.58,0
2017-12-10 08:33:49,188 -> Setting variables,0.00,0
2017-12-10 08:33:53,412 -> Reading datasets,4.22,0
2017-12-10 08:33:53,426 -> Points partitions: 2
2017-12-10 08:33:53,435 -> Centers partitions: 2
2017-12-10 08:34:00,731 -> 01.Indexing points,7.25,39429,40.0,7
2017-12-10 08:34:06,716 -> 02.Indexing centers,5.98,197676,40.0,7
2017-12-10 08:34:06,728 -> 1024
2017-12-10 08:34:06,736 -> 1024
2017-12-10 08:34:24,198 -> 03.Joining datasets,17.46,197673,40.0,7
Done!!! Sun Dec 10 08:34:24 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 08:34:26,972 -> Starting session,1.61,0
2017-12-10 08:34:26,973 -> Setting variables,0.00,0
2017-12-10 08:34:31,090 -> Reading datasets,4.12,0
2017-12-10 08:34:31,102 -> Points partitions: 2
2017-12-10 08:34:31,109 -> Centers partitions: 2
2017-12-10 08:34:38,275 -> 01.Indexing points,7.13,39429,40.0,7
2017-12-10 08:34:44,043 -> 02.Indexing centers,5.77,197676,40.0,7
2017-12-10 08:34:44,052 -> 1024
2017-12-10 08:34:44,058 -> 1024
2017-12-10 08:35:01,222 -> 03.Joining datasets,17.16,197673,40.0,7
Done!!! Sun Dec 10 08:35:01 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 08:35:03,800 -> Starting session,1.50,0
2017-12-10 08:35:03,801 -> Setting variables,0.00,0
2017-12-10 08:35:07,859 -> Reading datasets,4.06,0
2017-12-10 08:35:07,873 -> Points partitions: 2
2017-12-10 08:35:08,083 -> Centers partitions: 2
2017-12-10 08:35:15,016 -> 01.Indexing points,6.88,39429,40.0,7
2017-12-10 08:35:20,944 -> 02.Indexing centers,5.93,197676,40.0,7
2017-12-10 08:35:20,959 -> 1024
2017-12-10 08:35:20,971 -> 1024
2017-12-10 08:35:38,357 -> 03.Joining datasets,17.39,197673,40.0,7
Done!!! Sun Dec 10 08:35:38 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 08:35:41,106 -> Starting session,1.52,0
2017-12-10 08:35:41,107 -> Setting variables,0.00,0
2017-12-10 08:35:45,305 -> Reading datasets,4.20,0
2017-12-10 08:35:45,318 -> Points partitions: 2
2017-12-10 08:35:45,327 -> Centers partitions: 2
2017-12-10 08:35:52,285 -> 01.Indexing points,6.91,39429,40.0,7
2017-12-10 08:35:58,134 -> 02.Indexing centers,5.85,197676,40.0,7
2017-12-10 08:35:58,143 -> 1024
2017-12-10 08:35:58,150 -> 1024
2017-12-10 08:36:15,643 -> 03.Joining datasets,17.49,197673,40.0,7
Done!!! Sun Dec 10 08:36:16 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 08:36:24,532 -> Starting session,1.69,0
2017-12-10 08:36:24,533 -> Setting variables,0.00,0
2017-12-10 08:36:29,319 -> Reading datasets,4.79,0
2017-12-10 08:36:29,334 -> Points partitions: 2
2017-12-10 08:36:29,342 -> Centers partitions: 2
2017-12-10 08:36:36,284 -> 01.Indexing points,6.90,39429,40.0,14
2017-12-10 08:36:42,337 -> 02.Indexing centers,6.05,197676,40.0,14
2017-12-10 08:36:42,346 -> 1024
2017-12-10 08:36:42,352 -> 1024
2017-12-10 08:36:57,037 -> 03.Joining datasets,14.68,197673,40.0,14
Done!!! Sun Dec 10 08:36:57 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 08:36:59,778 -> Starting session,1.54,0
2017-12-10 08:36:59,778 -> Setting variables,0.00,0
2017-12-10 08:37:04,756 -> Reading datasets,4.98,0
2017-12-10 08:37:04,766 -> Points partitions: 2
2017-12-10 08:37:04,776 -> Centers partitions: 2
2017-12-10 08:37:11,690 -> 01.Indexing points,6.87,39429,40.0,14
2017-12-10 08:37:17,392 -> 02.Indexing centers,5.70,197676,40.0,14
2017-12-10 08:37:17,403 -> 1024
2017-12-10 08:37:17,413 -> 1024
2017-12-10 08:37:32,270 -> 03.Joining datasets,14.86,197673,40.0,14
Done!!! Sun Dec 10 08:37:32 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 08:37:35,207 -> Starting session,1.63,0
2017-12-10 08:37:35,207 -> Setting variables,0.00,0
2017-12-10 08:37:40,106 -> Reading datasets,4.90,0
2017-12-10 08:37:40,115 -> Points partitions: 2
2017-12-10 08:37:40,123 -> Centers partitions: 2
2017-12-10 08:37:47,082 -> 01.Indexing points,6.92,39429,40.0,14
2017-12-10 08:37:52,566 -> 02.Indexing centers,5.48,197676,40.0,14
2017-12-10 08:37:52,574 -> 1024
2017-12-10 08:37:52,580 -> 1024
2017-12-10 08:38:06,861 -> 03.Joining datasets,14.28,197673,40.0,14
Done!!! Sun Dec 10 08:38:07 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 08:38:09,786 -> Starting session,1.69,0
2017-12-10 08:38:09,787 -> Setting variables,0.00,0
2017-12-10 08:38:14,445 -> Reading datasets,4.66,0
2017-12-10 08:38:14,458 -> Points partitions: 2
2017-12-10 08:38:14,467 -> Centers partitions: 2
2017-12-10 08:38:21,302 -> 01.Indexing points,6.79,39429,40.0,14
2017-12-10 08:38:26,874 -> 02.Indexing centers,5.57,197676,40.0,14
2017-12-10 08:38:26,884 -> 1024
2017-12-10 08:38:26,894 -> 1024
2017-12-10 08:38:41,168 -> 03.Joining datasets,14.27,197673,40.0,14
Done!!! Sun Dec 10 08:38:41 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 08:38:43,966 -> Starting session,1.56,0
2017-12-10 08:38:43,966 -> Setting variables,0.00,0
2017-12-10 08:38:48,836 -> Reading datasets,4.87,0
2017-12-10 08:38:48,847 -> Points partitions: 2
2017-12-10 08:38:48,855 -> Centers partitions: 2
2017-12-10 08:38:56,007 -> 01.Indexing points,7.11,39429,40.0,14
2017-12-10 08:39:01,885 -> 02.Indexing centers,5.88,197676,40.0,14
2017-12-10 08:39:01,895 -> 1024
2017-12-10 08:39:01,904 -> 1024
2017-12-10 08:39:16,963 -> 03.Joining datasets,15.06,197673,40.0,14
Done!!! Sun Dec 10 08:39:17 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 08:39:25,277 -> Starting session,1.61,0
2017-12-10 08:39:25,279 -> Setting variables,0.00,0
2017-12-10 08:39:30,222 -> Reading datasets,4.94,0
2017-12-10 08:39:30,232 -> Points partitions: 2
2017-12-10 08:39:30,239 -> Centers partitions: 2
2017-12-10 08:39:37,877 -> 01.Indexing points,7.60,39429,40.0,21
2017-12-10 08:39:42,947 -> 02.Indexing centers,5.07,197676,40.0,21
2017-12-10 08:39:42,957 -> 1024
2017-12-10 08:39:42,967 -> 1024
2017-12-10 08:39:54,293 -> 03.Joining datasets,11.33,197673,40.0,21
Done!!! Sun Dec 10 08:39:54 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 08:39:57,012 -> Starting session,1.49,0
2017-12-10 08:39:57,012 -> Setting variables,0.00,0
2017-12-10 08:40:02,978 -> Reading datasets,5.97,0
2017-12-10 08:40:03,003 -> Points partitions: 2
2017-12-10 08:40:03,016 -> Centers partitions: 2
2017-12-10 08:40:09,664 -> 01.Indexing points,6.59,39429,40.0,21
2017-12-10 08:40:14,556 -> 02.Indexing centers,4.89,197676,40.0,21
2017-12-10 08:40:14,563 -> 1024
2017-12-10 08:40:14,569 -> 1024
2017-12-10 08:40:26,049 -> 03.Joining datasets,11.55,197673,40.0,21
Done!!! Sun Dec 10 08:40:26 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 08:40:28,952 -> Starting session,1.69,0
2017-12-10 08:40:28,953 -> Setting variables,0.00,0
2017-12-10 08:40:34,697 -> Reading datasets,5.74,0
2017-12-10 08:40:34,724 -> Points partitions: 2
2017-12-10 08:40:34,741 -> Centers partitions: 2
2017-12-10 08:40:40,880 -> 01.Indexing points,6.09,39429,40.0,21
2017-12-10 08:40:46,116 -> 02.Indexing centers,5.23,197676,40.0,21
2017-12-10 08:40:46,124 -> 1024
2017-12-10 08:40:46,130 -> 1024
2017-12-10 08:40:57,231 -> 03.Joining datasets,11.10,197673,40.0,21
Done!!! Sun Dec 10 08:40:57 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 08:41:00,065 -> Starting session,1.75,0
2017-12-10 08:41:00,065 -> Setting variables,0.00,0
2017-12-10 08:41:06,000 -> Reading datasets,5.93,0
2017-12-10 08:41:06,018 -> Points partitions: 2
2017-12-10 08:41:06,029 -> Centers partitions: 2
2017-12-10 08:41:12,855 -> 01.Indexing points,6.65,39429,40.0,21
2017-12-10 08:41:18,250 -> 02.Indexing centers,5.39,197676,40.0,21
2017-12-10 08:41:18,258 -> 1024
2017-12-10 08:41:18,264 -> 1024
2017-12-10 08:41:29,700 -> 03.Joining datasets,11.44,197673,40.0,21
Done!!! Sun Dec 10 08:41:30 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 08:41:32,638 -> Starting session,1.65,0
2017-12-10 08:41:32,639 -> Setting variables,0.00,0
2017-12-10 08:41:38,457 -> Reading datasets,5.82,0
2017-12-10 08:41:38,476 -> Points partitions: 2
2017-12-10 08:41:38,491 -> Centers partitions: 2
2017-12-10 08:41:44,609 -> 01.Indexing points,6.06,39429,40.0,21
2017-12-10 08:41:49,583 -> 02.Indexing centers,4.97,197676,40.0,21
2017-12-10 08:41:49,591 -> 1024
2017-12-10 08:41:49,596 -> 1024
2017-12-10 08:42:00,139 -> 03.Joining datasets,10.54,197673,40.0,21
Done!!! Sun Dec 10 08:42:00 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 08:42:08,647 -> Starting session,1.74,0
2017-12-10 08:42:08,647 -> Setting variables,0.00,0
2017-12-10 08:42:14,548 -> Reading datasets,5.90,0
2017-12-10 08:42:14,564 -> Points partitions: 2
2017-12-10 08:42:14,574 -> Centers partitions: 2
2017-12-10 08:42:23,238 -> 01.Indexing points,8.62,39429,40.0,28
2017-12-10 08:42:30,293 -> 02.Indexing centers,7.05,197676,40.0,28
2017-12-10 08:42:30,300 -> 1024
2017-12-10 08:42:30,306 -> 1024
2017-12-10 08:42:39,551 -> 03.Joining datasets,9.24,197673,40.0,28
Done!!! Sun Dec 10 08:42:40 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 08:42:42,277 -> Starting session,1.59,0
2017-12-10 08:42:42,278 -> Setting variables,0.00,0
2017-12-10 08:42:49,334 -> Reading datasets,7.06,0
2017-12-10 08:42:49,346 -> Points partitions: 2
2017-12-10 08:42:49,357 -> Centers partitions: 2
2017-12-10 08:42:56,010 -> 01.Indexing points,6.61,39429,40.0,28
2017-12-10 08:43:02,955 -> 02.Indexing centers,6.94,197676,40.0,28
2017-12-10 08:43:02,966 -> 1024
2017-12-10 08:43:02,972 -> 1024
2017-12-10 08:43:13,419 -> 03.Joining datasets,10.45,197673,40.0,28
Done!!! Sun Dec 10 08:43:13 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 08:43:16,238 -> Starting session,1.59,0
2017-12-10 08:43:16,239 -> Setting variables,0.00,0
2017-12-10 08:43:23,213 -> Reading datasets,6.97,0
2017-12-10 08:43:23,223 -> Points partitions: 2
2017-12-10 08:43:23,230 -> Centers partitions: 2
2017-12-10 08:43:29,500 -> 01.Indexing points,6.23,39429,40.0,28
2017-12-10 08:43:34,977 -> 02.Indexing centers,5.48,197676,40.0,28
2017-12-10 08:43:34,986 -> 1024
2017-12-10 08:43:34,994 -> 1024
2017-12-10 08:43:46,402 -> 03.Joining datasets,11.41,197673,40.0,28
Done!!! Sun Dec 10 08:43:46 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 08:43:49,099 -> Starting session,1.47,0
2017-12-10 08:43:49,100 -> Setting variables,0.00,0
2017-12-10 08:43:56,020 -> Reading datasets,6.92,0
2017-12-10 08:43:56,033 -> Points partitions: 2
2017-12-10 08:43:56,042 -> Centers partitions: 2
2017-12-10 08:44:02,872 -> 01.Indexing points,6.79,39429,40.0,28
2017-12-10 08:44:09,017 -> 02.Indexing centers,6.14,197676,40.0,28
2017-12-10 08:44:09,027 -> 1024
2017-12-10 08:44:09,036 -> 1024
2017-12-10 08:44:19,969 -> 03.Joining datasets,10.93,197673,40.0,28
Done!!! Sun Dec 10 08:44:20 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 08:44:22,741 -> Starting session,1.55,0
2017-12-10 08:44:22,741 -> Setting variables,0.00,0
2017-12-10 08:44:29,961 -> Reading datasets,7.22,0
2017-12-10 08:44:29,972 -> Points partitions: 2
2017-12-10 08:44:29,979 -> Centers partitions: 2
2017-12-10 08:44:37,102 -> 01.Indexing points,7.08,39429,40.0,28
2017-12-10 08:44:43,170 -> 02.Indexing centers,6.07,197676,40.0,28
2017-12-10 08:44:43,181 -> 1024
2017-12-10 08:44:43,190 -> 1024
2017-12-10 08:44:56,384 -> 03.Joining datasets,13.19,197673,40.0,28
Done!!! Sun Dec 10 08:44:56 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 08:45:04,924 -> Starting session,1.58,0
2017-12-10 08:45:04,925 -> Setting variables,0.00,0
2017-12-10 08:45:09,221 -> Reading datasets,4.30,0
2017-12-10 08:45:09,234 -> Points partitions: 2
2017-12-10 08:45:09,244 -> Centers partitions: 2
2017-12-10 08:45:15,954 -> 01.Indexing points,6.62,19715,40.0,7
2017-12-10 08:45:21,050 -> 02.Indexing centers,5.09,98838,40.0,7
2017-12-10 08:45:21,057 -> 992
2017-12-10 08:45:21,063 -> 1024
2017-12-10 08:45:35,752 -> 03.Joining datasets,14.69,98837,40.0,7
Done!!! Sun Dec 10 08:45:36 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 08:45:38,300 -> Starting session,1.47,0
2017-12-10 08:45:38,300 -> Setting variables,0.00,0
2017-12-10 08:45:42,626 -> Reading datasets,4.33,0
2017-12-10 08:45:42,639 -> Points partitions: 2
2017-12-10 08:45:42,649 -> Centers partitions: 2
2017-12-10 08:45:49,479 -> 01.Indexing points,6.79,19715,40.0,7
2017-12-10 08:45:54,841 -> 02.Indexing centers,5.36,98838,40.0,7
2017-12-10 08:45:54,854 -> 992
2017-12-10 08:45:54,862 -> 1024
2017-12-10 08:46:09,590 -> 03.Joining datasets,14.73,98837,40.0,7
Done!!! Sun Dec 10 08:46:10 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 08:46:12,177 -> Starting session,1.39,0
2017-12-10 08:46:12,177 -> Setting variables,0.00,0
2017-12-10 08:46:16,314 -> Reading datasets,4.14,0
2017-12-10 08:46:16,325 -> Points partitions: 2
2017-12-10 08:46:16,332 -> Centers partitions: 2
2017-12-10 08:46:23,279 -> 01.Indexing points,6.91,19715,40.0,7
2017-12-10 08:46:28,596 -> 02.Indexing centers,5.32,98838,40.0,7
2017-12-10 08:46:28,608 -> 992
2017-12-10 08:46:28,618 -> 1024
2017-12-10 08:46:44,255 -> 03.Joining datasets,15.64,98837,40.0,7
Done!!! Sun Dec 10 08:46:44 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 08:46:46,988 -> Starting session,1.55,0
2017-12-10 08:46:46,989 -> Setting variables,0.00,0
2017-12-10 08:46:51,061 -> Reading datasets,4.07,0
2017-12-10 08:46:51,073 -> Points partitions: 2
2017-12-10 08:46:51,081 -> Centers partitions: 2
2017-12-10 08:46:57,328 -> 01.Indexing points,6.21,19715,40.0,7
2017-12-10 08:47:02,275 -> 02.Indexing centers,4.95,98838,40.0,7
2017-12-10 08:47:02,286 -> 992
2017-12-10 08:47:02,295 -> 1024
2017-12-10 08:47:17,236 -> 03.Joining datasets,14.51,98837,40.0,7
Done!!! Sun Dec 10 08:47:17 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 08:47:19,980 -> Starting session,1.52,0
2017-12-10 08:47:19,981 -> Setting variables,0.00,0
2017-12-10 08:47:24,072 -> Reading datasets,4.09,0
2017-12-10 08:47:24,085 -> Points partitions: 2
2017-12-10 08:47:24,093 -> Centers partitions: 2
2017-12-10 08:47:30,902 -> 01.Indexing points,6.77,19715,40.0,7
2017-12-10 08:47:35,957 -> 02.Indexing centers,5.05,98838,40.0,7
2017-12-10 08:47:35,972 -> 992
2017-12-10 08:47:35,982 -> 1024
2017-12-10 08:47:50,772 -> 03.Joining datasets,14.79,98837,40.0,7
Done!!! Sun Dec 10 08:47:51 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 08:47:59,148 -> Starting session,1.78,0
2017-12-10 08:47:59,149 -> Setting variables,0.00,0
2017-12-10 08:48:03,778 -> Reading datasets,4.63,0
2017-12-10 08:48:03,789 -> Points partitions: 2
2017-12-10 08:48:03,797 -> Centers partitions: 2
2017-12-10 08:48:11,010 -> 01.Indexing points,7.18,19715,40.0,14
2017-12-10 08:48:15,789 -> 02.Indexing centers,4.78,98838,40.0,14
2017-12-10 08:48:15,799 -> 992
2017-12-10 08:48:15,807 -> 1024
2017-12-10 08:48:27,949 -> 03.Joining datasets,12.14,98837,40.0,14
Done!!! Sun Dec 10 08:48:28 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 08:48:30,685 -> Starting session,1.52,0
2017-12-10 08:48:30,686 -> Setting variables,0.00,0
2017-12-10 08:48:35,589 -> Reading datasets,4.90,0
2017-12-10 08:48:35,603 -> Points partitions: 2
2017-12-10 08:48:35,612 -> Centers partitions: 2
2017-12-10 08:48:43,026 -> 01.Indexing points,7.37,19715,40.0,14
2017-12-10 08:48:48,201 -> 02.Indexing centers,5.17,98838,40.0,14
2017-12-10 08:48:48,208 -> 992
2017-12-10 08:48:48,215 -> 1024
2017-12-10 08:49:00,533 -> 03.Joining datasets,12.32,98837,40.0,14
Done!!! Sun Dec 10 08:49:01 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 08:49:03,186 -> Starting session,1.55,0
2017-12-10 08:49:03,187 -> Setting variables,0.00,0
2017-12-10 08:49:08,066 -> Reading datasets,4.88,0
2017-12-10 08:49:08,079 -> Points partitions: 2
2017-12-10 08:49:08,086 -> Centers partitions: 2
2017-12-10 08:49:14,881 -> 01.Indexing points,6.76,19715,40.0,14
2017-12-10 08:49:19,826 -> 02.Indexing centers,4.94,98838,40.0,14
2017-12-10 08:49:19,837 -> 992
2017-12-10 08:49:19,846 -> 1024
2017-12-10 08:49:32,019 -> 03.Joining datasets,12.17,98837,40.0,14
Done!!! Sun Dec 10 08:49:32 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 08:49:34,890 -> Starting session,1.70,0
2017-12-10 08:49:34,890 -> Setting variables,0.00,0
2017-12-10 08:49:39,567 -> Reading datasets,4.68,0
2017-12-10 08:49:39,577 -> Points partitions: 2
2017-12-10 08:49:39,584 -> Centers partitions: 2
2017-12-10 08:49:46,737 -> 01.Indexing points,7.12,19715,40.0,14
2017-12-10 08:49:51,587 -> 02.Indexing centers,4.85,98838,40.0,14
2017-12-10 08:49:51,595 -> 992
2017-12-10 08:49:51,601 -> 1024
2017-12-10 08:50:03,062 -> 03.Joining datasets,11.46,98837,40.0,14
Done!!! Sun Dec 10 08:50:03 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 08:50:05,900 -> Starting session,1.63,0
2017-12-10 08:50:05,900 -> Setting variables,0.00,0
2017-12-10 08:50:10,533 -> Reading datasets,4.63,0
2017-12-10 08:50:10,546 -> Points partitions: 2
2017-12-10 08:50:10,554 -> Centers partitions: 2
2017-12-10 08:50:18,406 -> 01.Indexing points,7.81,19715,40.0,14
2017-12-10 08:50:22,890 -> 02.Indexing centers,4.48,98838,40.0,14
2017-12-10 08:50:22,898 -> 992
2017-12-10 08:50:22,904 -> 1024
2017-12-10 08:50:34,889 -> 03.Joining datasets,11.99,98837,40.0,14
Done!!! Sun Dec 10 08:50:35 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 08:50:43,304 -> Starting session,1.73,0
2017-12-10 08:50:43,306 -> Setting variables,0.00,0
2017-12-10 08:50:48,968 -> Reading datasets,5.66,0
2017-12-10 08:50:48,987 -> Points partitions: 2
2017-12-10 08:50:48,999 -> Centers partitions: 2
2017-12-10 08:50:53,924 -> 01.Indexing points,4.87,19715,40.0,21
2017-12-10 08:50:58,381 -> 02.Indexing centers,4.46,98838,40.0,21
2017-12-10 08:50:58,389 -> 992
2017-12-10 08:50:58,396 -> 1024
2017-12-10 08:51:08,639 -> 03.Joining datasets,10.24,98837,40.0,21
Done!!! Sun Dec 10 08:51:09 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 08:51:11,517 -> Starting session,1.63,0
2017-12-10 08:51:11,517 -> Setting variables,0.00,0
2017-12-10 08:51:17,207 -> Reading datasets,5.69,0
2017-12-10 08:51:17,219 -> Points partitions: 2
2017-12-10 08:51:17,226 -> Centers partitions: 2
2017-12-10 08:51:24,046 -> 01.Indexing points,6.78,19715,40.0,21
2017-12-10 08:51:28,369 -> 02.Indexing centers,4.32,98838,40.0,21
2017-12-10 08:51:28,381 -> 992
2017-12-10 08:51:28,391 -> 1024
2017-12-10 08:51:39,285 -> 03.Joining datasets,10.89,98837,40.0,21
Done!!! Sun Dec 10 08:51:39 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 08:51:42,044 -> Starting session,1.50,0
2017-12-10 08:51:42,044 -> Setting variables,0.00,0
2017-12-10 08:51:47,642 -> Reading datasets,5.60,0
2017-12-10 08:51:47,661 -> Points partitions: 2
2017-12-10 08:51:47,674 -> Centers partitions: 2
2017-12-10 08:51:52,707 -> 01.Indexing points,4.84,19715,40.0,21
2017-12-10 08:51:57,734 -> 02.Indexing centers,5.03,98838,40.0,21
2017-12-10 08:51:57,747 -> 992
2017-12-10 08:51:57,756 -> 1024
2017-12-10 08:52:07,850 -> 03.Joining datasets,10.09,98837,40.0,21
Done!!! Sun Dec 10 08:52:08 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 08:52:10,701 -> Starting session,1.75,0
2017-12-10 08:52:10,702 -> Setting variables,0.00,0
2017-12-10 08:52:16,157 -> Reading datasets,5.45,0
2017-12-10 08:52:16,179 -> Points partitions: 2
2017-12-10 08:52:16,194 -> Centers partitions: 2
2017-12-10 08:52:21,118 -> 01.Indexing points,4.88,19715,40.0,21
2017-12-10 08:52:25,852 -> 02.Indexing centers,4.73,98838,40.0,21
2017-12-10 08:52:25,864 -> 992
2017-12-10 08:52:25,872 -> 1024
2017-12-10 08:52:35,348 -> 03.Joining datasets,9.48,98837,40.0,21
Done!!! Sun Dec 10 08:52:35 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 08:52:38,165 -> Starting session,1.61,0
2017-12-10 08:52:38,165 -> Setting variables,0.00,0
2017-12-10 08:52:43,803 -> Reading datasets,5.64,0
2017-12-10 08:52:43,822 -> Points partitions: 2
2017-12-10 08:52:43,840 -> Centers partitions: 2
2017-12-10 08:52:51,855 -> 01.Indexing points,7.94,19715,40.0,21
2017-12-10 08:52:56,471 -> 02.Indexing centers,4.61,98838,40.0,21
2017-12-10 08:52:56,478 -> 992
2017-12-10 08:52:56,483 -> 1024
2017-12-10 08:53:07,856 -> 03.Joining datasets,11.37,98837,40.0,21
Done!!! Sun Dec 10 08:53:08 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 08:53:16,444 -> Starting session,1.90,0
2017-12-10 08:53:16,444 -> Setting variables,0.00,0
2017-12-10 08:53:23,304 -> Reading datasets,6.86,0
2017-12-10 08:53:23,324 -> Points partitions: 2
2017-12-10 08:53:23,337 -> Centers partitions: 2
2017-12-10 08:53:31,444 -> 01.Indexing points,8.06,19715,40.0,28
2017-12-10 08:53:37,325 -> 02.Indexing centers,5.88,98838,40.0,28
2017-12-10 08:53:37,336 -> 992
2017-12-10 08:53:37,345 -> 1024
2017-12-10 08:53:48,557 -> 03.Joining datasets,11.21,98837,40.0,28
Done!!! Sun Dec 10 08:53:49 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 08:53:51,433 -> Starting session,1.66,0
2017-12-10 08:53:51,433 -> Setting variables,0.00,0
2017-12-10 08:53:58,406 -> Reading datasets,6.97,0
2017-12-10 08:53:58,420 -> Points partitions: 2
2017-12-10 08:53:58,429 -> Centers partitions: 2
2017-12-10 08:54:06,466 -> 01.Indexing points,7.99,19715,40.0,28
2017-12-10 08:54:12,077 -> 02.Indexing centers,5.61,98838,40.0,28
2017-12-10 08:54:12,087 -> 992
2017-12-10 08:54:12,094 -> 1024
2017-12-10 08:54:23,457 -> 03.Joining datasets,11.36,98837,40.0,28
Done!!! Sun Dec 10 08:54:23 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 08:54:26,062 -> Starting session,1.56,0
2017-12-10 08:54:26,062 -> Setting variables,0.00,0
2017-12-10 08:54:33,001 -> Reading datasets,6.94,0
2017-12-10 08:54:33,018 -> Points partitions: 2
2017-12-10 08:54:33,028 -> Centers partitions: 2
2017-12-10 08:54:40,464 -> 01.Indexing points,7.39,19715,40.0,28
2017-12-10 08:54:45,355 -> 02.Indexing centers,4.89,98838,40.0,28
2017-12-10 08:54:45,366 -> 992
2017-12-10 08:54:45,374 -> 1024
2017-12-10 08:54:56,329 -> 03.Joining datasets,10.96,98837,40.0,28
Done!!! Sun Dec 10 08:54:56 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 08:54:59,112 -> Starting session,1.57,0
2017-12-10 08:54:59,112 -> Setting variables,0.00,0
2017-12-10 08:55:06,055 -> Reading datasets,6.94,0
2017-12-10 08:55:06,071 -> Points partitions: 2
2017-12-10 08:55:06,080 -> Centers partitions: 2
2017-12-10 08:55:13,932 -> 01.Indexing points,7.81,19715,40.0,28
2017-12-10 08:55:19,339 -> 02.Indexing centers,5.41,98838,40.0,28
2017-12-10 08:55:19,350 -> 992
2017-12-10 08:55:19,358 -> 1024
2017-12-10 08:55:29,637 -> 03.Joining datasets,10.28,98837,40.0,28
Done!!! Sun Dec 10 08:55:30 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 08:55:32,283 -> Starting session,1.56,0
2017-12-10 08:55:32,283 -> Setting variables,0.00,0
2017-12-10 08:55:39,157 -> Reading datasets,6.87,0
2017-12-10 08:55:39,177 -> Points partitions: 2
2017-12-10 08:55:39,195 -> Centers partitions: 2
2017-12-10 08:55:47,532 -> 01.Indexing points,8.28,19715,40.0,28
2017-12-10 08:55:52,284 -> 02.Indexing centers,4.75,98838,40.0,28
2017-12-10 08:55:52,298 -> 992
2017-12-10 08:55:52,308 -> 1024
2017-12-10 08:56:03,933 -> 03.Joining datasets,11.62,98837,40.0,28
Done!!! Sun Dec 10 08:56:04 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 08:56:12,410 -> Starting session,1.55,0
2017-12-10 08:56:12,410 -> Setting variables,0.00,0
2017-12-10 08:56:16,640 -> Reading datasets,4.23,0
2017-12-10 08:56:16,655 -> Points partitions: 2
2017-12-10 08:56:16,669 -> Centers partitions: 2
2017-12-10 08:56:24,748 -> 01.Indexing points,8.03,78857,30.0,7
2017-12-10 08:56:31,303 -> 02.Indexing centers,6.55,289496,30.0,7
2017-12-10 08:56:31,316 -> 1024
2017-12-10 08:56:31,324 -> 1024
2017-12-10 08:56:51,130 -> 03.Joining datasets,19.81,289486,30.0,7
Done!!! Sun Dec 10 08:56:51 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 08:56:53,858 -> Starting session,1.55,0
2017-12-10 08:56:53,859 -> Setting variables,0.00,0
2017-12-10 08:56:58,130 -> Reading datasets,4.27,0
2017-12-10 08:56:58,145 -> Points partitions: 2
2017-12-10 08:56:58,157 -> Centers partitions: 2
2017-12-10 08:57:06,238 -> 01.Indexing points,8.04,78857,30.0,7
2017-12-10 08:57:12,792 -> 02.Indexing centers,6.55,289496,30.0,7
2017-12-10 08:57:12,805 -> 1024
2017-12-10 08:57:12,814 -> 1024
2017-12-10 08:57:33,094 -> 03.Joining datasets,20.28,289486,30.0,7
Done!!! Sun Dec 10 08:57:33 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 08:57:35,807 -> Starting session,1.63,0
2017-12-10 08:57:35,808 -> Setting variables,0.00,0
2017-12-10 08:57:40,136 -> Reading datasets,4.33,0
2017-12-10 08:57:40,155 -> Points partitions: 2
2017-12-10 08:57:40,164 -> Centers partitions: 2
2017-12-10 08:57:47,308 -> 01.Indexing points,7.10,78857,30.0,7
2017-12-10 08:57:53,533 -> 02.Indexing centers,6.22,289496,30.0,7
2017-12-10 08:57:53,541 -> 1024
2017-12-10 08:57:53,546 -> 1024
2017-12-10 08:58:12,274 -> 03.Joining datasets,18.73,289486,30.0,7
Done!!! Sun Dec 10 08:58:12 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 08:58:15,023 -> Starting session,1.54,0
2017-12-10 08:58:15,023 -> Setting variables,0.00,0
2017-12-10 08:58:19,312 -> Reading datasets,4.29,0
2017-12-10 08:58:19,326 -> Points partitions: 2
2017-12-10 08:58:19,336 -> Centers partitions: 2
2017-12-10 08:58:26,587 -> 01.Indexing points,7.21,78857,30.0,7
2017-12-10 08:58:32,849 -> 02.Indexing centers,6.26,289496,30.0,7
2017-12-10 08:58:32,862 -> 1024
2017-12-10 08:58:32,872 -> 1024
2017-12-10 08:58:52,019 -> 03.Joining datasets,19.15,289486,30.0,7
Done!!! Sun Dec 10 08:58:52 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 08:58:54,690 -> Starting session,1.51,0
2017-12-10 08:58:54,691 -> Setting variables,0.00,0
2017-12-10 08:58:59,114 -> Reading datasets,4.42,0
2017-12-10 08:58:59,127 -> Points partitions: 2
2017-12-10 08:58:59,137 -> Centers partitions: 2
2017-12-10 08:59:06,742 -> 01.Indexing points,7.56,78857,30.0,7
2017-12-10 08:59:13,456 -> 02.Indexing centers,6.71,289496,30.0,7
2017-12-10 08:59:13,468 -> 1024
2017-12-10 08:59:13,476 -> 1024
2017-12-10 08:59:33,468 -> 03.Joining datasets,20.17,289486,30.0,7
Done!!! Sun Dec 10 08:59:33 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 08:59:41,850 -> Starting session,1.68,0
2017-12-10 08:59:41,852 -> Setting variables,0.00,0
2017-12-10 08:59:46,674 -> Reading datasets,4.82,0
2017-12-10 08:59:46,684 -> Points partitions: 2
2017-12-10 08:59:46,691 -> Centers partitions: 2
2017-12-10 08:59:54,557 -> 01.Indexing points,7.83,78857,30.0,14
2017-12-10 09:00:01,014 -> 02.Indexing centers,6.46,289496,30.0,14
2017-12-10 09:00:01,022 -> 1024
2017-12-10 09:00:01,029 -> 1024
2017-12-10 09:00:17,151 -> 03.Joining datasets,16.12,289486,30.0,14
Done!!! Sun Dec 10 09:00:17 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:00:19,859 -> Starting session,1.51,0
2017-12-10 09:00:19,859 -> Setting variables,0.00,0
2017-12-10 09:00:24,610 -> Reading datasets,4.75,0
2017-12-10 09:00:24,621 -> Points partitions: 2
2017-12-10 09:00:24,631 -> Centers partitions: 2
2017-12-10 09:00:32,322 -> 01.Indexing points,7.65,78857,30.0,14
2017-12-10 09:00:38,536 -> 02.Indexing centers,6.21,289496,30.0,14
2017-12-10 09:00:38,547 -> 1024
2017-12-10 09:00:38,557 -> 1024
2017-12-10 09:00:55,144 -> 03.Joining datasets,16.59,289486,30.0,14
Done!!! Sun Dec 10 09:00:56 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:00:58,810 -> Starting session,1.65,0
2017-12-10 09:00:58,813 -> Setting variables,0.00,0
2017-12-10 09:01:03,630 -> Reading datasets,4.82,0
2017-12-10 09:01:03,640 -> Points partitions: 2
2017-12-10 09:01:03,648 -> Centers partitions: 2
2017-12-10 09:01:11,355 -> 01.Indexing points,7.67,78857,30.0,14
2017-12-10 09:01:17,496 -> 02.Indexing centers,6.27,289496,30.0,14
2017-12-10 09:01:17,507 -> 1024
2017-12-10 09:01:17,513 -> 1024
2017-12-10 09:01:33,274 -> 03.Joining datasets,15.76,289486,30.0,14
Done!!! Sun Dec 10 09:01:33 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:01:35,908 -> Starting session,1.53,0
2017-12-10 09:01:35,909 -> Setting variables,0.00,0
2017-12-10 09:01:40,704 -> Reading datasets,4.79,0
2017-12-10 09:01:40,715 -> Points partitions: 2
2017-12-10 09:01:40,723 -> Centers partitions: 2
2017-12-10 09:01:47,950 -> 01.Indexing points,7.19,78857,30.0,14
2017-12-10 09:01:53,901 -> 02.Indexing centers,5.95,289496,30.0,14
2017-12-10 09:01:53,909 -> 1024
2017-12-10 09:01:53,916 -> 1024
2017-12-10 09:02:09,737 -> 03.Joining datasets,15.82,289486,30.0,14
Done!!! Sun Dec 10 09:02:10 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 09:02:12,439 -> Starting session,1.61,0
2017-12-10 09:02:12,439 -> Setting variables,0.00,0
2017-12-10 09:02:17,135 -> Reading datasets,4.70,0
2017-12-10 09:02:17,149 -> Points partitions: 2
2017-12-10 09:02:17,160 -> Centers partitions: 2
2017-12-10 09:02:24,908 -> 01.Indexing points,7.71,78857,30.0,14
2017-12-10 09:02:31,491 -> 02.Indexing centers,6.58,289496,30.0,14
2017-12-10 09:02:31,503 -> 1024
2017-12-10 09:02:31,512 -> 1024
2017-12-10 09:02:47,423 -> 03.Joining datasets,15.91,289486,30.0,14
Done!!! Sun Dec 10 09:02:47 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 09:02:55,766 -> Starting session,1.79,0
2017-12-10 09:02:55,767 -> Setting variables,0.00,0
2017-12-10 09:03:00,676 -> Reading datasets,4.91,0
2017-12-10 09:03:00,687 -> Points partitions: 2
2017-12-10 09:03:00,695 -> Centers partitions: 2
2017-12-10 09:03:08,658 -> 01.Indexing points,7.92,78857,30.0,21
2017-12-10 09:03:14,702 -> 02.Indexing centers,6.04,289496,30.0,21
2017-12-10 09:03:14,713 -> 1024
2017-12-10 09:03:14,722 -> 1024
2017-12-10 09:03:26,931 -> 03.Joining datasets,12.21,289486,30.0,21
Done!!! Sun Dec 10 09:03:27 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 09:03:29,787 -> Starting session,1.63,0
2017-12-10 09:03:29,788 -> Setting variables,0.00,0
2017-12-10 09:03:35,584 -> Reading datasets,5.80,0
2017-12-10 09:03:35,595 -> Points partitions: 2
2017-12-10 09:03:35,602 -> Centers partitions: 2
2017-12-10 09:03:42,798 -> 01.Indexing points,7.16,78857,30.0,21
2017-12-10 09:03:48,845 -> 02.Indexing centers,6.05,289496,30.0,21
2017-12-10 09:03:48,857 -> 1024
2017-12-10 09:03:48,866 -> 1024
2017-12-10 09:04:01,470 -> 03.Joining datasets,12.60,289486,30.0,21
Done!!! Sun Dec 10 09:04:01 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 09:04:04,338 -> Starting session,1.65,0
2017-12-10 09:04:04,339 -> Setting variables,0.00,0
2017-12-10 09:04:10,124 -> Reading datasets,5.79,0
2017-12-10 09:04:10,138 -> Points partitions: 2
2017-12-10 09:04:10,146 -> Centers partitions: 2
2017-12-10 09:04:17,237 -> 01.Indexing points,7.05,78857,30.0,21
2017-12-10 09:04:22,782 -> 02.Indexing centers,5.54,289496,30.0,21
2017-12-10 09:04:22,794 -> 1024
2017-12-10 09:04:22,802 -> 1024
2017-12-10 09:04:34,841 -> 03.Joining datasets,12.04,289486,30.0,21
Done!!! Sun Dec 10 09:04:35 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 09:04:37,519 -> Starting session,1.48,0
2017-12-10 09:04:37,519 -> Setting variables,0.00,0
2017-12-10 09:04:43,660 -> Reading datasets,6.14,0
2017-12-10 09:04:43,682 -> Points partitions: 2
2017-12-10 09:04:43,697 -> Centers partitions: 2
2017-12-10 09:04:50,614 -> 01.Indexing points,6.86,78857,30.0,21
2017-12-10 09:04:56,402 -> 02.Indexing centers,5.79,289496,30.0,21
2017-12-10 09:04:56,411 -> 1024
2017-12-10 09:04:56,419 -> 1024
2017-12-10 09:05:09,056 -> 03.Joining datasets,12.64,289486,30.0,21
Done!!! Sun Dec 10 09:05:09 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 09:05:11,822 -> Starting session,1.54,0
2017-12-10 09:05:11,822 -> Setting variables,0.00,0
2017-12-10 09:05:17,766 -> Reading datasets,5.94,0
2017-12-10 09:05:17,780 -> Points partitions: 2
2017-12-10 09:05:17,792 -> Centers partitions: 2
2017-12-10 09:05:24,257 -> 01.Indexing points,6.42,78857,30.0,21
2017-12-10 09:05:30,010 -> 02.Indexing centers,5.75,289496,30.0,21
2017-12-10 09:05:30,021 -> 1024
2017-12-10 09:05:30,030 -> 1024
2017-12-10 09:05:43,072 -> 03.Joining datasets,13.04,289486,30.0,21
Done!!! Sun Dec 10 09:05:43 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 09:05:51,319 -> Starting session,1.62,0
2017-12-10 09:05:51,319 -> Setting variables,0.00,0
2017-12-10 09:05:58,404 -> Reading datasets,7.08,0
2017-12-10 09:05:58,424 -> Points partitions: 2
2017-12-10 09:05:58,436 -> Centers partitions: 2
2017-12-10 09:06:06,759 -> 01.Indexing points,8.27,78857,30.0,28
2017-12-10 09:06:13,303 -> 02.Indexing centers,6.54,289496,30.0,28
2017-12-10 09:06:13,310 -> 1024
2017-12-10 09:06:13,317 -> 1024
2017-12-10 09:06:24,832 -> 03.Joining datasets,11.51,289486,30.0,28
Done!!! Sun Dec 10 09:06:25 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 09:06:27,698 -> Starting session,1.58,0
2017-12-10 09:06:27,698 -> Setting variables,0.00,0
2017-12-10 09:06:33,987 -> Reading datasets,6.29,0
2017-12-10 09:06:34,006 -> Points partitions: 2
2017-12-10 09:06:34,020 -> Centers partitions: 2
2017-12-10 09:06:42,501 -> 01.Indexing points,8.43,78857,30.0,28
2017-12-10 09:06:48,345 -> 02.Indexing centers,5.84,289496,30.0,28
2017-12-10 09:06:48,356 -> 1024
2017-12-10 09:06:48,364 -> 1024
2017-12-10 09:06:59,579 -> 03.Joining datasets,11.22,289486,30.0,28
Done!!! Sun Dec 10 09:07:00 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 09:07:02,444 -> Starting session,1.65,0
2017-12-10 09:07:02,445 -> Setting variables,0.00,0
2017-12-10 09:07:09,506 -> Reading datasets,7.17,0
2017-12-10 09:07:09,525 -> Points partitions: 2
2017-12-10 09:07:09,538 -> Centers partitions: 2
2017-12-10 09:07:18,015 -> 01.Indexing points,8.43,78857,30.0,28
2017-12-10 09:07:26,084 -> 02.Indexing centers,8.07,289496,30.0,28
2017-12-10 09:07:26,093 -> 1024
2017-12-10 09:07:26,100 -> 1024
2017-12-10 09:07:36,817 -> 03.Joining datasets,10.72,289486,30.0,28
Done!!! Sun Dec 10 09:07:37 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 09:07:39,489 -> Starting session,1.58,0
2017-12-10 09:07:39,490 -> Setting variables,0.00,0
2017-12-10 09:07:45,153 -> Reading datasets,5.66,0
2017-12-10 09:07:45,172 -> Points partitions: 2
2017-12-10 09:07:45,184 -> Centers partitions: 2
2017-12-10 09:07:54,178 -> 01.Indexing points,8.94,78857,30.0,28
2017-12-10 09:08:00,108 -> 02.Indexing centers,5.93,289496,30.0,28
2017-12-10 09:08:00,119 -> 1024
2017-12-10 09:08:00,128 -> 1024
2017-12-10 09:08:11,489 -> 03.Joining datasets,11.36,289486,30.0,28
Done!!! Sun Dec 10 09:08:11 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 09:08:14,139 -> Starting session,1.46,0
2017-12-10 09:08:14,140 -> Setting variables,0.00,0
2017-12-10 09:08:19,959 -> Reading datasets,5.82,0
2017-12-10 09:08:19,979 -> Points partitions: 2
2017-12-10 09:08:19,993 -> Centers partitions: 2
2017-12-10 09:08:29,577 -> 01.Indexing points,9.53,78857,30.0,28
2017-12-10 09:08:34,873 -> 02.Indexing centers,5.29,289496,30.0,28
2017-12-10 09:08:34,882 -> 1024
2017-12-10 09:08:34,889 -> 1024
2017-12-10 09:08:46,461 -> 03.Joining datasets,11.57,289486,30.0,28
Done!!! Sun Dec 10 09:08:47 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 09:08:54,968 -> Starting session,1.54,0
2017-12-10 09:08:54,969 -> Setting variables,0.00,0
2017-12-10 09:08:59,277 -> Reading datasets,4.31,0
2017-12-10 09:08:59,289 -> Points partitions: 2
2017-12-10 09:08:59,298 -> Centers partitions: 2
2017-12-10 09:09:06,540 -> 01.Indexing points,7.20,59143,30.0,7
2017-12-10 09:09:12,211 -> 02.Indexing centers,5.67,217122,30.0,7
2017-12-10 09:09:12,222 -> 1024
2017-12-10 09:09:12,230 -> 1024
2017-12-10 09:09:30,065 -> 03.Joining datasets,17.84,217115,30.0,7
Done!!! Sun Dec 10 09:09:30 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 09:09:32,770 -> Starting session,1.52,0
2017-12-10 09:09:32,770 -> Setting variables,0.00,0
2017-12-10 09:09:36,912 -> Reading datasets,4.14,0
2017-12-10 09:09:36,923 -> Points partitions: 2
2017-12-10 09:09:36,930 -> Centers partitions: 2
2017-12-10 09:09:44,338 -> 01.Indexing points,7.37,59143,30.0,7
2017-12-10 09:09:50,210 -> 02.Indexing centers,5.87,217122,30.0,7
2017-12-10 09:09:50,221 -> 1024
2017-12-10 09:09:50,229 -> 1024
2017-12-10 09:10:08,649 -> 03.Joining datasets,18.42,217115,30.0,7
Done!!! Sun Dec 10 09:10:09 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 09:10:11,199 -> Starting session,1.36,0
2017-12-10 09:10:11,199 -> Setting variables,0.00,0
2017-12-10 09:10:15,358 -> Reading datasets,4.16,0
2017-12-10 09:10:15,371 -> Points partitions: 2
2017-12-10 09:10:15,380 -> Centers partitions: 2
2017-12-10 09:10:23,278 -> 01.Indexing points,7.86,59143,30.0,7
2017-12-10 09:10:29,114 -> 02.Indexing centers,5.83,217122,30.0,7
2017-12-10 09:10:29,123 -> 1024
2017-12-10 09:10:29,130 -> 1024
2017-12-10 09:10:47,560 -> 03.Joining datasets,18.43,217115,30.0,7
Done!!! Sun Dec 10 09:10:48 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 09:10:50,364 -> Starting session,1.55,0
2017-12-10 09:10:50,364 -> Setting variables,0.00,0
2017-12-10 09:10:54,563 -> Reading datasets,4.20,0
2017-12-10 09:10:54,578 -> Points partitions: 2
2017-12-10 09:10:54,585 -> Centers partitions: 2
2017-12-10 09:11:02,286 -> 01.Indexing points,7.66,59143,30.0,7
2017-12-10 09:11:08,334 -> 02.Indexing centers,6.05,217122,30.0,7
2017-12-10 09:11:08,345 -> 1024
2017-12-10 09:11:08,353 -> 1024
2017-12-10 09:11:26,434 -> 03.Joining datasets,18.08,217115,30.0,7
Done!!! Sun Dec 10 09:11:26 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 09:11:29,224 -> Starting session,1.53,0
2017-12-10 09:11:29,225 -> Setting variables,0.00,0
2017-12-10 09:11:33,351 -> Reading datasets,4.13,0
2017-12-10 09:11:33,364 -> Points partitions: 2
2017-12-10 09:11:33,375 -> Centers partitions: 2
2017-12-10 09:11:40,605 -> 01.Indexing points,7.19,59143,30.0,7
2017-12-10 09:11:46,589 -> 02.Indexing centers,5.98,217122,30.0,7
2017-12-10 09:11:46,601 -> 1024
2017-12-10 09:11:46,609 -> 1024
2017-12-10 09:12:04,978 -> 03.Joining datasets,18.37,217115,30.0,7
Done!!! Sun Dec 10 09:12:05 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 09:12:13,408 -> Starting session,1.70,0
2017-12-10 09:12:13,409 -> Setting variables,0.00,0
2017-12-10 09:12:18,291 -> Reading datasets,4.88,0
2017-12-10 09:12:18,304 -> Points partitions: 2
2017-12-10 09:12:18,311 -> Centers partitions: 2
2017-12-10 09:12:26,113 -> 01.Indexing points,7.76,59143,30.0,14
2017-12-10 09:12:32,074 -> 02.Indexing centers,5.96,217122,30.0,14
2017-12-10 09:12:32,083 -> 1024
2017-12-10 09:12:32,090 -> 1024
2017-12-10 09:12:47,118 -> 03.Joining datasets,15.03,217115,30.0,14
Done!!! Sun Dec 10 09:12:47 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:12:49,856 -> Starting session,1.55,0
2017-12-10 09:12:49,857 -> Setting variables,0.00,0
2017-12-10 09:12:54,543 -> Reading datasets,4.69,0
2017-12-10 09:12:54,558 -> Points partitions: 2
2017-12-10 09:12:54,570 -> Centers partitions: 2
2017-12-10 09:13:01,708 -> 01.Indexing points,7.10,59143,30.0,14
2017-12-10 09:13:07,326 -> 02.Indexing centers,5.62,217122,30.0,14
2017-12-10 09:13:07,334 -> 1024
2017-12-10 09:13:07,341 -> 1024
2017-12-10 09:13:22,022 -> 03.Joining datasets,14.68,217115,30.0,14
Done!!! Sun Dec 10 09:13:23 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:13:25,588 -> Starting session,1.57,0
2017-12-10 09:13:25,589 -> Setting variables,0.00,0
2017-12-10 09:13:30,342 -> Reading datasets,4.75,0
2017-12-10 09:13:30,354 -> Points partitions: 2
2017-12-10 09:13:30,363 -> Centers partitions: 2
2017-12-10 09:13:38,065 -> 01.Indexing points,7.66,59143,30.0,14
2017-12-10 09:13:43,862 -> 02.Indexing centers,5.79,217122,30.0,14
2017-12-10 09:13:43,870 -> 1024
2017-12-10 09:13:43,876 -> 1024
2017-12-10 09:13:57,603 -> 03.Joining datasets,13.73,217115,30.0,14
Done!!! Sun Dec 10 09:13:58 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:14:00,318 -> Starting session,1.46,0
2017-12-10 09:14:00,319 -> Setting variables,0.00,0
2017-12-10 09:14:05,130 -> Reading datasets,4.81,0
2017-12-10 09:14:05,144 -> Points partitions: 2
2017-12-10 09:14:05,152 -> Centers partitions: 2
2017-12-10 09:14:12,946 -> 01.Indexing points,7.76,59143,30.0,14
2017-12-10 09:14:18,487 -> 02.Indexing centers,5.54,217122,30.0,14
2017-12-10 09:14:18,499 -> 1024
2017-12-10 09:14:18,508 -> 1024
2017-12-10 09:14:32,692 -> 03.Joining datasets,14.18,217115,30.0,14
Done!!! Sun Dec 10 09:14:33 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 09:14:35,358 -> Starting session,1.60,0
2017-12-10 09:14:35,359 -> Setting variables,0.00,0
2017-12-10 09:14:40,189 -> Reading datasets,4.83,0
2017-12-10 09:14:40,202 -> Points partitions: 2
2017-12-10 09:14:40,209 -> Centers partitions: 2
2017-12-10 09:14:47,658 -> 01.Indexing points,7.41,59143,30.0,14
2017-12-10 09:14:53,458 -> 02.Indexing centers,5.80,217122,30.0,14
2017-12-10 09:14:53,470 -> 1024
2017-12-10 09:14:53,478 -> 1024
2017-12-10 09:15:08,925 -> 03.Joining datasets,15.45,217115,30.0,14
Done!!! Sun Dec 10 09:15:09 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 09:15:17,298 -> Starting session,1.65,0
2017-12-10 09:15:17,300 -> Setting variables,0.00,0
2017-12-10 09:15:23,342 -> Reading datasets,6.04,0
2017-12-10 09:15:23,361 -> Points partitions: 2
2017-12-10 09:15:23,373 -> Centers partitions: 2
2017-12-10 09:15:30,112 -> 01.Indexing points,6.69,59143,30.0,21
2017-12-10 09:15:35,658 -> 02.Indexing centers,5.54,217122,30.0,21
2017-12-10 09:15:35,675 -> 1024
2017-12-10 09:15:35,686 -> 1024
2017-12-10 09:15:46,807 -> 03.Joining datasets,11.12,217115,30.0,21
Done!!! Sun Dec 10 09:15:47 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 09:15:49,855 -> Starting session,1.74,0
2017-12-10 09:15:49,855 -> Setting variables,0.00,0
2017-12-10 09:15:55,529 -> Reading datasets,5.67,0
2017-12-10 09:15:55,540 -> Points partitions: 2
2017-12-10 09:15:55,547 -> Centers partitions: 2
2017-12-10 09:16:02,024 -> 01.Indexing points,6.32,59143,30.0,21
2017-12-10 09:16:07,238 -> 02.Indexing centers,5.21,217122,30.0,21
2017-12-10 09:16:07,246 -> 1024
2017-12-10 09:16:07,251 -> 1024
2017-12-10 09:16:18,275 -> 03.Joining datasets,11.02,217115,30.0,21
Done!!! Sun Dec 10 09:16:18 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 09:16:20,900 -> Starting session,1.56,0
2017-12-10 09:16:20,901 -> Setting variables,0.00,0
2017-12-10 09:16:25,888 -> Reading datasets,4.99,0
2017-12-10 09:16:25,899 -> Points partitions: 2
2017-12-10 09:16:25,906 -> Centers partitions: 2
2017-12-10 09:16:33,606 -> 01.Indexing points,7.66,59143,30.0,21
2017-12-10 09:16:38,597 -> 02.Indexing centers,4.99,217122,30.0,21
2017-12-10 09:16:38,605 -> 1024
2017-12-10 09:16:38,612 -> 1024
2017-12-10 09:16:49,762 -> 03.Joining datasets,11.15,217115,30.0,21
Done!!! Sun Dec 10 09:16:50 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 09:16:52,489 -> Starting session,1.57,0
2017-12-10 09:16:52,489 -> Setting variables,0.00,0
2017-12-10 09:16:58,298 -> Reading datasets,5.81,0
2017-12-10 09:16:58,317 -> Points partitions: 2
2017-12-10 09:16:58,331 -> Centers partitions: 2
2017-12-10 09:17:05,348 -> 01.Indexing points,6.97,59143,30.0,21
2017-12-10 09:17:10,480 -> 02.Indexing centers,5.13,217122,30.0,21
2017-12-10 09:17:10,489 -> 1024
2017-12-10 09:17:10,499 -> 1024
2017-12-10 09:17:21,637 -> 03.Joining datasets,11.14,217115,30.0,21
Done!!! Sun Dec 10 09:17:22 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 09:17:24,386 -> Starting session,1.52,0
2017-12-10 09:17:24,386 -> Setting variables,0.00,0
2017-12-10 09:17:30,281 -> Reading datasets,5.89,0
2017-12-10 09:17:30,294 -> Points partitions: 2
2017-12-10 09:17:30,304 -> Centers partitions: 2
2017-12-10 09:17:36,913 -> 01.Indexing points,6.57,59143,30.0,21
2017-12-10 09:17:41,907 -> 02.Indexing centers,4.99,217122,30.0,21
2017-12-10 09:17:41,918 -> 1024
2017-12-10 09:17:41,926 -> 1024
2017-12-10 09:17:53,405 -> 03.Joining datasets,11.48,217115,30.0,21
Done!!! Sun Dec 10 09:17:53 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 09:18:01,826 -> Starting session,1.68,0
2017-12-10 09:18:01,826 -> Setting variables,0.00,0
2017-12-10 09:18:08,610 -> Reading datasets,6.78,0
2017-12-10 09:18:08,632 -> Points partitions: 2
2017-12-10 09:18:08,648 -> Centers partitions: 2
2017-12-10 09:18:16,128 -> 01.Indexing points,7.42,59143,30.0,28
2017-12-10 09:18:21,545 -> 02.Indexing centers,5.42,217122,30.0,28
2017-12-10 09:18:21,556 -> 1024
2017-12-10 09:18:21,565 -> 1024
2017-12-10 09:18:32,401 -> 03.Joining datasets,10.84,217115,30.0,28
Done!!! Sun Dec 10 09:18:32 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 09:18:35,033 -> Starting session,1.53,0
2017-12-10 09:18:35,034 -> Setting variables,0.00,0
2017-12-10 09:18:42,103 -> Reading datasets,7.07,0
2017-12-10 09:18:42,123 -> Points partitions: 2
2017-12-10 09:18:42,140 -> Centers partitions: 2
2017-12-10 09:18:50,274 -> 01.Indexing points,8.08,59143,30.0,28
2017-12-10 09:18:55,693 -> 02.Indexing centers,5.42,217122,30.0,28
2017-12-10 09:18:55,704 -> 1024
2017-12-10 09:18:55,711 -> 1024
2017-12-10 09:19:05,713 -> 03.Joining datasets,10.00,217115,30.0,28
Done!!! Sun Dec 10 09:19:06 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 09:19:08,467 -> Starting session,1.54,0
2017-12-10 09:19:08,468 -> Setting variables,0.00,0
2017-12-10 09:19:15,451 -> Reading datasets,6.98,0
2017-12-10 09:19:15,464 -> Points partitions: 2
2017-12-10 09:19:15,474 -> Centers partitions: 2
2017-12-10 09:19:22,662 -> 01.Indexing points,7.15,59143,30.0,28
2017-12-10 09:19:27,629 -> 02.Indexing centers,4.97,217122,30.0,28
2017-12-10 09:19:27,636 -> 1024
2017-12-10 09:19:27,642 -> 1024
2017-12-10 09:19:37,973 -> 03.Joining datasets,10.33,217115,30.0,28
Done!!! Sun Dec 10 09:19:38 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 09:19:40,733 -> Starting session,1.56,0
2017-12-10 09:19:40,733 -> Setting variables,0.00,0
2017-12-10 09:19:45,821 -> Reading datasets,5.09,0
2017-12-10 09:19:45,833 -> Points partitions: 2
2017-12-10 09:19:45,840 -> Centers partitions: 2
2017-12-10 09:19:55,205 -> 01.Indexing points,9.33,59143,30.0,28
2017-12-10 09:20:00,676 -> 02.Indexing centers,5.47,217122,30.0,28
2017-12-10 09:20:00,685 -> 1024
2017-12-10 09:20:00,691 -> 1024
2017-12-10 09:20:11,355 -> 03.Joining datasets,10.66,217115,30.0,28
Done!!! Sun Dec 10 09:20:11 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 09:20:14,242 -> Starting session,1.67,0
2017-12-10 09:20:14,242 -> Setting variables,0.00,0
2017-12-10 09:20:21,310 -> Reading datasets,7.07,0
2017-12-10 09:20:21,323 -> Points partitions: 2
2017-12-10 09:20:21,334 -> Centers partitions: 2
2017-12-10 09:20:29,688 -> 01.Indexing points,8.31,59143,30.0,28
2017-12-10 09:20:35,149 -> 02.Indexing centers,5.46,217122,30.0,28
2017-12-10 09:20:35,160 -> 1024
2017-12-10 09:20:35,169 -> 1024
2017-12-10 09:20:46,715 -> 03.Joining datasets,11.55,217115,30.0,28
Done!!! Sun Dec 10 09:20:47 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 09:20:55,078 -> Starting session,1.55,0
2017-12-10 09:20:55,078 -> Setting variables,0.00,0
2017-12-10 09:20:59,168 -> Reading datasets,4.09,0
2017-12-10 09:20:59,316 -> Points partitions: 2
2017-12-10 09:20:59,324 -> Centers partitions: 2
2017-12-10 09:21:06,381 -> 01.Indexing points,7.02,39429,30.0,7
2017-12-10 09:21:11,642 -> 02.Indexing centers,5.26,144748,30.0,7
2017-12-10 09:21:11,653 -> 1024
2017-12-10 09:21:11,662 -> 1024
2017-12-10 09:21:27,995 -> 03.Joining datasets,16.33,144743,30.0,7
Done!!! Sun Dec 10 09:21:28 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 09:21:30,688 -> Starting session,1.57,0
2017-12-10 09:21:30,688 -> Setting variables,0.00,0
2017-12-10 09:21:34,947 -> Reading datasets,4.26,0
2017-12-10 09:21:34,965 -> Points partitions: 2
2017-12-10 09:21:34,975 -> Centers partitions: 2
2017-12-10 09:21:42,162 -> 01.Indexing points,7.15,39429,30.0,7
2017-12-10 09:21:47,600 -> 02.Indexing centers,5.44,144748,30.0,7
2017-12-10 09:21:47,613 -> 1024
2017-12-10 09:21:47,622 -> 1024
2017-12-10 09:22:04,072 -> 03.Joining datasets,16.45,144743,30.0,7
Done!!! Sun Dec 10 09:22:04 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 09:22:06,812 -> Starting session,1.52,0
2017-12-10 09:22:06,812 -> Setting variables,0.00,0
2017-12-10 09:22:11,042 -> Reading datasets,4.23,0
2017-12-10 09:22:11,055 -> Points partitions: 2
2017-12-10 09:22:11,063 -> Centers partitions: 2
2017-12-10 09:22:18,202 -> 01.Indexing points,7.10,39429,30.0,7
2017-12-10 09:22:23,554 -> 02.Indexing centers,5.35,144748,30.0,7
2017-12-10 09:22:23,566 -> 1024
2017-12-10 09:22:23,574 -> 1024
2017-12-10 09:22:39,485 -> 03.Joining datasets,15.91,144743,30.0,7
Done!!! Sun Dec 10 09:22:39 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 09:22:42,262 -> Starting session,1.66,0
2017-12-10 09:22:42,262 -> Setting variables,0.00,0
2017-12-10 09:22:46,429 -> Reading datasets,4.17,0
2017-12-10 09:22:46,439 -> Points partitions: 2
2017-12-10 09:22:46,447 -> Centers partitions: 2
2017-12-10 09:22:53,560 -> 01.Indexing points,7.07,39429,30.0,7
2017-12-10 09:22:59,061 -> 02.Indexing centers,5.50,144748,30.0,7
2017-12-10 09:22:59,074 -> 1024
2017-12-10 09:22:59,083 -> 1024
2017-12-10 09:23:15,982 -> 03.Joining datasets,16.04,144743,30.0,7
Done!!! Sun Dec 10 09:23:16 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 09:23:18,660 -> Starting session,1.46,0
2017-12-10 09:23:18,660 -> Setting variables,0.00,0
2017-12-10 09:23:22,971 -> Reading datasets,4.31,0
2017-12-10 09:23:22,982 -> Points partitions: 2
2017-12-10 09:23:22,989 -> Centers partitions: 2
2017-12-10 09:23:30,291 -> 01.Indexing points,7.26,39429,30.0,7
2017-12-10 09:23:35,717 -> 02.Indexing centers,5.42,144748,30.0,7
2017-12-10 09:23:35,725 -> 1024
2017-12-10 09:23:35,731 -> 1024
2017-12-10 09:23:52,426 -> 03.Joining datasets,16.70,144743,30.0,7
Done!!! Sun Dec 10 09:23:52 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 09:24:00,884 -> Starting session,1.71,0
2017-12-10 09:24:00,885 -> Setting variables,0.00,0
2017-12-10 09:24:05,625 -> Reading datasets,4.74,0
2017-12-10 09:24:05,640 -> Points partitions: 2
2017-12-10 09:24:05,650 -> Centers partitions: 2
2017-12-10 09:24:12,600 -> 01.Indexing points,6.91,39429,30.0,14
2017-12-10 09:24:17,765 -> 02.Indexing centers,5.16,144748,30.0,14
2017-12-10 09:24:17,776 -> 1024
2017-12-10 09:24:17,785 -> 1024
2017-12-10 09:24:31,710 -> 03.Joining datasets,13.92,144743,30.0,14
Done!!! Sun Dec 10 09:24:32 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:24:34,520 -> Starting session,1.57,0
2017-12-10 09:24:34,521 -> Setting variables,0.00,0
2017-12-10 09:24:39,097 -> Reading datasets,4.58,0
2017-12-10 09:24:39,111 -> Points partitions: 2
2017-12-10 09:24:39,119 -> Centers partitions: 2
2017-12-10 09:24:46,166 -> 01.Indexing points,7.01,39429,30.0,14
2017-12-10 09:24:51,402 -> 02.Indexing centers,5.24,144748,30.0,14
2017-12-10 09:24:51,413 -> 1024
2017-12-10 09:24:51,422 -> 1024
2017-12-10 09:25:04,861 -> 03.Joining datasets,13.44,144743,30.0,14
Done!!! Sun Dec 10 09:25:05 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:25:07,695 -> Starting session,1.73,0
2017-12-10 09:25:07,695 -> Setting variables,0.00,0
2017-12-10 09:25:12,309 -> Reading datasets,4.61,0
2017-12-10 09:25:12,324 -> Points partitions: 2
2017-12-10 09:25:12,333 -> Centers partitions: 2
2017-12-10 09:25:19,514 -> 01.Indexing points,7.14,39429,30.0,14
2017-12-10 09:25:24,642 -> 02.Indexing centers,5.13,144748,30.0,14
2017-12-10 09:25:24,651 -> 1024
2017-12-10 09:25:24,657 -> 1024
2017-12-10 09:25:37,616 -> 03.Joining datasets,12.96,144743,30.0,14
Done!!! Sun Dec 10 09:25:38 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:25:40,414 -> Starting session,1.59,0
2017-12-10 09:25:40,415 -> Setting variables,0.00,0
2017-12-10 09:25:44,985 -> Reading datasets,4.57,0
2017-12-10 09:25:44,997 -> Points partitions: 2
2017-12-10 09:25:45,006 -> Centers partitions: 2
2017-12-10 09:25:52,054 -> 01.Indexing points,7.01,39429,30.0,14
2017-12-10 09:25:57,441 -> 02.Indexing centers,5.39,144748,30.0,14
2017-12-10 09:25:57,450 -> 1024
2017-12-10 09:25:57,457 -> 1024
2017-12-10 09:26:11,269 -> 03.Joining datasets,13.81,144743,30.0,14
Done!!! Sun Dec 10 09:26:12 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 09:26:15,110 -> Starting session,1.84,0
2017-12-10 09:26:15,111 -> Setting variables,0.00,0
2017-12-10 09:26:19,857 -> Reading datasets,4.75,0
2017-12-10 09:26:19,872 -> Points partitions: 2
2017-12-10 09:26:19,882 -> Centers partitions: 2
2017-12-10 09:26:26,977 -> 01.Indexing points,7.05,39429,30.0,14
2017-12-10 09:26:32,528 -> 02.Indexing centers,5.55,144748,30.0,14
2017-12-10 09:26:32,536 -> 1024
2017-12-10 09:26:32,542 -> 1024
2017-12-10 09:26:45,853 -> 03.Joining datasets,13.31,144743,30.0,14
Done!!! Sun Dec 10 09:26:46 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 09:26:54,269 -> Starting session,1.73,0
2017-12-10 09:26:54,269 -> Setting variables,0.00,0
2017-12-10 09:26:59,894 -> Reading datasets,5.62,0
2017-12-10 09:26:59,909 -> Points partitions: 2
2017-12-10 09:26:59,918 -> Centers partitions: 2
2017-12-10 09:27:06,045 -> 01.Indexing points,6.09,39429,30.0,21
2017-12-10 09:27:10,873 -> 02.Indexing centers,4.83,144748,30.0,21
2017-12-10 09:27:10,882 -> 1024
2017-12-10 09:27:10,888 -> 1024
2017-12-10 09:27:21,316 -> 03.Joining datasets,10.43,144743,30.0,21
Done!!! Sun Dec 10 09:27:21 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 09:27:24,043 -> Starting session,1.62,0
2017-12-10 09:27:24,043 -> Setting variables,0.00,0
2017-12-10 09:27:29,958 -> Reading datasets,5.91,0
2017-12-10 09:27:29,968 -> Points partitions: 2
2017-12-10 09:27:29,975 -> Centers partitions: 2
2017-12-10 09:27:36,368 -> 01.Indexing points,6.36,39429,30.0,21
2017-12-10 09:27:41,118 -> 02.Indexing centers,4.75,144748,30.0,21
2017-12-10 09:27:41,129 -> 1024
2017-12-10 09:27:41,136 -> 1024
2017-12-10 09:27:53,219 -> 03.Joining datasets,12.08,144743,30.0,21
Done!!! Sun Dec 10 09:27:53 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 09:27:55,938 -> Starting session,1.47,0
2017-12-10 09:27:55,939 -> Setting variables,0.00,0
2017-12-10 09:28:01,601 -> Reading datasets,5.66,0
2017-12-10 09:28:01,627 -> Points partitions: 2
2017-12-10 09:28:01,645 -> Centers partitions: 2
2017-12-10 09:28:08,111 -> 01.Indexing points,6.41,39429,30.0,21
2017-12-10 09:28:12,615 -> 02.Indexing centers,4.50,144748,30.0,21
2017-12-10 09:28:12,625 -> 1024
2017-12-10 09:28:12,633 -> 1024
2017-12-10 09:28:22,664 -> 03.Joining datasets,10.11,144743,30.0,21
Done!!! Sun Dec 10 09:28:23 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 09:28:25,655 -> Starting session,1.75,0
2017-12-10 09:28:25,655 -> Setting variables,0.00,0
2017-12-10 09:28:31,543 -> Reading datasets,5.89,0
2017-12-10 09:28:31,557 -> Points partitions: 2
2017-12-10 09:28:31,566 -> Centers partitions: 2
2017-12-10 09:28:37,952 -> 01.Indexing points,6.34,39429,30.0,21
2017-12-10 09:28:42,722 -> 02.Indexing centers,4.77,144748,30.0,21
2017-12-10 09:28:42,729 -> 1024
2017-12-10 09:28:42,736 -> 1024
2017-12-10 09:28:53,823 -> 03.Joining datasets,11.09,144743,30.0,21
Done!!! Sun Dec 10 09:28:54 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 09:28:56,573 -> Starting session,1.63,0
2017-12-10 09:28:56,574 -> Setting variables,0.00,0
2017-12-10 09:29:02,466 -> Reading datasets,5.89,0
2017-12-10 09:29:02,486 -> Points partitions: 2
2017-12-10 09:29:02,501 -> Centers partitions: 2
2017-12-10 09:29:08,964 -> 01.Indexing points,6.41,39429,30.0,21
2017-12-10 09:29:13,300 -> 02.Indexing centers,4.33,144748,30.0,21
2017-12-10 09:29:13,307 -> 1024
2017-12-10 09:29:13,312 -> 1024
2017-12-10 09:29:23,704 -> 03.Joining datasets,10.39,144743,30.0,21
Done!!! Sun Dec 10 09:29:24 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 09:29:32,209 -> Starting session,1.75,0
2017-12-10 09:29:32,209 -> Setting variables,0.00,0
2017-12-10 09:29:39,214 -> Reading datasets,7.00,0
2017-12-10 09:29:39,239 -> Points partitions: 2
2017-12-10 09:29:39,257 -> Centers partitions: 2
2017-12-10 09:29:46,065 -> 01.Indexing points,6.75,39429,30.0,28
2017-12-10 09:29:51,708 -> 02.Indexing centers,5.64,144748,30.0,28
2017-12-10 09:29:51,719 -> 1024
2017-12-10 09:29:51,727 -> 1024
2017-12-10 09:30:01,956 -> 03.Joining datasets,10.23,144743,30.0,28
Done!!! Sun Dec 10 09:30:02 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 09:30:04,798 -> Starting session,1.73,0
2017-12-10 09:30:04,798 -> Setting variables,0.00,0
2017-12-10 09:30:11,594 -> Reading datasets,6.80,0
2017-12-10 09:30:11,614 -> Points partitions: 2
2017-12-10 09:30:11,627 -> Centers partitions: 2
2017-12-10 09:30:18,968 -> 01.Indexing points,7.28,39429,30.0,28
2017-12-10 09:30:23,650 -> 02.Indexing centers,4.68,144748,30.0,28
2017-12-10 09:30:23,663 -> 1024
2017-12-10 09:30:23,674 -> 1024
2017-12-10 09:30:33,027 -> 03.Joining datasets,9.35,144743,30.0,28
Done!!! Sun Dec 10 09:30:33 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 09:30:35,817 -> Starting session,1.57,0
2017-12-10 09:30:35,818 -> Setting variables,0.00,0
2017-12-10 09:30:41,870 -> Reading datasets,6.05,0
2017-12-10 09:30:41,884 -> Points partitions: 2
2017-12-10 09:30:41,893 -> Centers partitions: 2
2017-12-10 09:30:50,425 -> 01.Indexing points,8.49,39429,30.0,28
2017-12-10 09:30:55,157 -> 02.Indexing centers,4.73,144748,30.0,28
2017-12-10 09:30:55,165 -> 1024
2017-12-10 09:30:55,171 -> 1024
2017-12-10 09:31:04,472 -> 03.Joining datasets,9.30,144743,30.0,28
Done!!! Sun Dec 10 09:31:04 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 09:31:07,140 -> Starting session,1.54,0
2017-12-10 09:31:07,141 -> Setting variables,0.00,0
2017-12-10 09:31:13,251 -> Reading datasets,6.11,0
2017-12-10 09:31:13,270 -> Points partitions: 2
2017-12-10 09:31:13,283 -> Centers partitions: 2
2017-12-10 09:31:21,790 -> 01.Indexing points,8.45,39429,30.0,28
2017-12-10 09:31:27,105 -> 02.Indexing centers,5.31,144748,30.0,28
2017-12-10 09:31:27,118 -> 1024
2017-12-10 09:31:27,127 -> 1024
2017-12-10 09:31:36,379 -> 03.Joining datasets,9.25,144743,30.0,28
Done!!! Sun Dec 10 09:31:36 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 09:31:39,084 -> Starting session,1.59,0
2017-12-10 09:31:39,085 -> Setting variables,0.00,0
2017-12-10 09:31:45,897 -> Reading datasets,6.81,0
2017-12-10 09:31:45,920 -> Points partitions: 2
2017-12-10 09:31:45,936 -> Centers partitions: 2
2017-12-10 09:31:53,721 -> 01.Indexing points,7.73,39429,30.0,28
2017-12-10 09:31:59,584 -> 02.Indexing centers,5.86,144748,30.0,28
2017-12-10 09:31:59,591 -> 1024
2017-12-10 09:31:59,597 -> 1024
2017-12-10 09:32:11,312 -> 03.Joining datasets,11.71,144743,30.0,28
Done!!! Sun Dec 10 09:32:11 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 09:32:19,812 -> Starting session,1.54,0
2017-12-10 09:32:19,813 -> Setting variables,0.00,0
2017-12-10 09:32:24,008 -> Reading datasets,4.19,0
2017-12-10 09:32:24,019 -> Points partitions: 2
2017-12-10 09:32:24,026 -> Centers partitions: 2
2017-12-10 09:32:30,566 -> 01.Indexing points,6.50,19715,30.0,7
2017-12-10 09:32:35,510 -> 02.Indexing centers,4.94,72374,30.0,7
2017-12-10 09:32:35,521 -> 992
2017-12-10 09:32:35,529 -> 1024
2017-12-10 09:32:48,710 -> 03.Joining datasets,13.18,72372,30.0,7
Done!!! Sun Dec 10 09:32:49 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 09:32:51,269 -> Starting session,1.49,0
2017-12-10 09:32:51,269 -> Setting variables,0.00,0
2017-12-10 09:32:55,482 -> Reading datasets,4.21,0
2017-12-10 09:32:55,495 -> Points partitions: 2
2017-12-10 09:32:55,505 -> Centers partitions: 2
2017-12-10 09:33:02,164 -> 01.Indexing points,6.62,19715,30.0,7
2017-12-10 09:33:06,896 -> 02.Indexing centers,4.73,72374,30.0,7
2017-12-10 09:33:06,910 -> 992
2017-12-10 09:33:06,918 -> 1024
2017-12-10 09:33:20,995 -> 03.Joining datasets,14.08,72372,30.0,7
Done!!! Sun Dec 10 09:33:21 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 09:33:23,833 -> Starting session,1.77,0
2017-12-10 09:33:23,834 -> Setting variables,0.00,0
2017-12-10 09:33:28,082 -> Reading datasets,4.25,0
2017-12-10 09:33:28,095 -> Points partitions: 2
2017-12-10 09:33:28,106 -> Centers partitions: 2
2017-12-10 09:33:34,624 -> 01.Indexing points,6.47,19715,30.0,7
2017-12-10 09:33:39,338 -> 02.Indexing centers,4.71,72374,30.0,7
2017-12-10 09:33:39,346 -> 992
2017-12-10 09:33:39,353 -> 1024
2017-12-10 09:33:52,581 -> 03.Joining datasets,13.23,72372,30.0,7
Done!!! Sun Dec 10 09:33:53 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 09:33:55,295 -> Starting session,1.50,0
2017-12-10 09:33:55,295 -> Setting variables,0.00,0
2017-12-10 09:33:59,516 -> Reading datasets,4.22,0
2017-12-10 09:33:59,530 -> Points partitions: 2
2017-12-10 09:33:59,542 -> Centers partitions: 2
2017-12-10 09:34:05,938 -> 01.Indexing points,6.48,19715,30.0,7
2017-12-10 09:34:10,712 -> 02.Indexing centers,4.77,72374,30.0,7
2017-12-10 09:34:10,724 -> 992
2017-12-10 09:34:10,733 -> 1024
2017-12-10 09:34:24,453 -> 03.Joining datasets,13.72,72372,30.0,7
Done!!! Sun Dec 10 09:34:24 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 09:34:27,191 -> Starting session,1.53,0
2017-12-10 09:34:27,192 -> Setting variables,0.00,0
2017-12-10 09:34:31,187 -> Reading datasets,3.99,0
2017-12-10 09:34:31,198 -> Points partitions: 2
2017-12-10 09:34:31,206 -> Centers partitions: 2
2017-12-10 09:34:37,684 -> 01.Indexing points,6.44,19715,30.0,7
2017-12-10 09:34:42,784 -> 02.Indexing centers,5.10,72374,30.0,7
2017-12-10 09:34:42,797 -> 992
2017-12-10 09:34:42,805 -> 1024
2017-12-10 09:34:56,524 -> 03.Joining datasets,13.72,72372,30.0,7
Done!!! Sun Dec 10 09:34:57 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 09:35:05,037 -> Starting session,1.75,0
2017-12-10 09:35:05,039 -> Setting variables,0.00,0
2017-12-10 09:35:10,058 -> Reading datasets,5.02,0
2017-12-10 09:35:10,070 -> Points partitions: 2
2017-12-10 09:35:10,078 -> Centers partitions: 2
2017-12-10 09:35:16,883 -> 01.Indexing points,6.77,19715,30.0,14
2017-12-10 09:35:21,562 -> 02.Indexing centers,4.68,72374,30.0,14
2017-12-10 09:35:21,569 -> 992
2017-12-10 09:35:21,575 -> 1024
2017-12-10 09:35:32,670 -> 03.Joining datasets,11.10,72372,30.0,14
Done!!! Sun Dec 10 09:35:33 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:35:36,408 -> Starting session,1.80,0
2017-12-10 09:35:36,409 -> Setting variables,0.00,0
2017-12-10 09:35:41,005 -> Reading datasets,4.60,0
2017-12-10 09:35:41,016 -> Points partitions: 2
2017-12-10 09:35:41,023 -> Centers partitions: 2
2017-12-10 09:35:48,063 -> 01.Indexing points,7.00,19715,30.0,14
2017-12-10 09:35:52,997 -> 02.Indexing centers,4.93,72374,30.0,14
2017-12-10 09:35:53,005 -> 992
2017-12-10 09:35:53,011 -> 1024
2017-12-10 09:36:04,338 -> 03.Joining datasets,11.33,72372,30.0,14
Done!!! Sun Dec 10 09:36:04 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:36:07,228 -> Starting session,1.71,0
2017-12-10 09:36:07,228 -> Setting variables,0.00,0
2017-12-10 09:36:12,043 -> Reading datasets,4.82,0
2017-12-10 09:36:12,054 -> Points partitions: 2
2017-12-10 09:36:12,062 -> Centers partitions: 2
2017-12-10 09:36:19,008 -> 01.Indexing points,6.90,19715,30.0,14
2017-12-10 09:36:23,711 -> 02.Indexing centers,4.70,72374,30.0,14
2017-12-10 09:36:23,830 -> 992
2017-12-10 09:36:23,837 -> 1024
2017-12-10 09:36:35,129 -> 03.Joining datasets,11.29,72372,30.0,14
Done!!! Sun Dec 10 09:36:36 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:36:38,757 -> Starting session,1.63,0
2017-12-10 09:36:38,758 -> Setting variables,0.00,0
2017-12-10 09:36:43,447 -> Reading datasets,4.69,0
2017-12-10 09:36:43,460 -> Points partitions: 2
2017-12-10 09:36:43,467 -> Centers partitions: 2
2017-12-10 09:36:50,807 -> 01.Indexing points,7.30,19715,30.0,14
2017-12-10 09:36:55,329 -> 02.Indexing centers,4.52,72374,30.0,14
2017-12-10 09:36:55,341 -> 992
2017-12-10 09:36:55,350 -> 1024
2017-12-10 09:37:06,112 -> 03.Joining datasets,10.76,72372,30.0,14
Done!!! Sun Dec 10 09:37:07 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 09:37:09,598 -> Starting session,1.49,0
2017-12-10 09:37:09,598 -> Setting variables,0.00,0
2017-12-10 09:37:14,336 -> Reading datasets,4.74,0
2017-12-10 09:37:14,350 -> Points partitions: 2
2017-12-10 09:37:14,361 -> Centers partitions: 2
2017-12-10 09:37:21,392 -> 01.Indexing points,6.99,19715,30.0,14
2017-12-10 09:37:26,081 -> 02.Indexing centers,4.69,72374,30.0,14
2017-12-10 09:37:26,092 -> 992
2017-12-10 09:37:26,101 -> 1024
2017-12-10 09:37:37,478 -> 03.Joining datasets,11.38,72372,30.0,14
Done!!! Sun Dec 10 09:37:37 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 09:37:45,855 -> Starting session,1.69,0
2017-12-10 09:37:45,855 -> Setting variables,0.00,0
2017-12-10 09:37:50,714 -> Reading datasets,4.86,0
2017-12-10 09:37:50,730 -> Points partitions: 2
2017-12-10 09:37:50,741 -> Centers partitions: 2
2017-12-10 09:37:59,413 -> 01.Indexing points,8.58,19715,30.0,21
2017-12-10 09:38:03,718 -> 02.Indexing centers,4.30,72374,30.0,21
2017-12-10 09:38:03,725 -> 992
2017-12-10 09:38:03,731 -> 1024
2017-12-10 09:38:13,974 -> 03.Joining datasets,10.31,72372,30.0,21
Done!!! Sun Dec 10 09:38:14 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 09:38:16,773 -> Starting session,1.58,0
2017-12-10 09:38:16,774 -> Setting variables,0.00,0
2017-12-10 09:38:22,397 -> Reading datasets,5.62,0
2017-12-10 09:38:22,415 -> Points partitions: 2
2017-12-10 09:38:22,426 -> Centers partitions: 2
2017-12-10 09:38:27,491 -> 01.Indexing points,5.01,19715,30.0,21
2017-12-10 09:38:32,206 -> 02.Indexing centers,4.71,72374,30.0,21
2017-12-10 09:38:32,214 -> 992
2017-12-10 09:38:32,221 -> 1024
2017-12-10 09:38:41,532 -> 03.Joining datasets,9.31,72372,30.0,21
Done!!! Sun Dec 10 09:38:42 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 09:38:44,429 -> Starting session,1.71,0
2017-12-10 09:38:44,429 -> Setting variables,0.00,0
2017-12-10 09:38:49,268 -> Reading datasets,4.84,0
2017-12-10 09:38:49,282 -> Points partitions: 2
2017-12-10 09:38:49,290 -> Centers partitions: 2
2017-12-10 09:38:57,532 -> 01.Indexing points,8.20,19715,30.0,21
2017-12-10 09:39:02,190 -> 02.Indexing centers,4.66,72374,30.0,21
2017-12-10 09:39:02,199 -> 992
2017-12-10 09:39:02,205 -> 1024
2017-12-10 09:39:12,949 -> 03.Joining datasets,10.74,72372,30.0,21
Done!!! Sun Dec 10 09:39:13 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 09:39:15,780 -> Starting session,1.70,0
2017-12-10 09:39:15,780 -> Setting variables,0.00,0
2017-12-10 09:39:21,555 -> Reading datasets,5.77,0
2017-12-10 09:39:21,574 -> Points partitions: 2
2017-12-10 09:39:21,586 -> Centers partitions: 2
2017-12-10 09:39:26,616 -> 01.Indexing points,4.98,19715,30.0,21
2017-12-10 09:39:30,906 -> 02.Indexing centers,4.29,72374,30.0,21
2017-12-10 09:39:30,919 -> 992
2017-12-10 09:39:30,931 -> 1024
2017-12-10 09:39:40,138 -> 03.Joining datasets,9.21,72372,30.0,21
Done!!! Sun Dec 10 09:39:40 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 09:39:43,036 -> Starting session,1.80,0
2017-12-10 09:39:43,037 -> Setting variables,0.00,0
2017-12-10 09:39:47,633 -> Reading datasets,4.60,0
2017-12-10 09:39:47,644 -> Points partitions: 2
2017-12-10 09:39:47,651 -> Centers partitions: 2
2017-12-10 09:39:55,269 -> 01.Indexing points,7.58,19715,30.0,21
2017-12-10 09:39:59,424 -> 02.Indexing centers,4.15,72374,30.0,21
2017-12-10 09:39:59,433 -> 992
2017-12-10 09:39:59,439 -> 1024
2017-12-10 09:40:10,628 -> 03.Joining datasets,11.19,72372,30.0,21
Done!!! Sun Dec 10 09:40:11 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 09:40:19,131 -> Starting session,1.77,0
2017-12-10 09:40:19,131 -> Setting variables,0.00,0
2017-12-10 09:40:24,960 -> Reading datasets,5.83,0
2017-12-10 09:40:24,979 -> Points partitions: 2
2017-12-10 09:40:24,991 -> Centers partitions: 2
2017-12-10 09:40:32,759 -> 01.Indexing points,7.71,19715,30.0,28
2017-12-10 09:40:37,256 -> 02.Indexing centers,4.75,72374,30.0,28
2017-12-10 09:40:37,266 -> 992
2017-12-10 09:40:37,275 -> 1024
2017-12-10 09:40:48,086 -> 03.Joining datasets,10.81,72372,30.0,28
Done!!! Sun Dec 10 09:40:48 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 09:40:50,968 -> Starting session,1.62,0
2017-12-10 09:40:50,968 -> Setting variables,0.00,0
2017-12-10 09:40:58,241 -> Reading datasets,7.27,0
2017-12-10 09:40:58,260 -> Points partitions: 2
2017-12-10 09:40:58,272 -> Centers partitions: 2
2017-12-10 09:41:05,821 -> 01.Indexing points,7.49,19715,30.0,28
2017-12-10 09:41:10,290 -> 02.Indexing centers,4.47,72374,30.0,28
2017-12-10 09:41:10,305 -> 992
2017-12-10 09:41:10,315 -> 1024
2017-12-10 09:41:21,742 -> 03.Joining datasets,11.43,72372,30.0,28
Done!!! Sun Dec 10 09:41:22 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 09:41:24,374 -> Starting session,1.53,0
2017-12-10 09:41:24,374 -> Setting variables,0.00,0
2017-12-10 09:41:31,275 -> Reading datasets,6.90,0
2017-12-10 09:41:31,288 -> Points partitions: 2
2017-12-10 09:41:31,299 -> Centers partitions: 2
2017-12-10 09:41:37,191 -> 01.Indexing points,5.85,19715,30.0,28
2017-12-10 09:41:42,526 -> 02.Indexing centers,5.33,72374,30.0,28
2017-12-10 09:41:42,537 -> 992
2017-12-10 09:41:42,545 -> 1024
2017-12-10 09:41:51,732 -> 03.Joining datasets,9.19,72372,30.0,28
Done!!! Sun Dec 10 09:41:52 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 09:41:54,402 -> Starting session,1.56,0
2017-12-10 09:41:54,402 -> Setting variables,0.00,0
2017-12-10 09:41:59,208 -> Reading datasets,4.81,0
2017-12-10 09:41:59,221 -> Points partitions: 2
2017-12-10 09:41:59,232 -> Centers partitions: 2
2017-12-10 09:42:09,627 -> 01.Indexing points,10.36,19715,30.0,28
2017-12-10 09:42:14,194 -> 02.Indexing centers,4.57,72374,30.0,28
2017-12-10 09:42:14,207 -> 992
2017-12-10 09:42:14,216 -> 1024
2017-12-10 09:42:24,878 -> 03.Joining datasets,10.66,72372,30.0,28
Done!!! Sun Dec 10 09:42:25 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 09:42:27,672 -> Starting session,1.59,0
2017-12-10 09:42:27,672 -> Setting variables,0.00,0
2017-12-10 09:42:34,504 -> Reading datasets,6.83,0
2017-12-10 09:42:34,520 -> Points partitions: 2
2017-12-10 09:42:34,533 -> Centers partitions: 2
2017-12-10 09:42:41,299 -> 01.Indexing points,6.72,19715,30.0,28
2017-12-10 09:42:45,767 -> 02.Indexing centers,4.47,72374,30.0,28
2017-12-10 09:42:45,776 -> 992
2017-12-10 09:42:45,782 -> 1024
2017-12-10 09:42:56,084 -> 03.Joining datasets,10.30,72372,30.0,28
Done!!! Sun Dec 10 09:42:56 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 09:43:04,736 -> Starting session,1.62,0
2017-12-10 09:43:04,736 -> Setting variables,0.00,0
2017-12-10 09:43:09,014 -> Reading datasets,4.28,0
2017-12-10 09:43:09,027 -> Points partitions: 2
2017-12-10 09:43:09,038 -> Centers partitions: 2
2017-12-10 09:43:16,594 -> 01.Indexing points,7.51,78857,20.0,7
2017-12-10 09:43:21,956 -> 02.Indexing centers,5.36,190656,20.0,7
2017-12-10 09:43:21,967 -> 1024
2017-12-10 09:43:21,975 -> 1024
2017-12-10 09:43:39,563 -> 03.Joining datasets,17.59,190652,20.0,7
Done!!! Sun Dec 10 09:43:40 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 09:43:42,237 -> Starting session,1.52,0
2017-12-10 09:43:42,237 -> Setting variables,0.00,0
2017-12-10 09:43:46,414 -> Reading datasets,4.18,0
2017-12-10 09:43:46,428 -> Points partitions: 2
2017-12-10 09:43:46,439 -> Centers partitions: 2
2017-12-10 09:43:54,544 -> 01.Indexing points,8.06,78857,20.0,7
2017-12-10 09:44:00,166 -> 02.Indexing centers,5.62,190656,20.0,7
2017-12-10 09:44:00,180 -> 1024
2017-12-10 09:44:00,191 -> 1024
2017-12-10 09:44:17,715 -> 03.Joining datasets,17.52,190652,20.0,7
Done!!! Sun Dec 10 09:44:18 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 09:44:20,440 -> Starting session,1.53,0
2017-12-10 09:44:20,440 -> Setting variables,0.00,0
2017-12-10 09:44:24,829 -> Reading datasets,4.39,0
2017-12-10 09:44:24,847 -> Points partitions: 2
2017-12-10 09:44:24,859 -> Centers partitions: 2
2017-12-10 09:44:32,715 -> 01.Indexing points,7.81,78857,20.0,7
2017-12-10 09:44:38,206 -> 02.Indexing centers,5.49,190656,20.0,7
2017-12-10 09:44:38,355 -> 1024
2017-12-10 09:44:38,361 -> 1024
2017-12-10 09:44:55,915 -> 03.Joining datasets,17.55,190652,20.0,7
Done!!! Sun Dec 10 09:44:56 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 09:44:59,020 -> Starting session,1.93,0
2017-12-10 09:44:59,021 -> Setting variables,0.00,0
2017-12-10 09:45:03,120 -> Reading datasets,4.10,0
2017-12-10 09:45:03,134 -> Points partitions: 2
2017-12-10 09:45:03,144 -> Centers partitions: 2
2017-12-10 09:45:10,756 -> 01.Indexing points,7.57,78857,20.0,7
2017-12-10 09:45:16,254 -> 02.Indexing centers,5.50,190656,20.0,7
2017-12-10 09:45:16,268 -> 1024
2017-12-10 09:45:16,278 -> 1024
2017-12-10 09:45:33,623 -> 03.Joining datasets,17.34,190652,20.0,7
Done!!! Sun Dec 10 09:45:34 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 09:45:36,383 -> Starting session,1.55,0
2017-12-10 09:45:36,384 -> Setting variables,0.00,0
2017-12-10 09:45:40,567 -> Reading datasets,4.18,0
2017-12-10 09:45:40,578 -> Points partitions: 2
2017-12-10 09:45:40,585 -> Centers partitions: 2
2017-12-10 09:45:48,480 -> 01.Indexing points,7.86,78857,20.0,7
2017-12-10 09:45:54,249 -> 02.Indexing centers,5.77,190656,20.0,7
2017-12-10 09:45:54,261 -> 1024
2017-12-10 09:45:54,269 -> 1024
2017-12-10 09:46:12,588 -> 03.Joining datasets,18.32,190652,20.0,7
Done!!! Sun Dec 10 09:46:13 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 09:46:20,972 -> Starting session,1.63,0
2017-12-10 09:46:20,973 -> Setting variables,0.00,0
2017-12-10 09:46:25,969 -> Reading datasets,5.00,0
2017-12-10 09:46:25,979 -> Points partitions: 2
2017-12-10 09:46:25,987 -> Centers partitions: 2
2017-12-10 09:46:33,876 -> 01.Indexing points,7.85,78857,20.0,14
2017-12-10 09:46:39,453 -> 02.Indexing centers,5.58,190656,20.0,14
2017-12-10 09:46:39,462 -> 1024
2017-12-10 09:46:39,468 -> 1024
2017-12-10 09:46:53,797 -> 03.Joining datasets,14.33,190652,20.0,14
Done!!! Sun Dec 10 09:46:54 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:46:56,636 -> Starting session,1.56,0
2017-12-10 09:46:56,636 -> Setting variables,0.00,0
2017-12-10 09:47:01,426 -> Reading datasets,4.79,0
2017-12-10 09:47:01,440 -> Points partitions: 2
2017-12-10 09:47:01,448 -> Centers partitions: 2
2017-12-10 09:47:09,426 -> 01.Indexing points,7.94,78857,20.0,14
2017-12-10 09:47:14,695 -> 02.Indexing centers,5.27,190656,20.0,14
2017-12-10 09:47:14,704 -> 1024
2017-12-10 09:47:14,710 -> 1024
2017-12-10 09:47:29,845 -> 03.Joining datasets,14.68,190652,20.0,14
Done!!! Sun Dec 10 09:47:30 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:47:32,796 -> Starting session,1.73,0
2017-12-10 09:47:32,796 -> Setting variables,0.00,0
2017-12-10 09:47:37,540 -> Reading datasets,4.74,0
2017-12-10 09:47:37,551 -> Points partitions: 2
2017-12-10 09:47:37,558 -> Centers partitions: 2
2017-12-10 09:47:45,352 -> 01.Indexing points,7.76,78857,20.0,14
2017-12-10 09:47:50,493 -> 02.Indexing centers,5.14,190656,20.0,14
2017-12-10 09:47:50,503 -> 1024
2017-12-10 09:47:50,615 -> 1024
2017-12-10 09:48:04,704 -> 03.Joining datasets,14.09,190652,20.0,14
Done!!! Sun Dec 10 09:48:05 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:48:07,547 -> Starting session,1.72,0
2017-12-10 09:48:07,547 -> Setting variables,0.00,0
2017-12-10 09:48:12,399 -> Reading datasets,4.85,0
2017-12-10 09:48:12,413 -> Points partitions: 2
2017-12-10 09:48:12,422 -> Centers partitions: 2
2017-12-10 09:48:20,226 -> 01.Indexing points,7.76,78857,20.0,14
2017-12-10 09:48:25,675 -> 02.Indexing centers,5.45,190656,20.0,14
2017-12-10 09:48:25,688 -> 1024
2017-12-10 09:48:25,697 -> 1024
2017-12-10 09:48:40,409 -> 03.Joining datasets,14.71,190652,20.0,14
Done!!! Sun Dec 10 09:48:40 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 09:48:43,250 -> Starting session,1.67,0
2017-12-10 09:48:43,250 -> Setting variables,0.00,0
2017-12-10 09:48:48,124 -> Reading datasets,4.87,0
2017-12-10 09:48:48,135 -> Points partitions: 2
2017-12-10 09:48:48,142 -> Centers partitions: 2
2017-12-10 09:48:56,184 -> 01.Indexing points,8.00,78857,20.0,14
2017-12-10 09:49:01,434 -> 02.Indexing centers,5.25,190656,20.0,14
2017-12-10 09:49:01,442 -> 1024
2017-12-10 09:49:01,448 -> 1024
2017-12-10 09:49:15,454 -> 03.Joining datasets,14.01,190652,20.0,14
Done!!! Sun Dec 10 09:49:15 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 09:49:23,736 -> Starting session,1.64,0
2017-12-10 09:49:23,737 -> Setting variables,0.00,0
2017-12-10 09:49:29,738 -> Reading datasets,6.00,0
2017-12-10 09:49:29,759 -> Points partitions: 2
2017-12-10 09:49:29,774 -> Centers partitions: 2
2017-12-10 09:49:36,556 -> 01.Indexing points,6.72,78857,20.0,21
2017-12-10 09:49:41,096 -> 02.Indexing centers,4.54,190656,20.0,21
2017-12-10 09:49:41,107 -> 1024
2017-12-10 09:49:41,114 -> 1024
2017-12-10 09:49:52,871 -> 03.Joining datasets,11.76,190652,20.0,21
Done!!! Sun Dec 10 09:49:53 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 09:49:55,665 -> Starting session,1.58,0
2017-12-10 09:49:55,666 -> Setting variables,0.00,0
2017-12-10 09:50:00,812 -> Reading datasets,5.15,0
2017-12-10 09:50:00,828 -> Points partitions: 2
2017-12-10 09:50:00,840 -> Centers partitions: 2
2017-12-10 09:50:08,860 -> 01.Indexing points,7.97,78857,20.0,21
2017-12-10 09:50:13,659 -> 02.Indexing centers,4.80,190656,20.0,21
2017-12-10 09:50:13,670 -> 1024
2017-12-10 09:50:13,678 -> 1024
2017-12-10 09:50:25,436 -> 03.Joining datasets,11.76,190652,20.0,21
Done!!! Sun Dec 10 09:50:25 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 09:50:28,310 -> Starting session,1.76,0
2017-12-10 09:50:28,311 -> Setting variables,0.00,0
2017-12-10 09:50:33,916 -> Reading datasets,5.60,0
2017-12-10 09:50:33,934 -> Points partitions: 2
2017-12-10 09:50:33,944 -> Centers partitions: 2
2017-12-10 09:50:41,328 -> 01.Indexing points,7.34,78857,20.0,21
2017-12-10 09:50:45,910 -> 02.Indexing centers,4.58,190656,20.0,21
2017-12-10 09:50:45,920 -> 1024
2017-12-10 09:50:45,929 -> 1024
2017-12-10 09:50:56,609 -> 03.Joining datasets,10.68,190652,20.0,21
Done!!! Sun Dec 10 09:50:57 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 09:50:59,365 -> Starting session,1.54,0
2017-12-10 09:50:59,366 -> Setting variables,0.00,0
2017-12-10 09:51:05,293 -> Reading datasets,5.93,0
2017-12-10 09:51:05,311 -> Points partitions: 2
2017-12-10 09:51:05,324 -> Centers partitions: 2
2017-12-10 09:51:12,580 -> 01.Indexing points,7.21,78857,20.0,21
2017-12-10 09:51:17,521 -> 02.Indexing centers,4.94,190656,20.0,21
2017-12-10 09:51:17,529 -> 1024
2017-12-10 09:51:17,535 -> 1024
2017-12-10 09:51:28,796 -> 03.Joining datasets,11.34,190652,20.0,21
Done!!! Sun Dec 10 09:51:29 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 09:51:31,556 -> Starting session,1.55,0
2017-12-10 09:51:31,556 -> Setting variables,0.00,0
2017-12-10 09:51:37,418 -> Reading datasets,5.86,0
2017-12-10 09:51:37,429 -> Points partitions: 2
2017-12-10 09:51:37,436 -> Centers partitions: 2
2017-12-10 09:51:44,365 -> 01.Indexing points,6.88,78857,20.0,21
2017-12-10 09:51:49,242 -> 02.Indexing centers,4.88,190656,20.0,21
2017-12-10 09:51:49,252 -> 1024
2017-12-10 09:51:49,261 -> 1024
2017-12-10 09:52:00,844 -> 03.Joining datasets,11.58,190652,20.0,21
Done!!! Sun Dec 10 09:52:01 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 09:52:09,293 -> Starting session,1.70,0
2017-12-10 09:52:09,294 -> Setting variables,0.00,0
2017-12-10 09:52:15,283 -> Reading datasets,5.99,0
2017-12-10 09:52:15,297 -> Points partitions: 2
2017-12-10 09:52:15,307 -> Centers partitions: 2
2017-12-10 09:52:24,400 -> 01.Indexing points,9.05,78857,20.0,28
2017-12-10 09:52:30,855 -> 02.Indexing centers,6.45,190656,20.0,28
2017-12-10 09:52:30,863 -> 1024
2017-12-10 09:52:30,869 -> 1024
2017-12-10 09:52:41,006 -> 03.Joining datasets,10.14,190652,20.0,28
Done!!! Sun Dec 10 09:52:41 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 09:52:43,785 -> Starting session,1.64,0
2017-12-10 09:52:43,786 -> Setting variables,0.00,0
2017-12-10 09:52:50,801 -> Reading datasets,7.02,0
2017-12-10 09:52:50,813 -> Points partitions: 2
2017-12-10 09:52:50,822 -> Centers partitions: 2
2017-12-10 09:52:59,899 -> 01.Indexing points,9.04,78857,20.0,28
2017-12-10 09:53:04,716 -> 02.Indexing centers,4.81,190656,20.0,28
2017-12-10 09:53:04,727 -> 1024
2017-12-10 09:53:04,736 -> 1024
2017-12-10 09:53:14,528 -> 03.Joining datasets,9.79,190652,20.0,28
Done!!! Sun Dec 10 09:53:15 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 09:53:17,341 -> Starting session,1.56,0
2017-12-10 09:53:17,341 -> Setting variables,0.00,0
2017-12-10 09:53:24,497 -> Reading datasets,7.16,0
2017-12-10 09:53:24,517 -> Points partitions: 2
2017-12-10 09:53:24,529 -> Centers partitions: 2
2017-12-10 09:53:31,348 -> 01.Indexing points,6.76,78857,20.0,28
2017-12-10 09:53:37,512 -> 02.Indexing centers,6.16,190656,20.0,28
2017-12-10 09:53:37,523 -> 1024
2017-12-10 09:53:37,531 -> 1024
2017-12-10 09:53:47,630 -> 03.Joining datasets,10.10,190652,20.0,28
Done!!! Sun Dec 10 09:53:48 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 09:53:50,455 -> Starting session,1.71,0
2017-12-10 09:53:50,456 -> Setting variables,0.00,0
2017-12-10 09:53:57,536 -> Reading datasets,7.08,0
2017-12-10 09:53:57,555 -> Points partitions: 2
2017-12-10 09:53:57,571 -> Centers partitions: 2
2017-12-10 09:54:04,415 -> 01.Indexing points,6.79,78857,20.0,28
2017-12-10 09:54:09,983 -> 02.Indexing centers,5.57,190656,20.0,28
2017-12-10 09:54:09,991 -> 1024
2017-12-10 09:54:09,997 -> 1024
2017-12-10 09:54:19,576 -> 03.Joining datasets,9.58,190652,20.0,28
Done!!! Sun Dec 10 09:54:20 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 09:54:22,313 -> Starting session,1.47,0
2017-12-10 09:54:22,314 -> Setting variables,0.00,0
2017-12-10 09:54:28,647 -> Reading datasets,6.33,0
2017-12-10 09:54:28,659 -> Points partitions: 2
2017-12-10 09:54:28,667 -> Centers partitions: 2
2017-12-10 09:54:37,785 -> 01.Indexing points,9.08,78857,20.0,28
2017-12-10 09:54:43,408 -> 02.Indexing centers,5.62,190656,20.0,28
2017-12-10 09:54:43,418 -> 1024
2017-12-10 09:54:43,425 -> 1024
2017-12-10 09:54:53,093 -> 03.Joining datasets,9.67,190652,20.0,28
Done!!! Sun Dec 10 09:54:53 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 09:55:01,490 -> Starting session,1.56,0
2017-12-10 09:55:01,490 -> Setting variables,0.00,0
2017-12-10 09:55:05,711 -> Reading datasets,4.22,0
2017-12-10 09:55:05,724 -> Points partitions: 2
2017-12-10 09:55:05,735 -> Centers partitions: 2
2017-12-10 09:55:13,060 -> 01.Indexing points,7.29,59143,20.0,7
2017-12-10 09:55:18,094 -> 02.Indexing centers,5.03,142992,20.0,7
2017-12-10 09:55:18,104 -> 1024
2017-12-10 09:55:18,112 -> 1024
2017-12-10 09:55:34,300 -> 03.Joining datasets,16.19,142988,20.0,7
Done!!! Sun Dec 10 09:55:34 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 09:55:36,937 -> Starting session,1.42,0
2017-12-10 09:55:36,937 -> Setting variables,0.00,0
2017-12-10 09:55:41,352 -> Reading datasets,4.41,0
2017-12-10 09:55:41,365 -> Points partitions: 2
2017-12-10 09:55:41,376 -> Centers partitions: 2
2017-12-10 09:55:48,688 -> 01.Indexing points,7.27,59143,20.0,7
2017-12-10 09:55:53,845 -> 02.Indexing centers,5.16,142992,20.0,7
2017-12-10 09:55:53,853 -> 1024
2017-12-10 09:55:53,859 -> 1024
2017-12-10 09:56:10,328 -> 03.Joining datasets,16.54,142988,20.0,7
Done!!! Sun Dec 10 09:56:10 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 09:56:13,001 -> Starting session,1.58,0
2017-12-10 09:56:13,002 -> Setting variables,0.00,0
2017-12-10 09:56:17,257 -> Reading datasets,4.26,0
2017-12-10 09:56:17,270 -> Points partitions: 2
2017-12-10 09:56:17,278 -> Centers partitions: 2
2017-12-10 09:56:24,581 -> 01.Indexing points,7.26,59143,20.0,7
2017-12-10 09:56:29,661 -> 02.Indexing centers,5.08,142992,20.0,7
2017-12-10 09:56:29,674 -> 1024
2017-12-10 09:56:29,684 -> 1024
2017-12-10 09:56:46,294 -> 03.Joining datasets,16.61,142988,20.0,7
Done!!! Sun Dec 10 09:56:46 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 09:56:49,046 -> Starting session,1.55,0
2017-12-10 09:56:49,047 -> Setting variables,0.00,0
2017-12-10 09:56:53,315 -> Reading datasets,4.27,0
2017-12-10 09:56:53,327 -> Points partitions: 2
2017-12-10 09:56:53,334 -> Centers partitions: 2
2017-12-10 09:57:01,027 -> 01.Indexing points,7.65,59143,20.0,7
2017-12-10 09:57:06,014 -> 02.Indexing centers,4.98,142992,20.0,7
2017-12-10 09:57:06,025 -> 1024
2017-12-10 09:57:06,033 -> 1024
2017-12-10 09:57:21,983 -> 03.Joining datasets,15.95,142988,20.0,7
Done!!! Sun Dec 10 09:57:22 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 09:57:24,754 -> Starting session,1.50,0
2017-12-10 09:57:24,755 -> Setting variables,0.00,0
2017-12-10 09:57:28,938 -> Reading datasets,4.18,0
2017-12-10 09:57:28,950 -> Points partitions: 2
2017-12-10 09:57:28,959 -> Centers partitions: 2
2017-12-10 09:57:36,533 -> 01.Indexing points,7.54,59143,20.0,7
2017-12-10 09:57:41,541 -> 02.Indexing centers,5.01,142992,20.0,7
2017-12-10 09:57:41,550 -> 1024
2017-12-10 09:57:41,556 -> 1024
2017-12-10 09:57:58,394 -> 03.Joining datasets,16.84,142988,20.0,7
Done!!! Sun Dec 10 09:57:58 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 09:58:06,741 -> Starting session,1.62,0
2017-12-10 09:58:06,742 -> Setting variables,0.00,0
2017-12-10 09:58:11,501 -> Reading datasets,4.76,0
2017-12-10 09:58:11,516 -> Points partitions: 2
2017-12-10 09:58:11,527 -> Centers partitions: 2
2017-12-10 09:58:18,679 -> 01.Indexing points,7.11,59143,20.0,14
2017-12-10 09:58:23,561 -> 02.Indexing centers,4.88,142992,20.0,14
2017-12-10 09:58:23,570 -> 1024
2017-12-10 09:58:23,575 -> 1024
2017-12-10 09:58:36,651 -> 03.Joining datasets,13.08,142988,20.0,14
Done!!! Sun Dec 10 09:58:37 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 09:58:39,357 -> Starting session,1.66,0
2017-12-10 09:58:39,357 -> Setting variables,0.00,0
2017-12-10 09:58:44,095 -> Reading datasets,4.74,0
2017-12-10 09:58:44,106 -> Points partitions: 2
2017-12-10 09:58:44,114 -> Centers partitions: 2
2017-12-10 09:58:51,332 -> 01.Indexing points,7.18,59143,20.0,14
2017-12-10 09:58:56,353 -> 02.Indexing centers,5.02,142992,20.0,14
2017-12-10 09:58:56,364 -> 1024
2017-12-10 09:58:56,372 -> 1024
2017-12-10 09:59:09,517 -> 03.Joining datasets,13.14,142988,20.0,14
Done!!! Sun Dec 10 09:59:09 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 09:59:12,292 -> Starting session,1.57,0
2017-12-10 09:59:12,293 -> Setting variables,0.00,0
2017-12-10 09:59:16,956 -> Reading datasets,4.66,0
2017-12-10 09:59:16,969 -> Points partitions: 2
2017-12-10 09:59:16,978 -> Centers partitions: 2
2017-12-10 09:59:24,689 -> 01.Indexing points,7.67,59143,20.0,14
2017-12-10 09:59:29,696 -> 02.Indexing centers,5.01,142992,20.0,14
2017-12-10 09:59:29,704 -> 1024
2017-12-10 09:59:29,711 -> 1024
2017-12-10 09:59:42,836 -> 03.Joining datasets,13.13,142988,20.0,14
Done!!! Sun Dec 10 09:59:43 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 09:59:45,633 -> Starting session,1.69,0
2017-12-10 09:59:45,634 -> Setting variables,0.00,0
2017-12-10 09:59:50,273 -> Reading datasets,4.64,0
2017-12-10 09:59:50,284 -> Points partitions: 2
2017-12-10 09:59:50,292 -> Centers partitions: 2
2017-12-10 09:59:57,644 -> 01.Indexing points,7.31,59143,20.0,14
2017-12-10 10:00:02,613 -> 02.Indexing centers,4.97,142992,20.0,14
2017-12-10 10:00:02,621 -> 1024
2017-12-10 10:00:02,627 -> 1024
2017-12-10 10:00:15,576 -> 03.Joining datasets,12.95,142988,20.0,14
Done!!! Sun Dec 10 10:00:16 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:00:18,405 -> Starting session,1.57,0
2017-12-10 10:00:18,406 -> Setting variables,0.00,0
2017-12-10 10:00:23,107 -> Reading datasets,4.70,0
2017-12-10 10:00:23,120 -> Points partitions: 2
2017-12-10 10:00:23,131 -> Centers partitions: 2
2017-12-10 10:00:30,450 -> 01.Indexing points,7.28,59143,20.0,14
2017-12-10 10:00:35,260 -> 02.Indexing centers,4.81,142992,20.0,14
2017-12-10 10:00:35,267 -> 1024
2017-12-10 10:00:35,274 -> 1024
2017-12-10 10:00:48,854 -> 03.Joining datasets,13.58,142988,20.0,14
Done!!! Sun Dec 10 10:00:49 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:00:57,088 -> Starting session,1.63,0
2017-12-10 10:00:57,090 -> Setting variables,0.00,0
2017-12-10 10:01:02,866 -> Reading datasets,5.78,0
2017-12-10 10:01:02,886 -> Points partitions: 2
2017-12-10 10:01:02,902 -> Centers partitions: 2
2017-12-10 10:01:09,581 -> 01.Indexing points,6.63,59143,20.0,21
2017-12-10 10:01:14,131 -> 02.Indexing centers,4.55,142992,20.0,21
2017-12-10 10:01:14,141 -> 1024
2017-12-10 10:01:14,148 -> 1024
2017-12-10 10:01:24,966 -> 03.Joining datasets,10.82,142988,20.0,21
Done!!! Sun Dec 10 10:01:25 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:01:27,818 -> Starting session,1.60,0
2017-12-10 10:01:27,819 -> Setting variables,0.00,0
2017-12-10 10:01:33,611 -> Reading datasets,5.79,0
2017-12-10 10:01:33,629 -> Points partitions: 2
2017-12-10 10:01:33,640 -> Centers partitions: 2
2017-12-10 10:01:40,527 -> 01.Indexing points,6.83,59143,20.0,21
2017-12-10 10:01:45,026 -> 02.Indexing centers,4.50,142992,20.0,21
2017-12-10 10:01:45,038 -> 1024
2017-12-10 10:01:45,047 -> 1024
2017-12-10 10:01:55,730 -> 03.Joining datasets,10.68,142988,20.0,21
Done!!! Sun Dec 10 10:01:56 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:01:58,402 -> Starting session,1.47,0
2017-12-10 10:01:58,402 -> Setting variables,0.00,0
2017-12-10 10:02:04,229 -> Reading datasets,5.83,0
2017-12-10 10:02:04,248 -> Points partitions: 2
2017-12-10 10:02:04,260 -> Centers partitions: 2
2017-12-10 10:02:11,258 -> 01.Indexing points,6.95,59143,20.0,21
2017-12-10 10:02:16,208 -> 02.Indexing centers,4.95,142992,20.0,21
2017-12-10 10:02:16,217 -> 1024
2017-12-10 10:02:16,223 -> 1024
2017-12-10 10:02:27,172 -> 03.Joining datasets,10.95,142988,20.0,21
Done!!! Sun Dec 10 10:02:27 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:02:29,857 -> Starting session,1.47,0
2017-12-10 10:02:29,858 -> Setting variables,0.00,0
2017-12-10 10:02:35,710 -> Reading datasets,5.85,0
2017-12-10 10:02:35,727 -> Points partitions: 2
2017-12-10 10:02:35,735 -> Centers partitions: 2
2017-12-10 10:02:42,644 -> 01.Indexing points,6.86,59143,20.0,21
2017-12-10 10:02:46,777 -> 02.Indexing centers,4.13,142992,20.0,21
2017-12-10 10:02:46,785 -> 1024
2017-12-10 10:02:46,792 -> 1024
2017-12-10 10:02:58,043 -> 03.Joining datasets,11.25,142988,20.0,21
Done!!! Sun Dec 10 10:02:58 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:03:00,948 -> Starting session,1.60,0
2017-12-10 10:03:00,949 -> Setting variables,0.00,0
2017-12-10 10:03:05,626 -> Reading datasets,4.68,0
2017-12-10 10:03:05,643 -> Points partitions: 2
2017-12-10 10:03:05,656 -> Centers partitions: 2
2017-12-10 10:03:13,578 -> 01.Indexing points,7.87,59143,20.0,21
2017-12-10 10:03:17,997 -> 02.Indexing centers,4.42,142992,20.0,21
2017-12-10 10:03:18,006 -> 1024
2017-12-10 10:03:18,013 -> 1024
2017-12-10 10:03:28,364 -> 03.Joining datasets,10.35,142988,20.0,21
Done!!! Sun Dec 10 10:03:28 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:03:36,800 -> Starting session,1.68,0
2017-12-10 10:03:36,801 -> Setting variables,0.00,0
2017-12-10 10:03:43,706 -> Reading datasets,6.91,0
2017-12-10 10:03:43,728 -> Points partitions: 2
2017-12-10 10:03:43,744 -> Centers partitions: 2
2017-12-10 10:03:52,157 -> 01.Indexing points,8.36,59143,20.0,28
2017-12-10 10:03:57,055 -> 02.Indexing centers,4.90,142992,20.0,28
2017-12-10 10:03:57,065 -> 1024
2017-12-10 10:03:57,073 -> 1024
2017-12-10 10:04:06,672 -> 03.Joining datasets,9.60,142988,20.0,28
Done!!! Sun Dec 10 10:04:07 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:04:10,187 -> Starting session,1.50,0
2017-12-10 10:04:10,187 -> Setting variables,0.00,0
2017-12-10 10:04:15,883 -> Reading datasets,5.69,0
2017-12-10 10:04:15,896 -> Points partitions: 2
2017-12-10 10:04:15,906 -> Centers partitions: 2
2017-12-10 10:04:25,194 -> 01.Indexing points,9.25,59143,20.0,28
2017-12-10 10:04:30,245 -> 02.Indexing centers,5.05,142992,20.0,28
2017-12-10 10:04:30,260 -> 1024
2017-12-10 10:04:30,268 -> 1024
2017-12-10 10:04:40,525 -> 03.Joining datasets,10.26,142988,20.0,28
Done!!! Sun Dec 10 10:04:41 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:04:43,348 -> Starting session,1.69,0
2017-12-10 10:04:43,349 -> Setting variables,0.00,0
2017-12-10 10:04:50,507 -> Reading datasets,7.16,0
2017-12-10 10:04:50,525 -> Points partitions: 2
2017-12-10 10:04:50,535 -> Centers partitions: 2
2017-12-10 10:04:58,756 -> 01.Indexing points,8.17,59143,20.0,28
2017-12-10 10:05:03,519 -> 02.Indexing centers,4.76,142992,20.0,28
2017-12-10 10:05:03,529 -> 1024
2017-12-10 10:05:03,534 -> 1024
2017-12-10 10:05:12,777 -> 03.Joining datasets,9.24,142988,20.0,28
Done!!! Sun Dec 10 10:05:13 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:05:15,454 -> Starting session,1.56,0
2017-12-10 10:05:15,454 -> Setting variables,0.00,0
2017-12-10 10:05:21,186 -> Reading datasets,5.73,0
2017-12-10 10:05:21,199 -> Points partitions: 2
2017-12-10 10:05:21,210 -> Centers partitions: 2
2017-12-10 10:05:30,226 -> 01.Indexing points,9.07,59143,20.0,28
2017-12-10 10:05:35,585 -> 02.Indexing centers,5.36,142992,20.0,28
2017-12-10 10:05:35,594 -> 1024
2017-12-10 10:05:35,600 -> 1024
2017-12-10 10:05:45,347 -> 03.Joining datasets,9.75,142988,20.0,28
Done!!! Sun Dec 10 10:05:45 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:05:48,051 -> Starting session,1.60,0
2017-12-10 10:05:48,052 -> Setting variables,0.00,0
2017-12-10 10:05:52,679 -> Reading datasets,4.63,0
2017-12-10 10:05:52,691 -> Points partitions: 2
2017-12-10 10:05:52,700 -> Centers partitions: 2
2017-12-10 10:06:02,935 -> 01.Indexing points,10.19,59143,20.0,28
2017-12-10 10:06:08,259 -> 02.Indexing centers,5.32,142992,20.0,28
2017-12-10 10:06:08,272 -> 1024
2017-12-10 10:06:08,282 -> 1024
2017-12-10 10:06:17,229 -> 03.Joining datasets,8.95,142988,20.0,28
Done!!! Sun Dec 10 10:06:17 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:06:25,712 -> Starting session,1.54,0
2017-12-10 10:06:25,713 -> Setting variables,0.00,0
2017-12-10 10:06:29,962 -> Reading datasets,4.25,0
2017-12-10 10:06:29,973 -> Points partitions: 2
2017-12-10 10:06:29,980 -> Centers partitions: 2
2017-12-10 10:06:37,255 -> 01.Indexing points,7.23,39429,20.0,7
2017-12-10 10:06:42,172 -> 02.Indexing centers,4.92,95328,20.0,7
2017-12-10 10:06:42,183 -> 1024
2017-12-10 10:06:42,192 -> 1024
2017-12-10 10:06:57,199 -> 03.Joining datasets,15.01,95325,20.0,7
Done!!! Sun Dec 10 10:06:57 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 10:06:59,893 -> Starting session,1.54,0
2017-12-10 10:06:59,893 -> Setting variables,0.00,0
2017-12-10 10:07:04,061 -> Reading datasets,4.17,0
2017-12-10 10:07:04,072 -> Points partitions: 2
2017-12-10 10:07:04,080 -> Centers partitions: 2
2017-12-10 10:07:11,825 -> 01.Indexing points,7.71,39429,20.0,7
2017-12-10 10:07:16,761 -> 02.Indexing centers,4.93,95328,20.0,7
2017-12-10 10:07:16,771 -> 1024
2017-12-10 10:07:16,779 -> 1024
2017-12-10 10:07:31,634 -> 03.Joining datasets,14.85,95325,20.0,7
Done!!! Sun Dec 10 10:07:32 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 10:07:34,212 -> Starting session,1.49,0
2017-12-10 10:07:34,213 -> Setting variables,0.00,0
2017-12-10 10:07:38,178 -> Reading datasets,3.96,0
2017-12-10 10:07:38,188 -> Points partitions: 2
2017-12-10 10:07:38,196 -> Centers partitions: 2
2017-12-10 10:07:45,089 -> 01.Indexing points,6.86,39429,20.0,7
2017-12-10 10:07:50,014 -> 02.Indexing centers,4.92,95328,20.0,7
2017-12-10 10:07:50,030 -> 1024
2017-12-10 10:07:50,048 -> 1024
2017-12-10 10:08:04,899 -> 03.Joining datasets,14.85,95325,20.0,7
Done!!! Sun Dec 10 10:08:05 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 10:08:07,630 -> Starting session,1.53,0
2017-12-10 10:08:07,630 -> Setting variables,0.00,0
2017-12-10 10:08:11,780 -> Reading datasets,4.15,0
2017-12-10 10:08:11,791 -> Points partitions: 2
2017-12-10 10:08:11,799 -> Centers partitions: 2
2017-12-10 10:08:18,901 -> 01.Indexing points,7.06,39429,20.0,7
2017-12-10 10:08:23,881 -> 02.Indexing centers,4.98,95328,20.0,7
2017-12-10 10:08:23,893 -> 1024
2017-12-10 10:08:23,902 -> 1024
2017-12-10 10:08:38,469 -> 03.Joining datasets,14.57,95325,20.0,7
Done!!! Sun Dec 10 10:08:38 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 10:08:41,193 -> Starting session,1.54,0
2017-12-10 10:08:41,194 -> Setting variables,0.00,0
2017-12-10 10:08:45,467 -> Reading datasets,4.27,0
2017-12-10 10:08:45,480 -> Points partitions: 2
2017-12-10 10:08:45,491 -> Centers partitions: 2
2017-12-10 10:08:52,393 -> 01.Indexing points,6.86,39429,20.0,7
2017-12-10 10:08:57,161 -> 02.Indexing centers,4.77,95328,20.0,7
2017-12-10 10:08:57,174 -> 1024
2017-12-10 10:08:57,184 -> 1024
2017-12-10 10:09:12,028 -> 03.Joining datasets,14.84,95325,20.0,7
Done!!! Sun Dec 10 10:09:12 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 10:09:20,427 -> Starting session,1.63,0
2017-12-10 10:09:20,428 -> Setting variables,0.00,0
2017-12-10 10:09:25,179 -> Reading datasets,4.75,0
2017-12-10 10:09:25,190 -> Points partitions: 2
2017-12-10 10:09:25,197 -> Centers partitions: 2
2017-12-10 10:09:32,045 -> 01.Indexing points,6.81,39429,20.0,14
2017-12-10 10:09:36,632 -> 02.Indexing centers,4.59,95328,20.0,14
2017-12-10 10:09:36,641 -> 1024
2017-12-10 10:09:36,650 -> 1024
2017-12-10 10:09:48,664 -> 03.Joining datasets,12.01,95325,20.0,14
Done!!! Sun Dec 10 10:09:49 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 10:09:51,729 -> Starting session,1.84,0
2017-12-10 10:09:51,730 -> Setting variables,0.00,0
2017-12-10 10:09:56,509 -> Reading datasets,4.78,0
2017-12-10 10:09:56,524 -> Points partitions: 2
2017-12-10 10:09:56,535 -> Centers partitions: 2
2017-12-10 10:10:03,536 -> 01.Indexing points,6.96,39429,20.0,14
2017-12-10 10:10:08,153 -> 02.Indexing centers,4.62,95328,20.0,14
2017-12-10 10:10:08,163 -> 1024
2017-12-10 10:10:08,169 -> 1024
2017-12-10 10:10:20,679 -> 03.Joining datasets,12.51,95325,20.0,14
Done!!! Sun Dec 10 10:10:21 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 10:10:23,435 -> Starting session,1.51,0
2017-12-10 10:10:23,436 -> Setting variables,0.00,0
2017-12-10 10:10:28,006 -> Reading datasets,4.57,0
2017-12-10 10:10:28,019 -> Points partitions: 2
2017-12-10 10:10:28,028 -> Centers partitions: 2
2017-12-10 10:10:35,053 -> 01.Indexing points,6.98,39429,20.0,14
2017-12-10 10:10:39,788 -> 02.Indexing centers,4.73,95328,20.0,14
2017-12-10 10:10:39,798 -> 1024
2017-12-10 10:10:39,807 -> 1024
2017-12-10 10:10:51,953 -> 03.Joining datasets,12.22,95325,20.0,14
Done!!! Sun Dec 10 10:10:52 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 10:10:54,723 -> Starting session,1.58,0
2017-12-10 10:10:54,724 -> Setting variables,0.00,0
2017-12-10 10:10:59,533 -> Reading datasets,4.81,0
2017-12-10 10:10:59,544 -> Points partitions: 2
2017-12-10 10:10:59,551 -> Centers partitions: 2
2017-12-10 10:11:06,492 -> 01.Indexing points,6.90,39429,20.0,14
2017-12-10 10:11:11,184 -> 02.Indexing centers,4.69,95328,20.0,14
2017-12-10 10:11:11,192 -> 1024
2017-12-10 10:11:11,197 -> 1024
2017-12-10 10:11:23,596 -> 03.Joining datasets,12.40,95325,20.0,14
Done!!! Sun Dec 10 10:11:24 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:11:26,299 -> Starting session,1.60,0
2017-12-10 10:11:26,299 -> Setting variables,0.00,0
2017-12-10 10:11:30,688 -> Reading datasets,4.39,0
2017-12-10 10:11:30,699 -> Points partitions: 2
2017-12-10 10:11:30,707 -> Centers partitions: 2
2017-12-10 10:11:37,668 -> 01.Indexing points,6.80,39429,20.0,14
2017-12-10 10:11:42,395 -> 02.Indexing centers,4.73,95328,20.0,14
2017-12-10 10:11:42,407 -> 1024
2017-12-10 10:11:42,417 -> 1024
2017-12-10 10:11:55,280 -> 03.Joining datasets,12.86,95325,20.0,14
Done!!! Sun Dec 10 10:11:55 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:12:03,748 -> Starting session,1.68,0
2017-12-10 10:12:03,749 -> Setting variables,0.00,0
2017-12-10 10:12:09,573 -> Reading datasets,5.82,0
2017-12-10 10:12:09,591 -> Points partitions: 2
2017-12-10 10:12:09,601 -> Centers partitions: 2
2017-12-10 10:12:16,075 -> 01.Indexing points,6.43,39429,20.0,21
2017-12-10 10:12:20,407 -> 02.Indexing centers,4.33,95328,20.0,21
2017-12-10 10:12:20,416 -> 1024
2017-12-10 10:12:20,424 -> 1024
2017-12-10 10:12:30,660 -> 03.Joining datasets,10.24,95325,20.0,21
Done!!! Sun Dec 10 10:12:31 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:12:33,510 -> Starting session,1.74,0
2017-12-10 10:12:33,511 -> Setting variables,0.00,0
2017-12-10 10:12:39,093 -> Reading datasets,5.58,0
2017-12-10 10:12:39,110 -> Points partitions: 2
2017-12-10 10:12:39,120 -> Centers partitions: 2
2017-12-10 10:12:45,096 -> 01.Indexing points,5.83,39429,20.0,21
2017-12-10 10:12:49,031 -> 02.Indexing centers,3.93,95328,20.0,21
2017-12-10 10:12:49,041 -> 1024
2017-12-10 10:12:49,049 -> 1024
2017-12-10 10:12:59,279 -> 03.Joining datasets,10.23,95325,20.0,21
Done!!! Sun Dec 10 10:12:59 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:13:02,172 -> Starting session,1.67,0
2017-12-10 10:13:02,172 -> Setting variables,0.00,0
2017-12-10 10:13:07,796 -> Reading datasets,5.62,0
2017-12-10 10:13:07,811 -> Points partitions: 2
2017-12-10 10:13:07,819 -> Centers partitions: 2
2017-12-10 10:13:14,235 -> 01.Indexing points,6.37,39429,20.0,21
2017-12-10 10:13:18,242 -> 02.Indexing centers,4.01,95328,20.0,21
2017-12-10 10:13:18,251 -> 1024
2017-12-10 10:13:18,257 -> 1024
2017-12-10 10:13:28,060 -> 03.Joining datasets,9.80,95325,20.0,21
Done!!! Sun Dec 10 10:13:28 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:13:30,890 -> Starting session,1.64,0
2017-12-10 10:13:30,891 -> Setting variables,0.00,0
2017-12-10 10:13:36,617 -> Reading datasets,5.73,0
2017-12-10 10:13:36,631 -> Points partitions: 2
2017-12-10 10:13:36,643 -> Centers partitions: 2
2017-12-10 10:13:42,913 -> 01.Indexing points,6.23,39429,20.0,21
2017-12-10 10:13:47,058 -> 02.Indexing centers,4.14,95328,20.0,21
2017-12-10 10:13:47,067 -> 1024
2017-12-10 10:13:47,074 -> 1024
2017-12-10 10:13:56,741 -> 03.Joining datasets,9.67,95325,20.0,21
Done!!! Sun Dec 10 10:13:57 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:13:59,700 -> Starting session,1.82,0
2017-12-10 10:13:59,701 -> Setting variables,0.00,0
2017-12-10 10:14:05,504 -> Reading datasets,5.80,0
2017-12-10 10:14:05,517 -> Points partitions: 2
2017-12-10 10:14:05,527 -> Centers partitions: 2
2017-12-10 10:14:12,538 -> 01.Indexing points,6.97,39429,20.0,21
2017-12-10 10:14:16,555 -> 02.Indexing centers,4.01,95328,20.0,21
2017-12-10 10:14:16,563 -> 1024
2017-12-10 10:14:16,569 -> 1024
2017-12-10 10:14:26,091 -> 03.Joining datasets,9.52,95325,20.0,21
Done!!! Sun Dec 10 10:14:27 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:14:35,131 -> Starting session,1.62,0
2017-12-10 10:14:35,132 -> Setting variables,0.00,0
2017-12-10 10:14:40,939 -> Reading datasets,5.81,0
2017-12-10 10:14:40,952 -> Points partitions: 2
2017-12-10 10:14:40,965 -> Centers partitions: 2
2017-12-10 10:14:49,917 -> 01.Indexing points,8.91,39429,20.0,28
2017-12-10 10:14:53,715 -> 02.Indexing centers,3.80,95328,20.0,28
2017-12-10 10:14:53,726 -> 1024
2017-12-10 10:14:53,735 -> 1024
2017-12-10 10:15:02,421 -> 03.Joining datasets,8.69,95325,20.0,28
Done!!! Sun Dec 10 10:15:02 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:15:05,291 -> Starting session,1.60,0
2017-12-10 10:15:05,292 -> Setting variables,0.00,0
2017-12-10 10:15:12,218 -> Reading datasets,6.93,0
2017-12-10 10:15:12,238 -> Points partitions: 2
2017-12-10 10:15:12,255 -> Centers partitions: 2
2017-12-10 10:15:19,091 -> 01.Indexing points,6.77,39429,20.0,28
2017-12-10 10:15:24,375 -> 02.Indexing centers,4.84,95328,20.0,28
2017-12-10 10:15:24,386 -> 1024
2017-12-10 10:15:24,393 -> 1024
2017-12-10 10:15:32,603 -> 03.Joining datasets,8.21,95325,20.0,28
Done!!! Sun Dec 10 10:15:33 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:15:35,384 -> Starting session,1.62,0
2017-12-10 10:15:35,385 -> Setting variables,0.00,0
2017-12-10 10:15:42,533 -> Reading datasets,7.15,0
2017-12-10 10:15:42,547 -> Points partitions: 2
2017-12-10 10:15:42,557 -> Centers partitions: 2
2017-12-10 10:15:50,392 -> 01.Indexing points,7.80,39429,20.0,28
2017-12-10 10:15:54,901 -> 02.Indexing centers,4.51,95328,20.0,28
2017-12-10 10:15:54,911 -> 1024
2017-12-10 10:15:54,917 -> 1024
2017-12-10 10:16:05,145 -> 03.Joining datasets,10.23,95325,20.0,28
Done!!! Sun Dec 10 10:16:05 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:16:07,955 -> Starting session,1.61,0
2017-12-10 10:16:07,956 -> Setting variables,0.00,0
2017-12-10 10:16:12,766 -> Reading datasets,4.81,0
2017-12-10 10:16:12,780 -> Points partitions: 2
2017-12-10 10:16:12,790 -> Centers partitions: 2
2017-12-10 10:16:22,136 -> 01.Indexing points,9.30,39429,20.0,28
2017-12-10 10:16:26,305 -> 02.Indexing centers,4.17,95328,20.0,28
2017-12-10 10:16:26,315 -> 1024
2017-12-10 10:16:26,324 -> 1024
2017-12-10 10:16:35,585 -> 03.Joining datasets,9.26,95325,20.0,28
Done!!! Sun Dec 10 10:16:36 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:16:39,135 -> Starting session,1.58,0
2017-12-10 10:16:39,136 -> Setting variables,0.00,0
2017-12-10 10:16:45,993 -> Reading datasets,6.86,0
2017-12-10 10:16:46,007 -> Points partitions: 2
2017-12-10 10:16:46,017 -> Centers partitions: 2
2017-12-10 10:16:52,969 -> 01.Indexing points,6.90,39429,20.0,28
2017-12-10 10:16:57,038 -> 02.Indexing centers,4.07,95328,20.0,28
2017-12-10 10:16:57,046 -> 1024
2017-12-10 10:16:57,052 -> 1024
2017-12-10 10:17:06,548 -> 03.Joining datasets,9.50,95325,20.0,28
Done!!! Sun Dec 10 10:17:07 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:17:15,036 -> Starting session,1.55,0
2017-12-10 10:17:15,037 -> Setting variables,0.00,0
2017-12-10 10:17:19,171 -> Reading datasets,4.13,0
2017-12-10 10:17:19,182 -> Points partitions: 2
2017-12-10 10:17:19,189 -> Centers partitions: 2
2017-12-10 10:17:25,999 -> 01.Indexing points,6.77,19715,20.0,7
2017-12-10 10:17:30,458 -> 02.Indexing centers,4.46,47664,20.0,7
2017-12-10 10:17:30,471 -> 992
2017-12-10 10:17:30,481 -> 1024
2017-12-10 10:17:43,296 -> 03.Joining datasets,12.81,47663,20.0,7
Done!!! Sun Dec 10 10:17:43 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 10:17:45,997 -> Starting session,1.63,0
2017-12-10 10:17:45,997 -> Setting variables,0.00,0
2017-12-10 10:17:50,162 -> Reading datasets,4.16,0
2017-12-10 10:17:50,176 -> Points partitions: 2
2017-12-10 10:17:50,187 -> Centers partitions: 2
2017-12-10 10:17:56,746 -> 01.Indexing points,6.52,19715,20.0,7
2017-12-10 10:18:00,967 -> 02.Indexing centers,4.22,47664,20.0,7
2017-12-10 10:18:00,980 -> 992
2017-12-10 10:18:00,989 -> 1024
2017-12-10 10:18:13,932 -> 03.Joining datasets,12.94,47663,20.0,7
Done!!! Sun Dec 10 10:18:14 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 10:18:16,681 -> Starting session,1.70,0
2017-12-10 10:18:16,682 -> Setting variables,0.00,0
2017-12-10 10:18:20,787 -> Reading datasets,4.11,0
2017-12-10 10:18:20,801 -> Points partitions: 2
2017-12-10 10:18:20,812 -> Centers partitions: 2
2017-12-10 10:18:27,921 -> 01.Indexing points,7.07,19715,20.0,7
2017-12-10 10:18:32,543 -> 02.Indexing centers,4.62,47664,20.0,7
2017-12-10 10:18:32,553 -> 992
2017-12-10 10:18:32,561 -> 1024
2017-12-10 10:18:45,325 -> 03.Joining datasets,12.76,47663,20.0,7
Done!!! Sun Dec 10 10:18:45 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 10:18:48,163 -> Starting session,1.72,0
2017-12-10 10:18:48,163 -> Setting variables,0.00,0
2017-12-10 10:18:52,063 -> Reading datasets,3.90,0
2017-12-10 10:18:52,080 -> Points partitions: 2
2017-12-10 10:18:52,092 -> Centers partitions: 2
2017-12-10 10:18:58,354 -> 01.Indexing points,6.21,19715,20.0,7
2017-12-10 10:19:02,464 -> 02.Indexing centers,4.11,47664,20.0,7
2017-12-10 10:19:02,475 -> 992
2017-12-10 10:19:02,481 -> 1024
2017-12-10 10:19:15,328 -> 03.Joining datasets,12.85,47663,20.0,7
Done!!! Sun Dec 10 10:19:15 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 10:19:18,080 -> Starting session,1.61,0
2017-12-10 10:19:18,081 -> Setting variables,0.00,0
2017-12-10 10:19:22,266 -> Reading datasets,4.19,0
2017-12-10 10:19:22,277 -> Points partitions: 2
2017-12-10 10:19:22,284 -> Centers partitions: 2
2017-12-10 10:19:29,227 -> 01.Indexing points,6.91,19715,20.0,7
2017-12-10 10:19:33,945 -> 02.Indexing centers,4.72,47664,20.0,7
2017-12-10 10:19:33,957 -> 992
2017-12-10 10:19:33,981 -> 1024
2017-12-10 10:19:46,968 -> 03.Joining datasets,12.99,47663,20.0,7
Done!!! Sun Dec 10 10:19:47 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 10:19:55,399 -> Starting session,1.70,0
2017-12-10 10:19:55,401 -> Setting variables,0.00,0
2017-12-10 10:20:00,139 -> Reading datasets,4.74,0
2017-12-10 10:20:00,154 -> Points partitions: 2
2017-12-10 10:20:00,164 -> Centers partitions: 2
2017-12-10 10:20:07,045 -> 01.Indexing points,6.84,19715,20.0,14
2017-12-10 10:20:11,124 -> 02.Indexing centers,4.08,47664,20.0,14
2017-12-10 10:20:11,135 -> 992
2017-12-10 10:20:11,144 -> 1024
2017-12-10 10:20:22,336 -> 03.Joining datasets,11.19,47663,20.0,14
Done!!! Sun Dec 10 10:20:23 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 10:20:26,028 -> Starting session,1.59,0
2017-12-10 10:20:26,029 -> Setting variables,0.00,0
2017-12-10 10:20:30,903 -> Reading datasets,4.87,0
2017-12-10 10:20:30,914 -> Points partitions: 2
2017-12-10 10:20:30,921 -> Centers partitions: 2
2017-12-10 10:20:37,972 -> 01.Indexing points,7.01,19715,20.0,14
2017-12-10 10:20:42,228 -> 02.Indexing centers,4.25,47664,20.0,14
2017-12-10 10:20:42,240 -> 992
2017-12-10 10:20:42,248 -> 1024
2017-12-10 10:20:52,483 -> 03.Joining datasets,10.23,47663,20.0,14
Done!!! Sun Dec 10 10:20:52 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 10:20:55,372 -> Starting session,1.69,0
2017-12-10 10:20:55,373 -> Setting variables,0.00,0
2017-12-10 10:21:00,088 -> Reading datasets,4.72,0
2017-12-10 10:21:00,101 -> Points partitions: 2
2017-12-10 10:21:00,112 -> Centers partitions: 2
2017-12-10 10:21:06,768 -> 01.Indexing points,6.61,19715,20.0,14
2017-12-10 10:21:11,062 -> 02.Indexing centers,4.29,47664,20.0,14
2017-12-10 10:21:11,071 -> 992
2017-12-10 10:21:11,077 -> 1024
2017-12-10 10:21:21,732 -> 03.Joining datasets,10.65,47663,20.0,14
Done!!! Sun Dec 10 10:21:22 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 10:21:24,520 -> Starting session,1.54,0
2017-12-10 10:21:24,521 -> Setting variables,0.00,0
2017-12-10 10:21:29,182 -> Reading datasets,4.66,0
2017-12-10 10:21:29,196 -> Points partitions: 2
2017-12-10 10:21:29,208 -> Centers partitions: 2
2017-12-10 10:21:36,381 -> 01.Indexing points,7.13,19715,20.0,14
2017-12-10 10:21:40,333 -> 02.Indexing centers,3.95,47664,20.0,14
2017-12-10 10:21:40,341 -> 992
2017-12-10 10:21:40,347 -> 1024
2017-12-10 10:21:50,908 -> 03.Joining datasets,10.56,47663,20.0,14
Done!!! Sun Dec 10 10:21:51 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:21:53,744 -> Starting session,1.74,0
2017-12-10 10:21:53,744 -> Setting variables,0.00,0
2017-12-10 10:21:58,346 -> Reading datasets,4.60,0
2017-12-10 10:21:58,359 -> Points partitions: 2
2017-12-10 10:21:58,370 -> Centers partitions: 2
2017-12-10 10:22:05,296 -> 01.Indexing points,6.89,19715,20.0,14
2017-12-10 10:22:09,494 -> 02.Indexing centers,4.20,47664,20.0,14
2017-12-10 10:22:09,508 -> 992
2017-12-10 10:22:09,515 -> 1024
2017-12-10 10:22:20,714 -> 03.Joining datasets,11.20,47663,20.0,14
Done!!! Sun Dec 10 10:22:21 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:22:29,924 -> Starting session,1.67,0
2017-12-10 10:22:29,925 -> Setting variables,0.00,0
2017-12-10 10:22:35,697 -> Reading datasets,5.77,0
2017-12-10 10:22:35,717 -> Points partitions: 2
2017-12-10 10:22:35,730 -> Centers partitions: 2
2017-12-10 10:22:40,732 -> 01.Indexing points,4.94,19715,20.0,21
2017-12-10 10:22:44,672 -> 02.Indexing centers,3.94,47664,20.0,21
2017-12-10 10:22:44,680 -> 992
2017-12-10 10:22:44,686 -> 1024
2017-12-10 10:22:53,085 -> 03.Joining datasets,8.40,47663,20.0,21
Done!!! Sun Dec 10 10:22:53 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:22:55,892 -> Starting session,1.61,0
2017-12-10 10:22:55,893 -> Setting variables,0.00,0
2017-12-10 10:23:00,753 -> Reading datasets,4.86,0
2017-12-10 10:23:00,765 -> Points partitions: 2
2017-12-10 10:23:00,773 -> Centers partitions: 2
2017-12-10 10:23:08,779 -> 01.Indexing points,7.96,19715,20.0,21
2017-12-10 10:23:12,812 -> 02.Indexing centers,4.03,47664,20.0,21
2017-12-10 10:23:12,820 -> 992
2017-12-10 10:23:12,827 -> 1024
2017-12-10 10:23:22,651 -> 03.Joining datasets,9.82,47663,20.0,21
Done!!! Sun Dec 10 10:23:23 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:23:25,564 -> Starting session,1.73,0
2017-12-10 10:23:25,565 -> Setting variables,0.00,0
2017-12-10 10:23:30,156 -> Reading datasets,4.59,0
2017-12-10 10:23:30,167 -> Points partitions: 2
2017-12-10 10:23:30,174 -> Centers partitions: 2
2017-12-10 10:23:38,102 -> 01.Indexing points,7.89,19715,20.0,21
2017-12-10 10:23:42,186 -> 02.Indexing centers,4.08,47664,20.0,21
2017-12-10 10:23:42,198 -> 992
2017-12-10 10:23:42,204 -> 1024
2017-12-10 10:23:51,873 -> 03.Joining datasets,9.67,47663,20.0,21
Done!!! Sun Dec 10 10:23:52 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:23:54,570 -> Starting session,1.47,0
2017-12-10 10:23:54,571 -> Setting variables,0.00,0
2017-12-10 10:23:59,178 -> Reading datasets,4.75,0
2017-12-10 10:23:59,192 -> Points partitions: 2
2017-12-10 10:23:59,203 -> Centers partitions: 2
2017-12-10 10:24:05,099 -> 01.Indexing points,5.85,19715,20.0,21
2017-12-10 10:24:09,042 -> 02.Indexing centers,3.94,47664,20.0,21
2017-12-10 10:24:09,050 -> 992
2017-12-10 10:24:09,056 -> 1024
2017-12-10 10:24:17,592 -> 03.Joining datasets,8.54,47663,20.0,21
Done!!! Sun Dec 10 10:24:18 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:24:20,295 -> Starting session,1.51,0
2017-12-10 10:24:20,295 -> Setting variables,0.00,0
2017-12-10 10:24:25,978 -> Reading datasets,5.68,0
2017-12-10 10:24:25,989 -> Points partitions: 2
2017-12-10 10:24:25,998 -> Centers partitions: 2
2017-12-10 10:24:33,337 -> 01.Indexing points,7.30,19715,20.0,21
2017-12-10 10:24:37,397 -> 02.Indexing centers,4.06,47664,20.0,21
2017-12-10 10:24:37,409 -> 992
2017-12-10 10:24:37,419 -> 1024
2017-12-10 10:24:48,040 -> 03.Joining datasets,10.62,47663,20.0,21
Done!!! Sun Dec 10 10:24:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:24:56,536 -> Starting session,1.83,0
2017-12-10 10:24:56,537 -> Setting variables,0.00,0
2017-12-10 10:25:03,551 -> Reading datasets,7.01,0
2017-12-10 10:25:03,570 -> Points partitions: 2
2017-12-10 10:25:03,581 -> Centers partitions: 2
2017-12-10 10:25:10,627 -> 01.Indexing points,6.99,19715,20.0,28
2017-12-10 10:25:14,801 -> 02.Indexing centers,4.17,47664,20.0,28
2017-12-10 10:25:14,814 -> 992
2017-12-10 10:25:14,822 -> 1024
2017-12-10 10:25:24,006 -> 03.Joining datasets,9.18,47663,20.0,28
Done!!! Sun Dec 10 10:25:24 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:25:26,967 -> Starting session,1.69,0
2017-12-10 10:25:26,967 -> Setting variables,0.00,0
2017-12-10 10:25:33,975 -> Reading datasets,7.01,0
2017-12-10 10:25:33,993 -> Points partitions: 2
2017-12-10 10:25:34,006 -> Centers partitions: 2
2017-12-10 10:25:41,250 -> 01.Indexing points,7.20,19715,20.0,28
2017-12-10 10:25:45,412 -> 02.Indexing centers,4.16,47664,20.0,28
2017-12-10 10:25:45,422 -> 992
2017-12-10 10:25:45,429 -> 1024
2017-12-10 10:25:54,832 -> 03.Joining datasets,9.40,47663,20.0,28
Done!!! Sun Dec 10 10:25:55 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:25:57,671 -> Starting session,1.64,0
2017-12-10 10:25:57,673 -> Setting variables,0.00,0
2017-12-10 10:26:04,407 -> Reading datasets,6.73,0
2017-12-10 10:26:04,418 -> Points partitions: 2
2017-12-10 10:26:04,426 -> Centers partitions: 2
2017-12-10 10:26:12,998 -> 01.Indexing points,8.53,19715,20.0,28
2017-12-10 10:26:16,941 -> 02.Indexing centers,3.94,47664,20.0,28
2017-12-10 10:26:16,955 -> 992
2017-12-10 10:26:16,966 -> 1024
2017-12-10 10:26:26,941 -> 03.Joining datasets,9.98,47663,20.0,28
Done!!! Sun Dec 10 10:26:27 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:26:29,725 -> Starting session,1.48,0
2017-12-10 10:26:29,726 -> Setting variables,0.00,0
2017-12-10 10:26:35,834 -> Reading datasets,6.11,0
2017-12-10 10:26:35,853 -> Points partitions: 2
2017-12-10 10:26:35,864 -> Centers partitions: 2
2017-12-10 10:26:42,853 -> 01.Indexing points,6.93,19715,20.0,28
2017-12-10 10:26:47,815 -> 02.Indexing centers,4.96,47664,20.0,28
2017-12-10 10:26:47,825 -> 992
2017-12-10 10:26:47,833 -> 1024
2017-12-10 10:26:57,483 -> 03.Joining datasets,9.65,47663,20.0,28
Done!!! Sun Dec 10 10:26:57 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:27:00,264 -> Starting session,1.67,0
2017-12-10 10:27:00,264 -> Setting variables,0.00,0
2017-12-10 10:27:07,211 -> Reading datasets,6.95,0
2017-12-10 10:27:07,231 -> Points partitions: 2
2017-12-10 10:27:07,243 -> Centers partitions: 2
2017-12-10 10:27:13,409 -> 01.Indexing points,6.11,19715,20.0,28
2017-12-10 10:27:17,369 -> 02.Indexing centers,3.96,47664,20.0,28
2017-12-10 10:27:17,377 -> 992
2017-12-10 10:27:17,382 -> 1024
2017-12-10 10:27:27,265 -> 03.Joining datasets,9.88,47663,20.0,28
Done!!! Sun Dec 10 10:27:27 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:27:35,728 -> Starting session,1.53,0
2017-12-10 10:27:35,728 -> Setting variables,0.00,0
2017-12-10 10:27:40,081 -> Reading datasets,4.35,0
2017-12-10 10:27:40,092 -> Points partitions: 2
2017-12-10 10:27:40,099 -> Centers partitions: 2
2017-12-10 10:27:47,899 -> 01.Indexing points,7.76,78857,10.0,7
2017-12-10 10:27:52,483 -> 02.Indexing centers,4.58,89928,10.0,7
2017-12-10 10:27:52,496 -> 1024
2017-12-10 10:27:52,506 -> 1024
2017-12-10 10:28:08,072 -> 03.Joining datasets,15.57,89927,10.0,7
Done!!! Sun Dec 10 10:28:08 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 10:28:10,673 -> Starting session,1.38,0
2017-12-10 10:28:10,673 -> Setting variables,0.00,0
2017-12-10 10:28:15,009 -> Reading datasets,4.34,0
2017-12-10 10:28:15,021 -> Points partitions: 2
2017-12-10 10:28:15,030 -> Centers partitions: 2
2017-12-10 10:28:22,715 -> 01.Indexing points,7.65,78857,10.0,7
2017-12-10 10:28:27,126 -> 02.Indexing centers,4.41,89928,10.0,7
2017-12-10 10:28:27,135 -> 1024
2017-12-10 10:28:27,142 -> 1024
2017-12-10 10:28:42,092 -> 03.Joining datasets,14.95,89927,10.0,7
Done!!! Sun Dec 10 10:28:42 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 10:28:44,663 -> Starting session,1.37,0
2017-12-10 10:28:44,663 -> Setting variables,0.00,0
2017-12-10 10:28:49,141 -> Reading datasets,4.48,0
2017-12-10 10:28:49,152 -> Points partitions: 2
2017-12-10 10:28:49,160 -> Centers partitions: 2
2017-12-10 10:28:56,932 -> 01.Indexing points,7.74,78857,10.0,7
2017-12-10 10:29:01,348 -> 02.Indexing centers,4.41,89928,10.0,7
2017-12-10 10:29:01,358 -> 1024
2017-12-10 10:29:01,365 -> 1024
2017-12-10 10:29:16,474 -> 03.Joining datasets,15.16,89927,10.0,7
Done!!! Sun Dec 10 10:29:16 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 10:29:19,102 -> Starting session,1.53,0
2017-12-10 10:29:19,103 -> Setting variables,0.00,0
2017-12-10 10:29:23,409 -> Reading datasets,4.31,0
2017-12-10 10:29:23,420 -> Points partitions: 2
2017-12-10 10:29:23,427 -> Centers partitions: 2
2017-12-10 10:29:30,731 -> 01.Indexing points,7.27,78857,10.0,7
2017-12-10 10:29:35,369 -> 02.Indexing centers,4.64,89928,10.0,7
2017-12-10 10:29:35,490 -> 1024
2017-12-10 10:29:35,496 -> 1024
2017-12-10 10:29:50,704 -> 03.Joining datasets,15.21,89927,10.0,7
Done!!! Sun Dec 10 10:29:51 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 10:29:53,286 -> Starting session,1.47,0
2017-12-10 10:29:53,286 -> Setting variables,0.00,0
2017-12-10 10:29:57,519 -> Reading datasets,4.23,0
2017-12-10 10:29:57,530 -> Points partitions: 2
2017-12-10 10:29:57,538 -> Centers partitions: 2
2017-12-10 10:30:05,058 -> 01.Indexing points,7.48,78857,10.0,7
2017-12-10 10:30:09,562 -> 02.Indexing centers,4.50,89928,10.0,7
2017-12-10 10:30:09,573 -> 1024
2017-12-10 10:30:09,580 -> 1024
2017-12-10 10:30:24,937 -> 03.Joining datasets,15.43,89927,10.0,7
Done!!! Sun Dec 10 10:30:25 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 10:30:33,401 -> Starting session,1.73,0
2017-12-10 10:30:33,401 -> Setting variables,0.00,0
2017-12-10 10:30:38,247 -> Reading datasets,4.85,0
2017-12-10 10:30:38,257 -> Points partitions: 2
2017-12-10 10:30:38,264 -> Centers partitions: 2
2017-12-10 10:30:46,163 -> 01.Indexing points,7.86,78857,10.0,14
2017-12-10 10:30:50,477 -> 02.Indexing centers,4.31,89928,10.0,14
2017-12-10 10:30:50,486 -> 1024
2017-12-10 10:30:50,492 -> 1024
2017-12-10 10:31:02,817 -> 03.Joining datasets,12.32,89927,10.0,14
Done!!! Sun Dec 10 10:31:03 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 10:31:05,540 -> Starting session,1.61,0
2017-12-10 10:31:05,541 -> Setting variables,0.00,0
2017-12-10 10:31:10,239 -> Reading datasets,4.70,0
2017-12-10 10:31:10,253 -> Points partitions: 2
2017-12-10 10:31:10,262 -> Centers partitions: 2
2017-12-10 10:31:17,877 -> 01.Indexing points,7.58,78857,10.0,14
2017-12-10 10:31:22,146 -> 02.Indexing centers,4.27,89928,10.0,14
2017-12-10 10:31:22,157 -> 1024
2017-12-10 10:31:22,166 -> 1024
2017-12-10 10:31:34,478 -> 03.Joining datasets,12.31,89927,10.0,14
Done!!! Sun Dec 10 10:31:34 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 10:31:37,312 -> Starting session,1.64,0
2017-12-10 10:31:37,312 -> Setting variables,0.00,0
2017-12-10 10:31:42,042 -> Reading datasets,4.73,0
2017-12-10 10:31:42,057 -> Points partitions: 2
2017-12-10 10:31:42,068 -> Centers partitions: 2
2017-12-10 10:31:49,861 -> 01.Indexing points,7.75,78857,10.0,14
2017-12-10 10:31:54,265 -> 02.Indexing centers,4.40,89928,10.0,14
2017-12-10 10:31:54,278 -> 1024
2017-12-10 10:31:54,287 -> 1024
2017-12-10 10:32:06,269 -> 03.Joining datasets,11.98,89927,10.0,14
Done!!! Sun Dec 10 10:32:06 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 10:32:08,985 -> Starting session,1.60,0
2017-12-10 10:32:08,985 -> Setting variables,0.00,0
2017-12-10 10:32:13,560 -> Reading datasets,4.57,0
2017-12-10 10:32:13,571 -> Points partitions: 2
2017-12-10 10:32:13,579 -> Centers partitions: 2
2017-12-10 10:32:21,223 -> 01.Indexing points,7.61,78857,10.0,14
2017-12-10 10:32:25,685 -> 02.Indexing centers,4.46,89928,10.0,14
2017-12-10 10:32:25,695 -> 1024
2017-12-10 10:32:25,701 -> 1024
2017-12-10 10:32:37,358 -> 03.Joining datasets,11.66,89927,10.0,14
Done!!! Sun Dec 10 10:32:37 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:32:40,444 -> Starting session,1.93,0
2017-12-10 10:32:40,444 -> Setting variables,0.00,0
2017-12-10 10:32:45,084 -> Reading datasets,4.64,0
2017-12-10 10:32:45,098 -> Points partitions: 2
2017-12-10 10:32:45,108 -> Centers partitions: 2
2017-12-10 10:32:53,017 -> 01.Indexing points,7.87,78857,10.0,14
2017-12-10 10:32:57,572 -> 02.Indexing centers,4.55,89928,10.0,14
2017-12-10 10:32:57,579 -> 1024
2017-12-10 10:32:57,585 -> 1024
2017-12-10 10:33:09,341 -> 03.Joining datasets,11.76,89927,10.0,14
Done!!! Sun Dec 10 10:33:09 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:33:17,604 -> Starting session,1.69,0
2017-12-10 10:33:17,605 -> Setting variables,0.00,0
2017-12-10 10:33:22,624 -> Reading datasets,5.02,0
2017-12-10 10:33:22,634 -> Points partitions: 2
2017-12-10 10:33:22,642 -> Centers partitions: 2
2017-12-10 10:33:30,368 -> 01.Indexing points,7.69,78857,10.0,21
2017-12-10 10:33:34,737 -> 02.Indexing centers,4.37,89928,10.0,21
2017-12-10 10:33:34,744 -> 1024
2017-12-10 10:33:34,751 -> 1024
2017-12-10 10:33:44,016 -> 03.Joining datasets,9.26,89927,10.0,21
Done!!! Sun Dec 10 10:33:44 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:33:46,818 -> Starting session,1.58,0
2017-12-10 10:33:46,819 -> Setting variables,0.00,0
2017-12-10 10:33:51,602 -> Reading datasets,4.78,0
2017-12-10 10:33:51,612 -> Points partitions: 2
2017-12-10 10:33:51,620 -> Centers partitions: 2
2017-12-10 10:33:59,139 -> 01.Indexing points,7.48,78857,10.0,21
2017-12-10 10:34:02,789 -> 02.Indexing centers,3.65,89928,10.0,21
2017-12-10 10:34:02,797 -> 1024
2017-12-10 10:34:02,802 -> 1024
2017-12-10 10:34:12,302 -> 03.Joining datasets,9.50,89927,10.0,21
Done!!! Sun Dec 10 10:34:12 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:34:15,224 -> Starting session,1.67,0
2017-12-10 10:34:15,225 -> Setting variables,0.00,0
2017-12-10 10:34:21,077 -> Reading datasets,5.85,0
2017-12-10 10:34:21,088 -> Points partitions: 2
2017-12-10 10:34:21,095 -> Centers partitions: 2
2017-12-10 10:34:28,375 -> 01.Indexing points,7.24,78857,10.0,21
2017-12-10 10:34:32,166 -> 02.Indexing centers,3.79,89928,10.0,21
2017-12-10 10:34:32,175 -> 1024
2017-12-10 10:34:32,181 -> 1024
2017-12-10 10:34:42,035 -> 03.Joining datasets,9.85,89927,10.0,21
Done!!! Sun Dec 10 10:34:42 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:34:44,748 -> Starting session,1.62,0
2017-12-10 10:34:44,749 -> Setting variables,0.00,0
2017-12-10 10:34:49,420 -> Reading datasets,4.67,0
2017-12-10 10:34:49,432 -> Points partitions: 2
2017-12-10 10:34:49,442 -> Centers partitions: 2
2017-12-10 10:34:57,315 -> 01.Indexing points,7.83,78857,10.0,21
2017-12-10 10:35:01,483 -> 02.Indexing centers,4.17,89928,10.0,21
2017-12-10 10:35:01,494 -> 1024
2017-12-10 10:35:01,500 -> 1024
2017-12-10 10:35:11,055 -> 03.Joining datasets,9.55,89927,10.0,21
Done!!! Sun Dec 10 10:35:11 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:35:13,859 -> Starting session,1.57,0
2017-12-10 10:35:13,860 -> Setting variables,0.00,0
2017-12-10 10:35:18,551 -> Reading datasets,4.69,0
2017-12-10 10:35:18,571 -> Points partitions: 2
2017-12-10 10:35:18,584 -> Centers partitions: 2
2017-12-10 10:35:26,336 -> 01.Indexing points,7.59,78857,10.0,21
2017-12-10 10:35:30,572 -> 02.Indexing centers,4.23,89928,10.0,21
2017-12-10 10:35:30,584 -> 1024
2017-12-10 10:35:30,592 -> 1024
2017-12-10 10:35:40,856 -> 03.Joining datasets,9.82,89927,10.0,21
Done!!! Sun Dec 10 10:35:41 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:35:49,270 -> Starting session,1.76,0
2017-12-10 10:35:49,271 -> Setting variables,0.00,0
2017-12-10 10:35:55,052 -> Reading datasets,5.78,0
2017-12-10 10:35:55,066 -> Points partitions: 2
2017-12-10 10:35:55,076 -> Centers partitions: 2
2017-12-10 10:36:04,485 -> 01.Indexing points,9.37,78857,10.0,28
2017-12-10 10:36:08,411 -> 02.Indexing centers,3.92,89928,10.0,28
2017-12-10 10:36:08,418 -> 1024
2017-12-10 10:36:08,425 -> 1024
2017-12-10 10:36:17,398 -> 03.Joining datasets,8.97,89927,10.0,28
Done!!! Sun Dec 10 10:36:17 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:36:20,373 -> Starting session,1.77,0
2017-12-10 10:36:20,374 -> Setting variables,0.00,0
2017-12-10 10:36:25,274 -> Reading datasets,4.90,0
2017-12-10 10:36:25,287 -> Points partitions: 2
2017-12-10 10:36:25,298 -> Centers partitions: 2
2017-12-10 10:36:35,252 -> 01.Indexing points,9.91,78857,10.0,28
2017-12-10 10:36:39,793 -> 02.Indexing centers,4.54,89928,10.0,28
2017-12-10 10:36:39,802 -> 1024
2017-12-10 10:36:39,810 -> 1024
2017-12-10 10:36:50,166 -> 03.Joining datasets,10.36,89927,10.0,28
Done!!! Sun Dec 10 10:36:50 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:36:52,963 -> Starting session,1.72,0
2017-12-10 10:36:52,963 -> Setting variables,0.00,0
2017-12-10 10:36:59,947 -> Reading datasets,6.98,0
2017-12-10 10:36:59,961 -> Points partitions: 2
2017-12-10 10:36:59,972 -> Centers partitions: 2
2017-12-10 10:37:08,386 -> 01.Indexing points,8.38,78857,10.0,28
2017-12-10 10:37:11,862 -> 02.Indexing centers,3.47,89928,10.0,28
2017-12-10 10:37:11,869 -> 1024
2017-12-10 10:37:11,875 -> 1024
2017-12-10 10:37:21,021 -> 03.Joining datasets,9.15,89927,10.0,28
Done!!! Sun Dec 10 10:37:21 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:37:23,718 -> Starting session,1.60,0
2017-12-10 10:37:23,718 -> Setting variables,0.00,0
2017-12-10 10:37:31,213 -> Reading datasets,7.49,0
2017-12-10 10:37:31,225 -> Points partitions: 2
2017-12-10 10:37:31,235 -> Centers partitions: 2
2017-12-10 10:37:38,971 -> 01.Indexing points,7.70,78857,10.0,28
2017-12-10 10:37:42,898 -> 02.Indexing centers,3.93,89928,10.0,28
2017-12-10 10:37:42,909 -> 1024
2017-12-10 10:37:42,917 -> 1024
2017-12-10 10:37:51,725 -> 03.Joining datasets,8.81,89927,10.0,28
Done!!! Sun Dec 10 10:37:52 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:37:54,597 -> Starting session,1.63,0
2017-12-10 10:37:54,598 -> Setting variables,0.00,0
2017-12-10 10:38:00,628 -> Reading datasets,6.03,0
2017-12-10 10:38:00,641 -> Points partitions: 2
2017-12-10 10:38:00,652 -> Centers partitions: 2
2017-12-10 10:38:10,333 -> 01.Indexing points,9.64,78857,10.0,28
2017-12-10 10:38:14,763 -> 02.Indexing centers,4.43,89928,10.0,28
2017-12-10 10:38:14,773 -> 1024
2017-12-10 10:38:14,781 -> 1024
2017-12-10 10:38:23,571 -> 03.Joining datasets,8.79,89927,10.0,28
Done!!! Sun Dec 10 10:38:24 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:38:31,906 -> Starting session,1.51,0
2017-12-10 10:38:31,907 -> Setting variables,0.00,0
2017-12-10 10:38:36,195 -> Reading datasets,4.29,0
2017-12-10 10:38:36,210 -> Points partitions: 2
2017-12-10 10:38:36,221 -> Centers partitions: 2
2017-12-10 10:38:43,402 -> 01.Indexing points,7.14,59143,10.0,7
2017-12-10 10:38:47,833 -> 02.Indexing centers,4.43,67446,10.0,7
2017-12-10 10:38:47,840 -> 1024
2017-12-10 10:38:47,845 -> 1024
2017-12-10 10:39:01,837 -> 03.Joining datasets,13.99,67446,10.0,7
Done!!! Sun Dec 10 10:39:02 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 10:39:04,403 -> Starting session,1.45,0
2017-12-10 10:39:04,404 -> Setting variables,0.00,0
2017-12-10 10:39:08,778 -> Reading datasets,4.37,0
2017-12-10 10:39:08,789 -> Points partitions: 2
2017-12-10 10:39:08,800 -> Centers partitions: 2
2017-12-10 10:39:15,880 -> 01.Indexing points,7.04,59143,10.0,7
2017-12-10 10:39:20,268 -> 02.Indexing centers,4.39,67446,10.0,7
2017-12-10 10:39:20,275 -> 1024
2017-12-10 10:39:20,281 -> 1024
2017-12-10 10:39:33,861 -> 03.Joining datasets,13.58,67446,10.0,7
Done!!! Sun Dec 10 10:39:34 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 10:39:36,518 -> Starting session,1.55,0
2017-12-10 10:39:36,518 -> Setting variables,0.00,0
2017-12-10 10:39:40,610 -> Reading datasets,4.09,0
2017-12-10 10:39:40,621 -> Points partitions: 2
2017-12-10 10:39:40,628 -> Centers partitions: 2
2017-12-10 10:39:48,160 -> 01.Indexing points,7.49,59143,10.0,7
2017-12-10 10:39:52,684 -> 02.Indexing centers,4.52,67446,10.0,7
2017-12-10 10:39:52,692 -> 1024
2017-12-10 10:39:52,698 -> 1024
2017-12-10 10:40:07,028 -> 03.Joining datasets,14.33,67446,10.0,7
Done!!! Sun Dec 10 10:40:07 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 10:40:09,790 -> Starting session,1.62,0
2017-12-10 10:40:09,790 -> Setting variables,0.00,0
2017-12-10 10:40:14,023 -> Reading datasets,4.23,0
2017-12-10 10:40:14,037 -> Points partitions: 2
2017-12-10 10:40:14,048 -> Centers partitions: 2
2017-12-10 10:40:21,293 -> 01.Indexing points,7.21,59143,10.0,7
2017-12-10 10:40:25,583 -> 02.Indexing centers,4.29,67446,10.0,7
2017-12-10 10:40:25,590 -> 1024
2017-12-10 10:40:25,596 -> 1024
2017-12-10 10:40:39,580 -> 03.Joining datasets,13.98,67446,10.0,7
Done!!! Sun Dec 10 10:40:39 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 10:40:42,154 -> Starting session,1.43,0
2017-12-10 10:40:42,155 -> Setting variables,0.00,0
2017-12-10 10:40:46,664 -> Reading datasets,4.51,0
2017-12-10 10:40:46,674 -> Points partitions: 2
2017-12-10 10:40:46,681 -> Centers partitions: 2
2017-12-10 10:40:54,305 -> 01.Indexing points,7.59,59143,10.0,7
2017-12-10 10:40:58,651 -> 02.Indexing centers,4.34,67446,10.0,7
2017-12-10 10:40:58,661 -> 1024
2017-12-10 10:40:58,667 -> 1024
2017-12-10 10:41:12,987 -> 03.Joining datasets,14.32,67446,10.0,7
Done!!! Sun Dec 10 10:41:13 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 10:41:21,256 -> Starting session,1.69,0
2017-12-10 10:41:21,258 -> Setting variables,0.00,0
2017-12-10 10:41:26,278 -> Reading datasets,5.02,0
2017-12-10 10:41:26,288 -> Points partitions: 2
2017-12-10 10:41:26,295 -> Centers partitions: 2
2017-12-10 10:41:33,955 -> 01.Indexing points,7.62,59143,10.0,14
2017-12-10 10:41:38,584 -> 02.Indexing centers,4.63,67446,10.0,14
2017-12-10 10:41:38,596 -> 1024
2017-12-10 10:41:38,602 -> 1024
2017-12-10 10:41:50,975 -> 03.Joining datasets,12.37,67446,10.0,14
Done!!! Sun Dec 10 10:41:51 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 10:41:53,903 -> Starting session,1.72,0
2017-12-10 10:41:53,903 -> Setting variables,0.00,0
2017-12-10 10:41:58,823 -> Reading datasets,4.92,0
2017-12-10 10:41:58,833 -> Points partitions: 2
2017-12-10 10:41:58,840 -> Centers partitions: 2
2017-12-10 10:42:06,937 -> 01.Indexing points,8.06,59143,10.0,14
2017-12-10 10:42:11,315 -> 02.Indexing centers,4.38,67446,10.0,14
2017-12-10 10:42:11,325 -> 1024
2017-12-10 10:42:11,334 -> 1024
2017-12-10 10:42:23,260 -> 03.Joining datasets,11.93,67446,10.0,14
Done!!! Sun Dec 10 10:42:24 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 10:42:27,022 -> Starting session,1.77,0
2017-12-10 10:42:27,023 -> Setting variables,0.00,0
2017-12-10 10:42:31,788 -> Reading datasets,4.76,0
2017-12-10 10:42:31,800 -> Points partitions: 2
2017-12-10 10:42:31,810 -> Centers partitions: 2
2017-12-10 10:42:39,902 -> 01.Indexing points,8.05,59143,10.0,14
2017-12-10 10:42:44,431 -> 02.Indexing centers,4.53,67446,10.0,14
2017-12-10 10:42:44,439 -> 1024
2017-12-10 10:42:44,445 -> 1024
2017-12-10 10:42:57,082 -> 03.Joining datasets,12.64,67446,10.0,14
Done!!! Sun Dec 10 10:42:57 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 10:42:59,887 -> Starting session,1.70,0
2017-12-10 10:42:59,888 -> Setting variables,0.00,0
2017-12-10 10:43:04,622 -> Reading datasets,4.73,0
2017-12-10 10:43:04,635 -> Points partitions: 2
2017-12-10 10:43:04,644 -> Centers partitions: 2
2017-12-10 10:43:12,978 -> 01.Indexing points,8.29,59143,10.0,14
2017-12-10 10:43:17,271 -> 02.Indexing centers,4.29,67446,10.0,14
2017-12-10 10:43:17,279 -> 1024
2017-12-10 10:43:17,285 -> 1024
2017-12-10 10:43:30,503 -> 03.Joining datasets,13.29,67446,10.0,14
Done!!! Sun Dec 10 10:43:30 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:43:33,183 -> Starting session,1.62,0
2017-12-10 10:43:33,183 -> Setting variables,0.00,0
2017-12-10 10:43:38,213 -> Reading datasets,5.03,0
2017-12-10 10:43:38,223 -> Points partitions: 2
2017-12-10 10:43:38,231 -> Centers partitions: 2
2017-12-10 10:43:46,215 -> 01.Indexing points,7.94,59143,10.0,14
2017-12-10 10:43:50,593 -> 02.Indexing centers,4.38,67446,10.0,14
2017-12-10 10:43:50,601 -> 1024
2017-12-10 10:43:50,606 -> 1024
2017-12-10 10:44:02,539 -> 03.Joining datasets,11.93,67446,10.0,14
Done!!! Sun Dec 10 10:44:02 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:44:10,756 -> Starting session,1.74,0
2017-12-10 10:44:10,756 -> Setting variables,0.00,0
2017-12-10 10:44:15,793 -> Reading datasets,5.04,0
2017-12-10 10:44:15,804 -> Points partitions: 2
2017-12-10 10:44:15,811 -> Centers partitions: 2
2017-12-10 10:44:23,501 -> 01.Indexing points,7.61,59143,10.0,21
2017-12-10 10:44:27,253 -> 02.Indexing centers,3.75,67446,10.0,21
2017-12-10 10:44:27,261 -> 1024
2017-12-10 10:44:27,267 -> 1024
2017-12-10 10:44:36,580 -> 03.Joining datasets,9.31,67446,10.0,21
Done!!! Sun Dec 10 10:44:37 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:44:40,301 -> Starting session,1.69,0
2017-12-10 10:44:40,301 -> Setting variables,0.00,0
2017-12-10 10:44:45,257 -> Reading datasets,4.96,0
2017-12-10 10:44:45,267 -> Points partitions: 2
2017-12-10 10:44:45,274 -> Centers partitions: 2
2017-12-10 10:44:53,144 -> 01.Indexing points,7.83,59143,10.0,21
2017-12-10 10:44:57,111 -> 02.Indexing centers,3.97,67446,10.0,21
2017-12-10 10:44:57,121 -> 1024
2017-12-10 10:44:57,129 -> 1024
2017-12-10 10:45:06,361 -> 03.Joining datasets,9.23,67446,10.0,21
Done!!! Sun Dec 10 10:45:06 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:45:09,218 -> Starting session,1.63,0
2017-12-10 10:45:09,219 -> Setting variables,0.00,0
2017-12-10 10:45:15,191 -> Reading datasets,5.97,0
2017-12-10 10:45:15,202 -> Points partitions: 2
2017-12-10 10:45:15,209 -> Centers partitions: 2
2017-12-10 10:45:22,123 -> 01.Indexing points,6.87,59143,10.0,21
2017-12-10 10:45:25,770 -> 02.Indexing centers,3.65,67446,10.0,21
2017-12-10 10:45:25,778 -> 1024
2017-12-10 10:45:25,784 -> 1024
2017-12-10 10:45:35,097 -> 03.Joining datasets,9.31,67446,10.0,21
Done!!! Sun Dec 10 10:45:35 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:45:38,064 -> Starting session,1.79,0
2017-12-10 10:45:38,064 -> Setting variables,0.00,0
2017-12-10 10:45:43,671 -> Reading datasets,5.61,0
2017-12-10 10:45:43,682 -> Points partitions: 2
2017-12-10 10:45:43,689 -> Centers partitions: 2
2017-12-10 10:45:50,935 -> 01.Indexing points,7.21,59143,10.0,21
2017-12-10 10:45:54,755 -> 02.Indexing centers,3.82,67446,10.0,21
2017-12-10 10:45:54,763 -> 1024
2017-12-10 10:45:54,769 -> 1024
2017-12-10 10:46:04,285 -> 03.Joining datasets,9.52,67446,10.0,21
Done!!! Sun Dec 10 10:46:05 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:46:07,817 -> Starting session,1.51,0
2017-12-10 10:46:07,818 -> Setting variables,0.00,0
2017-12-10 10:46:13,889 -> Reading datasets,6.07,0
2017-12-10 10:46:13,902 -> Points partitions: 2
2017-12-10 10:46:13,913 -> Centers partitions: 2
2017-12-10 10:46:20,911 -> 01.Indexing points,6.96,59143,10.0,21
2017-12-10 10:46:24,880 -> 02.Indexing centers,3.97,67446,10.0,21
2017-12-10 10:46:24,893 -> 1024
2017-12-10 10:46:24,903 -> 1024
2017-12-10 10:46:34,410 -> 03.Joining datasets,9.51,67446,10.0,21
Done!!! Sun Dec 10 10:46:34 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:46:42,835 -> Starting session,1.89,0
2017-12-10 10:46:42,835 -> Setting variables,0.00,0
2017-12-10 10:46:50,130 -> Reading datasets,7.29,0
2017-12-10 10:46:50,140 -> Points partitions: 2
2017-12-10 10:46:50,147 -> Centers partitions: 2
2017-12-10 10:46:58,688 -> 01.Indexing points,8.50,59143,10.0,28
2017-12-10 10:47:02,771 -> 02.Indexing centers,4.08,67446,10.0,28
2017-12-10 10:47:02,779 -> 1024
2017-12-10 10:47:02,785 -> 1024
2017-12-10 10:47:11,544 -> 03.Joining datasets,8.76,67446,10.0,28
Done!!! Sun Dec 10 10:47:12 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:47:14,346 -> Starting session,1.57,0
2017-12-10 10:47:14,346 -> Setting variables,0.00,0
2017-12-10 10:47:21,388 -> Reading datasets,7.04,0
2017-12-10 10:47:21,398 -> Points partitions: 2
2017-12-10 10:47:21,406 -> Centers partitions: 2
2017-12-10 10:47:29,745 -> 01.Indexing points,8.29,59143,10.0,28
2017-12-10 10:47:33,756 -> 02.Indexing centers,4.01,67446,10.0,28
2017-12-10 10:47:33,766 -> 1024
2017-12-10 10:47:33,774 -> 1024
2017-12-10 10:47:41,687 -> 03.Joining datasets,8.57,67446,10.0,28
Done!!! Sun Dec 10 10:47:42 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:47:44,579 -> Starting session,1.64,0
2017-12-10 10:47:44,580 -> Setting variables,0.00,0
2017-12-10 10:47:51,540 -> Reading datasets,6.96,0
2017-12-10 10:47:51,553 -> Points partitions: 2
2017-12-10 10:47:51,563 -> Centers partitions: 2
2017-12-10 10:47:59,654 -> 01.Indexing points,8.05,59143,10.0,28
2017-12-10 10:48:03,538 -> 02.Indexing centers,3.88,67446,10.0,28
2017-12-10 10:48:03,548 -> 1024
2017-12-10 10:48:03,555 -> 1024
2017-12-10 10:48:12,192 -> 03.Joining datasets,8.64,67446,10.0,28
Done!!! Sun Dec 10 10:48:12 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:48:14,975 -> Starting session,1.67,0
2017-12-10 10:48:14,975 -> Setting variables,0.00,0
2017-12-10 10:48:19,732 -> Reading datasets,4.76,0
2017-12-10 10:48:19,743 -> Points partitions: 2
2017-12-10 10:48:19,750 -> Centers partitions: 2
2017-12-10 10:48:29,383 -> 01.Indexing points,9.60,59143,10.0,28
2017-12-10 10:48:33,242 -> 02.Indexing centers,3.86,67446,10.0,28
2017-12-10 10:48:33,250 -> 1024
2017-12-10 10:48:33,256 -> 1024
2017-12-10 10:48:41,627 -> 03.Joining datasets,8.37,67446,10.0,28
Done!!! Sun Dec 10 10:48:42 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:48:44,453 -> Starting session,1.64,0
2017-12-10 10:48:44,454 -> Setting variables,0.00,0
2017-12-10 10:48:51,525 -> Reading datasets,7.07,0
2017-12-10 10:48:51,539 -> Points partitions: 2
2017-12-10 10:48:51,550 -> Centers partitions: 2
2017-12-10 10:48:58,476 -> 01.Indexing points,6.89,59143,10.0,28
2017-12-10 10:49:02,445 -> 02.Indexing centers,3.97,67446,10.0,28
2017-12-10 10:49:02,452 -> 1024
2017-12-10 10:49:02,458 -> 1024
2017-12-10 10:49:11,054 -> 03.Joining datasets,8.59,67446,10.0,28
Done!!! Sun Dec 10 10:49:11 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:49:19,361 -> Starting session,1.58,0
2017-12-10 10:49:19,362 -> Setting variables,0.00,0
2017-12-10 10:49:23,569 -> Reading datasets,4.21,0
2017-12-10 10:49:23,583 -> Points partitions: 2
2017-12-10 10:49:23,594 -> Centers partitions: 2
2017-12-10 10:49:30,490 -> 01.Indexing points,6.85,39429,10.0,7
2017-12-10 10:49:34,455 -> 02.Indexing centers,3.96,44964,10.0,7
2017-12-10 10:49:34,464 -> 1024
2017-12-10 10:49:34,470 -> 1024
2017-12-10 10:49:47,646 -> 03.Joining datasets,13.18,44964,10.0,7
Done!!! Sun Dec 10 10:49:48 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 10:49:50,300 -> Starting session,1.56,0
2017-12-10 10:49:50,301 -> Setting variables,0.00,0
2017-12-10 10:49:54,403 -> Reading datasets,4.10,0
2017-12-10 10:49:54,416 -> Points partitions: 2
2017-12-10 10:49:54,427 -> Centers partitions: 2
2017-12-10 10:50:02,285 -> 01.Indexing points,7.82,39429,10.0,7
2017-12-10 10:50:06,362 -> 02.Indexing centers,4.08,44964,10.0,7
2017-12-10 10:50:06,371 -> 1024
2017-12-10 10:50:06,379 -> 1024
2017-12-10 10:50:19,897 -> 03.Joining datasets,13.52,44964,10.0,7
Done!!! Sun Dec 10 10:50:20 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 10:50:22,553 -> Starting session,1.64,0
2017-12-10 10:50:22,554 -> Setting variables,0.00,0
2017-12-10 10:50:26,656 -> Reading datasets,4.10,0
2017-12-10 10:50:26,669 -> Points partitions: 2
2017-12-10 10:50:26,681 -> Centers partitions: 2
2017-12-10 10:50:33,572 -> 01.Indexing points,6.85,39429,10.0,7
2017-12-10 10:50:37,847 -> 02.Indexing centers,4.27,44964,10.0,7
2017-12-10 10:50:37,854 -> 1024
2017-12-10 10:50:37,860 -> 1024
2017-12-10 10:50:50,626 -> 03.Joining datasets,12.77,44964,10.0,7
Done!!! Sun Dec 10 10:50:51 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 10:50:53,233 -> Starting session,1.47,0
2017-12-10 10:50:53,234 -> Setting variables,0.00,0
2017-12-10 10:50:57,446 -> Reading datasets,4.21,0
2017-12-10 10:50:57,457 -> Points partitions: 2
2017-12-10 10:50:57,464 -> Centers partitions: 2
2017-12-10 10:51:04,561 -> 01.Indexing points,7.06,39429,10.0,7
2017-12-10 10:51:08,762 -> 02.Indexing centers,4.20,44964,10.0,7
2017-12-10 10:51:08,770 -> 1024
2017-12-10 10:51:08,776 -> 1024
2017-12-10 10:51:21,685 -> 03.Joining datasets,12.91,44964,10.0,7
Done!!! Sun Dec 10 10:51:22 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 10:51:24,247 -> Starting session,1.42,0
2017-12-10 10:51:24,247 -> Setting variables,0.00,0
2017-12-10 10:51:28,492 -> Reading datasets,4.24,0
2017-12-10 10:51:28,503 -> Points partitions: 2
2017-12-10 10:51:28,514 -> Centers partitions: 2
2017-12-10 10:51:35,716 -> 01.Indexing points,7.16,39429,10.0,7
2017-12-10 10:51:39,980 -> 02.Indexing centers,4.26,44964,10.0,7
2017-12-10 10:51:39,988 -> 1024
2017-12-10 10:51:39,995 -> 1024
2017-12-10 10:51:53,787 -> 03.Joining datasets,13.79,44964,10.0,7
Done!!! Sun Dec 10 10:51:54 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 10:52:01,850 -> Starting session,1.67,0
2017-12-10 10:52:01,850 -> Setting variables,0.00,0
2017-12-10 10:52:06,738 -> Reading datasets,4.89,0
2017-12-10 10:52:06,749 -> Points partitions: 2
2017-12-10 10:52:06,760 -> Centers partitions: 2
2017-12-10 10:52:14,423 -> 01.Indexing points,7.62,39429,10.0,14
2017-12-10 10:52:18,604 -> 02.Indexing centers,4.18,44964,10.0,14
2017-12-10 10:52:18,612 -> 1024
2017-12-10 10:52:18,619 -> 1024
2017-12-10 10:52:30,279 -> 03.Joining datasets,11.66,44964,10.0,14
Done!!! Sun Dec 10 10:52:31 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 10:52:33,803 -> Starting session,1.62,0
2017-12-10 10:52:33,804 -> Setting variables,0.00,0
2017-12-10 10:52:38,823 -> Reading datasets,5.02,0
2017-12-10 10:52:38,833 -> Points partitions: 2
2017-12-10 10:52:38,840 -> Centers partitions: 2
2017-12-10 10:52:46,485 -> 01.Indexing points,7.61,39429,10.0,14
2017-12-10 10:52:50,368 -> 02.Indexing centers,3.88,44964,10.0,14
2017-12-10 10:52:50,377 -> 1024
2017-12-10 10:52:50,385 -> 1024
2017-12-10 10:53:01,054 -> 03.Joining datasets,10.67,44964,10.0,14
Done!!! Sun Dec 10 10:53:01 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 10:53:03,900 -> Starting session,1.70,0
2017-12-10 10:53:03,901 -> Setting variables,0.00,0
2017-12-10 10:53:08,850 -> Reading datasets,4.95,0
2017-12-10 10:53:08,860 -> Points partitions: 2
2017-12-10 10:53:08,871 -> Centers partitions: 2
2017-12-10 10:53:16,088 -> 01.Indexing points,7.18,39429,10.0,14
2017-12-10 10:53:20,293 -> 02.Indexing centers,4.20,44964,10.0,14
2017-12-10 10:53:20,305 -> 1024
2017-12-10 10:53:20,314 -> 1024
2017-12-10 10:53:31,211 -> 03.Joining datasets,10.90,44964,10.0,14
Done!!! Sun Dec 10 10:53:32 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 10:53:34,866 -> Starting session,1.78,0
2017-12-10 10:53:34,866 -> Setting variables,0.00,0
2017-12-10 10:53:39,874 -> Reading datasets,5.01,0
2017-12-10 10:53:39,884 -> Points partitions: 2
2017-12-10 10:53:39,891 -> Centers partitions: 2
2017-12-10 10:53:47,170 -> 01.Indexing points,7.24,39429,10.0,14
2017-12-10 10:53:51,531 -> 02.Indexing centers,4.36,44964,10.0,14
2017-12-10 10:53:51,542 -> 1024
2017-12-10 10:53:51,550 -> 1024
2017-12-10 10:54:02,683 -> 03.Joining datasets,11.20,44964,10.0,14
Done!!! Sun Dec 10 10:54:03 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 10:54:05,704 -> Starting session,1.83,0
2017-12-10 10:54:05,705 -> Setting variables,0.00,0
2017-12-10 10:54:10,627 -> Reading datasets,4.92,0
2017-12-10 10:54:10,642 -> Points partitions: 2
2017-12-10 10:54:10,653 -> Centers partitions: 2
2017-12-10 10:54:18,214 -> 01.Indexing points,7.52,39429,10.0,14
2017-12-10 10:54:22,367 -> 02.Indexing centers,4.15,44964,10.0,14
2017-12-10 10:54:22,378 -> 1024
2017-12-10 10:54:22,386 -> 1024
2017-12-10 10:54:33,445 -> 03.Joining datasets,11.06,44964,10.0,14
Done!!! Sun Dec 10 10:54:34 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 10:54:42,586 -> Starting session,1.71,0
2017-12-10 10:54:42,586 -> Setting variables,0.00,0
2017-12-10 10:54:48,370 -> Reading datasets,5.78,0
2017-12-10 10:54:48,384 -> Points partitions: 2
2017-12-10 10:54:48,395 -> Centers partitions: 2
2017-12-10 10:54:55,007 -> 01.Indexing points,6.57,39429,10.0,21
2017-12-10 10:54:58,539 -> 02.Indexing centers,3.53,44964,10.0,21
2017-12-10 10:54:58,550 -> 1024
2017-12-10 10:54:58,559 -> 1024
2017-12-10 10:55:07,188 -> 03.Joining datasets,8.63,44964,10.0,21
Done!!! Sun Dec 10 10:55:07 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 10:55:09,949 -> Starting session,1.66,0
2017-12-10 10:55:09,950 -> Setting variables,0.00,0
2017-12-10 10:55:14,972 -> Reading datasets,5.02,0
2017-12-10 10:55:14,986 -> Points partitions: 2
2017-12-10 10:55:14,994 -> Centers partitions: 2
2017-12-10 10:55:22,571 -> 01.Indexing points,7.53,39429,10.0,21
2017-12-10 10:55:26,441 -> 02.Indexing centers,3.87,44964,10.0,21
2017-12-10 10:55:26,449 -> 1024
2017-12-10 10:55:26,455 -> 1024
2017-12-10 10:55:35,868 -> 03.Joining datasets,9.41,44964,10.0,21
Done!!! Sun Dec 10 10:55:36 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 10:55:38,671 -> Starting session,1.70,0
2017-12-10 10:55:38,672 -> Setting variables,0.00,0
2017-12-10 10:55:43,574 -> Reading datasets,4.90,0
2017-12-10 10:55:43,588 -> Points partitions: 2
2017-12-10 10:55:43,598 -> Centers partitions: 2
2017-12-10 10:55:52,083 -> 01.Indexing points,7.56,39429,10.0,21
2017-12-10 10:55:55,748 -> 02.Indexing centers,3.66,44964,10.0,21
2017-12-10 10:55:55,756 -> 1024
2017-12-10 10:55:55,762 -> 1024
2017-12-10 10:56:04,762 -> 03.Joining datasets,9.00,44964,10.0,21
Done!!! Sun Dec 10 10:56:05 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 10:56:07,597 -> Starting session,1.69,0
2017-12-10 10:56:07,598 -> Setting variables,0.00,0
2017-12-10 10:56:12,486 -> Reading datasets,4.89,0
2017-12-10 10:56:12,500 -> Points partitions: 2
2017-12-10 10:56:12,511 -> Centers partitions: 2
2017-12-10 10:56:20,201 -> 01.Indexing points,7.64,39429,10.0,21
2017-12-10 10:56:23,717 -> 02.Indexing centers,3.52,44964,10.0,21
2017-12-10 10:56:23,725 -> 1024
2017-12-10 10:56:23,732 -> 1024
2017-12-10 10:56:32,644 -> 03.Joining datasets,8.91,44964,10.0,21
Done!!! Sun Dec 10 10:56:33 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 10:56:36,604 -> Starting session,1.93,0
2017-12-10 10:56:36,604 -> Setting variables,0.00,0
2017-12-10 10:56:42,415 -> Reading datasets,5.81,0
2017-12-10 10:56:42,428 -> Points partitions: 2
2017-12-10 10:56:42,438 -> Centers partitions: 2
2017-12-10 10:56:49,214 -> 01.Indexing points,6.73,39429,10.0,21
2017-12-10 10:56:53,255 -> 02.Indexing centers,4.04,44964,10.0,21
2017-12-10 10:56:53,264 -> 1024
2017-12-10 10:56:53,269 -> 1024
2017-12-10 10:57:02,114 -> 03.Joining datasets,8.84,44964,10.0,21
Done!!! Sun Dec 10 10:57:02 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 10:57:10,607 -> Starting session,1.75,0
2017-12-10 10:57:10,608 -> Setting variables,0.00,0
2017-12-10 10:57:17,779 -> Reading datasets,7.17,0
2017-12-10 10:57:17,792 -> Points partitions: 2
2017-12-10 10:57:17,801 -> Centers partitions: 2
2017-12-10 10:57:25,302 -> 01.Indexing points,7.46,39429,10.0,28
2017-12-10 10:57:28,928 -> 02.Indexing centers,3.62,44964,10.0,28
2017-12-10 10:57:28,938 -> 1024
2017-12-10 10:57:28,944 -> 1024
2017-12-10 10:57:37,005 -> 03.Joining datasets,8.06,44964,10.0,28
Done!!! Sun Dec 10 10:57:37 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 10:57:39,871 -> Starting session,1.77,0
2017-12-10 10:57:39,872 -> Setting variables,0.00,0
2017-12-10 10:57:47,071 -> Reading datasets,7.20,0
2017-12-10 10:57:47,081 -> Points partitions: 2
2017-12-10 10:57:47,088 -> Centers partitions: 2
2017-12-10 10:57:54,349 -> 01.Indexing points,7.22,39429,10.0,28
2017-12-10 10:57:58,388 -> 02.Indexing centers,4.04,44964,10.0,28
2017-12-10 10:57:58,397 -> 1024
2017-12-10 10:57:58,403 -> 1024
2017-12-10 10:58:06,765 -> 03.Joining datasets,8.36,44964,10.0,28
Done!!! Sun Dec 10 10:58:07 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 10:58:09,782 -> Starting session,1.76,0
2017-12-10 10:58:09,783 -> Setting variables,0.00,0
2017-12-10 10:58:15,836 -> Reading datasets,6.05,0
2017-12-10 10:58:15,846 -> Points partitions: 2
2017-12-10 10:58:15,854 -> Centers partitions: 2
2017-12-10 10:58:24,315 -> 01.Indexing points,8.42,39429,10.0,28
2017-12-10 10:58:27,874 -> 02.Indexing centers,3.56,44964,10.0,28
2017-12-10 10:58:27,884 -> 1024
2017-12-10 10:58:27,892 -> 1024
2017-12-10 10:58:35,698 -> 03.Joining datasets,7.81,44964,10.0,28
Done!!! Sun Dec 10 10:58:36 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 10:58:38,550 -> Starting session,1.66,0
2017-12-10 10:58:38,551 -> Setting variables,0.00,0
2017-12-10 10:58:45,640 -> Reading datasets,7.09,0
2017-12-10 10:58:45,650 -> Points partitions: 2
2017-12-10 10:58:45,660 -> Centers partitions: 2
2017-12-10 10:58:53,150 -> 01.Indexing points,7.45,39429,10.0,28
2017-12-10 10:58:56,672 -> 02.Indexing centers,3.52,44964,10.0,28
2017-12-10 10:58:56,679 -> 1024
2017-12-10 10:58:56,685 -> 1024
2017-12-10 10:59:04,785 -> 03.Joining datasets,8.10,44964,10.0,28
Done!!! Sun Dec 10 10:59:05 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 10:59:07,695 -> Starting session,1.69,0
2017-12-10 10:59:07,695 -> Setting variables,0.00,0
2017-12-10 10:59:13,698 -> Reading datasets,6.00,0
2017-12-10 10:59:13,711 -> Points partitions: 2
2017-12-10 10:59:13,720 -> Centers partitions: 2
2017-12-10 10:59:20,244 -> 01.Indexing points,6.48,39429,10.0,28
2017-12-10 10:59:24,801 -> 02.Indexing centers,4.56,44964,10.0,28
2017-12-10 10:59:24,809 -> 1024
2017-12-10 10:59:24,815 -> 1024
2017-12-10 10:59:34,061 -> 03.Joining datasets,9.25,44964,10.0,28
Done!!! Sun Dec 10 10:59:34 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-10 10:59:42,337 -> Starting session,1.61,0
2017-12-10 10:59:42,338 -> Setting variables,0.00,0
2017-12-10 10:59:46,318 -> Reading datasets,3.98,0
2017-12-10 10:59:46,332 -> Points partitions: 2
2017-12-10 10:59:46,343 -> Centers partitions: 2
2017-12-10 10:59:53,297 -> 01.Indexing points,6.88,19715,10.0,7
2017-12-10 10:59:57,249 -> 02.Indexing centers,3.95,22482,10.0,7
2017-12-10 10:59:57,259 -> 992
2017-12-10 10:59:57,264 -> 1024
2017-12-10 11:00:09,664 -> 03.Joining datasets,12.40,22482,10.0,7
Done!!! Sun Dec 10 11:00:10 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-10 11:00:12,290 -> Starting session,1.50,0
2017-12-10 11:00:12,291 -> Setting variables,0.00,0
2017-12-10 11:00:16,381 -> Reading datasets,4.09,0
2017-12-10 11:00:16,391 -> Points partitions: 2
2017-12-10 11:00:16,398 -> Centers partitions: 2
2017-12-10 11:00:23,104 -> 01.Indexing points,6.67,19715,10.0,7
2017-12-10 11:00:26,888 -> 02.Indexing centers,3.78,22482,10.0,7
2017-12-10 11:00:26,897 -> 992
2017-12-10 11:00:26,903 -> 1024
2017-12-10 11:00:38,635 -> 03.Joining datasets,11.73,22482,10.0,7
Done!!! Sun Dec 10 11:00:39 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-10 11:00:41,520 -> Starting session,1.86,0
2017-12-10 11:00:41,520 -> Setting variables,0.00,0
2017-12-10 11:00:45,367 -> Reading datasets,3.85,0
2017-12-10 11:00:45,380 -> Points partitions: 2
2017-12-10 11:00:45,390 -> Centers partitions: 2
2017-12-10 11:00:51,818 -> 01.Indexing points,6.38,19715,10.0,7
2017-12-10 11:00:55,870 -> 02.Indexing centers,4.05,22482,10.0,7
2017-12-10 11:00:55,878 -> 992
2017-12-10 11:00:55,883 -> 1024
2017-12-10 11:01:07,928 -> 03.Joining datasets,12.04,22482,10.0,7
Done!!! Sun Dec 10 11:01:08 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-10 11:01:10,457 -> Starting session,1.52,0
2017-12-10 11:01:10,457 -> Setting variables,0.00,0
2017-12-10 11:01:14,700 -> Reading datasets,4.24,0
2017-12-10 11:01:14,713 -> Points partitions: 2
2017-12-10 11:01:14,740 -> Centers partitions: 2
2017-12-10 11:01:21,480 -> 01.Indexing points,6.69,19715,10.0,7
2017-12-10 11:01:25,717 -> 02.Indexing centers,4.24,22482,10.0,7
2017-12-10 11:01:25,724 -> 992
2017-12-10 11:01:25,730 -> 1024
2017-12-10 11:01:37,876 -> 03.Joining datasets,12.15,22482,10.0,7
Done!!! Sun Dec 10 11:01:38 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-10 11:01:40,382 -> Starting session,1.51,0
2017-12-10 11:01:40,383 -> Setting variables,0.00,0
2017-12-10 11:01:44,609 -> Reading datasets,4.23,0
2017-12-10 11:01:44,623 -> Points partitions: 2
2017-12-10 11:01:44,635 -> Centers partitions: 2
2017-12-10 11:01:51,453 -> 01.Indexing points,6.77,19715,10.0,7
2017-12-10 11:01:55,410 -> 02.Indexing centers,3.96,22482,10.0,7
2017-12-10 11:01:55,417 -> 992
2017-12-10 11:01:55,423 -> 1024
2017-12-10 11:02:07,735 -> 03.Joining datasets,12.31,22482,10.0,7
Done!!! Sun Dec 10 11:02:08 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-10 11:02:15,860 -> Starting session,1.73,0
2017-12-10 11:02:15,860 -> Setting variables,0.00,0
2017-12-10 11:02:20,995 -> Reading datasets,5.13,0
2017-12-10 11:02:21,006 -> Points partitions: 2
2017-12-10 11:02:21,013 -> Centers partitions: 2
2017-12-10 11:02:28,957 -> 01.Indexing points,7.91,19715,10.0,14
2017-12-10 11:02:32,962 -> 02.Indexing centers,4.00,22482,10.0,14
2017-12-10 11:02:32,970 -> 992
2017-12-10 11:02:32,976 -> 1024
2017-12-10 11:02:43,582 -> 03.Joining datasets,10.61,22482,10.0,14
Done!!! Sun Dec 10 11:02:44 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-10 11:02:46,461 -> Starting session,1.64,0
2017-12-10 11:02:46,461 -> Setting variables,0.00,0
2017-12-10 11:02:51,354 -> Reading datasets,4.89,0
2017-12-10 11:02:51,365 -> Points partitions: 2
2017-12-10 11:02:51,373 -> Centers partitions: 2
2017-12-10 11:02:58,569 -> 01.Indexing points,7.15,19715,10.0,14
2017-12-10 11:03:02,800 -> 02.Indexing centers,4.23,22482,10.0,14
2017-12-10 11:03:02,812 -> 992
2017-12-10 11:03:02,821 -> 1024
2017-12-10 11:03:12,691 -> 03.Joining datasets,9.87,22482,10.0,14
Done!!! Sun Dec 10 11:03:13 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-10 11:03:15,619 -> Starting session,1.71,0
2017-12-10 11:03:15,620 -> Setting variables,0.00,0
2017-12-10 11:03:20,500 -> Reading datasets,4.88,0
2017-12-10 11:03:20,515 -> Points partitions: 2
2017-12-10 11:03:20,526 -> Centers partitions: 2
2017-12-10 11:03:27,837 -> 01.Indexing points,7.27,19715,10.0,14
2017-12-10 11:03:32,089 -> 02.Indexing centers,4.25,22482,10.0,14
2017-12-10 11:03:32,099 -> 992
2017-12-10 11:03:32,107 -> 1024
2017-12-10 11:03:42,433 -> 03.Joining datasets,10.40,22482,10.0,14
Done!!! Sun Dec 10 11:03:42 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-10 11:03:45,264 -> Starting session,1.60,0
2017-12-10 11:03:45,265 -> Setting variables,0.00,0
2017-12-10 11:03:50,319 -> Reading datasets,5.05,0
2017-12-10 11:03:50,330 -> Points partitions: 2
2017-12-10 11:03:50,338 -> Centers partitions: 2
2017-12-10 11:03:58,087 -> 01.Indexing points,7.71,19715,10.0,14
2017-12-10 11:04:02,184 -> 02.Indexing centers,4.10,22482,10.0,14
2017-12-10 11:04:02,191 -> 992
2017-12-10 11:04:02,198 -> 1024
2017-12-10 11:04:12,463 -> 03.Joining datasets,10.26,22482,10.0,14
Done!!! Sun Dec 10 11:04:12 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-10 11:04:15,330 -> Starting session,1.62,0
2017-12-10 11:04:15,330 -> Setting variables,0.00,0
2017-12-10 11:04:20,005 -> Reading datasets,4.67,0
2017-12-10 11:04:20,015 -> Points partitions: 2
2017-12-10 11:04:20,023 -> Centers partitions: 2
2017-12-10 11:04:27,282 -> 01.Indexing points,7.22,19715,10.0,14
2017-12-10 11:04:31,615 -> 02.Indexing centers,4.33,22482,10.0,14
2017-12-10 11:04:31,623 -> 992
2017-12-10 11:04:31,629 -> 1024
2017-12-10 11:04:41,923 -> 03.Joining datasets,10.29,22482,10.0,14
Done!!! Sun Dec 10 11:04:42 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-10 11:04:50,250 -> Starting session,1.69,0
2017-12-10 11:04:50,251 -> Setting variables,0.00,0
2017-12-10 11:04:56,119 -> Reading datasets,5.87,0
2017-12-10 11:04:56,133 -> Points partitions: 2
2017-12-10 11:04:56,144 -> Centers partitions: 2
2017-12-10 11:05:03,297 -> 01.Indexing points,7.11,19715,10.0,21
2017-12-10 11:05:07,102 -> 02.Indexing centers,3.80,22482,10.0,21
2017-12-10 11:05:07,114 -> 992
2017-12-10 11:05:07,122 -> 1024
2017-12-10 11:05:17,092 -> 03.Joining datasets,9.97,22482,10.0,21
Done!!! Sun Dec 10 11:05:17 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-10 11:05:19,843 -> Starting session,1.65,0
2017-12-10 11:05:19,844 -> Setting variables,0.00,0
2017-12-10 11:05:25,687 -> Reading datasets,5.84,0
2017-12-10 11:05:25,698 -> Points partitions: 2
2017-12-10 11:05:25,706 -> Centers partitions: 2
2017-12-10 11:05:33,432 -> 01.Indexing points,7.69,19715,10.0,21
2017-12-10 11:05:37,077 -> 02.Indexing centers,3.64,22482,10.0,21
2017-12-10 11:05:37,089 -> 992
2017-12-10 11:05:37,099 -> 1024
2017-12-10 11:05:46,966 -> 03.Joining datasets,9.87,22482,10.0,21
Done!!! Sun Dec 10 11:05:47 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-10 11:05:49,934 -> Starting session,1.70,0
2017-12-10 11:05:49,934 -> Setting variables,0.00,0
2017-12-10 11:05:54,789 -> Reading datasets,4.85,0
2017-12-10 11:05:54,802 -> Points partitions: 2
2017-12-10 11:05:54,813 -> Centers partitions: 2
2017-12-10 11:06:03,553 -> 01.Indexing points,8.70,19715,10.0,21
2017-12-10 11:06:07,486 -> 02.Indexing centers,3.93,22482,10.0,21
2017-12-10 11:06:07,496 -> 992
2017-12-10 11:06:07,504 -> 1024
2017-12-10 11:06:17,228 -> 03.Joining datasets,9.72,22482,10.0,21
Done!!! Sun Dec 10 11:06:17 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-10 11:06:20,131 -> Starting session,1.70,0
2017-12-10 11:06:20,131 -> Setting variables,0.00,0
2017-12-10 11:06:25,830 -> Reading datasets,5.70,0
2017-12-10 11:06:25,841 -> Points partitions: 2
2017-12-10 11:06:25,848 -> Centers partitions: 2
2017-12-10 11:06:33,081 -> 01.Indexing points,7.19,19715,10.0,21
2017-12-10 11:06:36,867 -> 02.Indexing centers,3.78,22482,10.0,21
2017-12-10 11:06:36,875 -> 992
2017-12-10 11:06:36,881 -> 1024
2017-12-10 11:06:46,559 -> 03.Joining datasets,9.68,22482,10.0,21
Done!!! Sun Dec 10 11:06:47 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-10 11:06:49,313 -> Starting session,1.55,0
2017-12-10 11:06:49,314 -> Setting variables,0.00,0
2017-12-10 11:06:55,000 -> Reading datasets,5.69,0
2017-12-10 11:06:55,013 -> Points partitions: 2
2017-12-10 11:06:55,024 -> Centers partitions: 2
2017-12-10 11:07:02,530 -> 01.Indexing points,7.46,19715,10.0,21
2017-12-10 11:07:06,226 -> 02.Indexing centers,3.70,22482,10.0,21
2017-12-10 11:07:06,236 -> 992
2017-12-10 11:07:06,243 -> 1024
2017-12-10 11:07:16,052 -> 03.Joining datasets,9.81,22482,10.0,21
Done!!! Sun Dec 10 11:07:16 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-10 11:07:24,455 -> Starting session,1.86,0
2017-12-10 11:07:24,456 -> Setting variables,0.00,0
2017-12-10 11:07:31,432 -> Reading datasets,6.98,0
2017-12-10 11:07:31,445 -> Points partitions: 2
2017-12-10 11:07:31,455 -> Centers partitions: 2
2017-12-10 11:07:38,149 -> 01.Indexing points,6.65,19715,10.0,28
2017-12-10 11:07:42,201 -> 02.Indexing centers,4.05,22482,10.0,28
2017-12-10 11:07:42,213 -> 992
2017-12-10 11:07:42,222 -> 1024
2017-12-10 11:07:50,891 -> 03.Joining datasets,8.67,22482,10.0,28
Done!!! Sun Dec 10 11:07:51 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-10 11:07:53,729 -> Starting session,1.62,0
2017-12-10 11:07:53,729 -> Setting variables,0.00,0
2017-12-10 11:08:00,765 -> Reading datasets,7.04,0
2017-12-10 11:08:00,778 -> Points partitions: 2
2017-12-10 11:08:00,788 -> Centers partitions: 2
2017-12-10 11:08:07,058 -> 01.Indexing points,6.23,19715,10.0,28
2017-12-10 11:08:10,978 -> 02.Indexing centers,3.92,22482,10.0,28
2017-12-10 11:08:10,989 -> 992
2017-12-10 11:08:10,997 -> 1024
2017-12-10 11:08:19,465 -> 03.Joining datasets,8.47,22482,10.0,28
Done!!! Sun Dec 10 11:08:20 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-10 11:08:23,024 -> Starting session,1.64,0
2017-12-10 11:08:23,027 -> Setting variables,0.00,0
2017-12-10 11:08:29,916 -> Reading datasets,6.89,0
2017-12-10 11:08:29,926 -> Points partitions: 2
2017-12-10 11:08:29,934 -> Centers partitions: 2
2017-12-10 11:08:38,656 -> 01.Indexing points,8.68,19715,10.0,28
2017-12-10 11:08:42,645 -> 02.Indexing centers,3.99,22482,10.0,28
2017-12-10 11:08:42,652 -> 992
2017-12-10 11:08:42,658 -> 1024
2017-12-10 11:08:52,841 -> 03.Joining datasets,10.18,22482,10.0,28
Done!!! Sun Dec 10 11:08:53 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-10 11:08:55,721 -> Starting session,1.62,0
2017-12-10 11:08:55,722 -> Setting variables,0.00,0
2017-12-10 11:09:02,754 -> Reading datasets,7.03,0
2017-12-10 11:09:02,767 -> Points partitions: 2
2017-12-10 11:09:02,774 -> Centers partitions: 2
2017-12-10 11:09:09,823 -> 01.Indexing points,7.01,19715,10.0,28
2017-12-10 11:09:13,643 -> 02.Indexing centers,3.82,22482,10.0,28
2017-12-10 11:09:13,653 -> 992
2017-12-10 11:09:13,660 -> 1024
2017-12-10 11:09:21,971 -> 03.Joining datasets,8.37,22482,10.0,28
Done!!! Sun Dec 10 11:09:22 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-10 11:09:24,737 -> Starting session,1.59,0
2017-12-10 11:09:24,737 -> Setting variables,0.00,0
2017-12-10 11:09:31,837 -> Reading datasets,7.10,0
2017-12-10 11:09:31,850 -> Points partitions: 2
2017-12-10 11:09:31,860 -> Centers partitions: 2
2017-12-10 11:09:39,297 -> 01.Indexing points,7.40,19715,10.0,28
2017-12-10 11:09:43,075 -> 02.Indexing centers,3.78,22482,10.0,28
2017-12-10 11:09:43,085 -> 992
2017-12-10 11:09:43,093 -> 1024
2017-12-10 11:09:51,297 -> 03.Joining datasets,8.20,22482,10.0,28
Done!!! Sun Dec 10 11:09:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/1 for 7 cores...
2017-12-14 11:37:17,384 -> Starting session...
2017-12-14 11:37:26,668 -> 01.Indexing Points,4.25,19714,20.0,7
2017-12-14 11:37:46,147 -> 01.Indexing Points,15.01,39428,20.0,7
2017-12-14 11:38:06,440 -> 01.Indexing Points,17.61,59142,20.0,7
2017-12-14 11:38:30,527 -> 01.Indexing Points,20.74,78856,20.0,7
2017-12-14 11:38:34,729 -> 02.Indexing Centers,2.67,47664,20.0,7
2017-12-14 11:39:08,720 -> 02.Indexing Centers,27.58,95328,20.0,7
2017-12-14 11:39:51,327 -> 02.Indexing Centers,36.10,142992,20.0,7
2017-12-14 11:40:42,767 -> 02.Indexing Centers,44.76,190656,20.0,7
2017-12-14 11:41:12,639 -> 03.Joining datasets,29.87,2552,20.0,7
2017-12-14 11:41:44,492 -> 03.Joining datasets,31.85,5104,20.0,7
2017-12-14 11:42:17,572 -> 03.Joining datasets,33.08,7656,20.0,7
2017-12-14 11:42:51,665 -> 03.Joining datasets,34.09,10208,20.0,7
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/1 for 14 cores...
2017-12-14 11:42:59,911 -> Starting session...
2017-12-14 11:43:10,691 -> 01.Indexing Points,5.15,19714,20.0,14
2017-12-14 11:43:25,398 -> 01.Indexing Points,10.57,39428,20.0,14
2017-12-14 11:43:39,388 -> 01.Indexing Points,11.57,59142,20.0,14
2017-12-14 11:43:54,397 -> 01.Indexing Points,12.81,78856,20.0,14
2017-12-14 11:43:58,223 -> 02.Indexing Centers,2.42,47664,20.0,14
2017-12-14 11:44:18,278 -> 02.Indexing Centers,16.28,95328,20.0,14
2017-12-14 11:44:43,144 -> 02.Indexing Centers,21.00,142992,20.0,14
2017-12-14 11:45:13,249 -> 02.Indexing Centers,26.19,190656,20.0,14
2017-12-14 11:45:31,389 -> 03.Joining datasets,18.14,2489,20.0,14
2017-12-14 11:45:50,259 -> 03.Joining datasets,18.87,4978,20.0,14
2017-12-14 11:46:09,674 -> 03.Joining datasets,19.42,7467,20.0,14
2017-12-14 11:46:30,768 -> 03.Joining datasets,20.27,9956,20.0,14
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/1 for 21 cores...
2017-12-14 11:46:40,223 -> Starting session...
2017-12-14 11:46:50,525 -> 01.Indexing Points,4.34,19714,20.0,21
2017-12-14 11:47:02,150 -> 01.Indexing Points,8.49,39428,20.0,21
2017-12-14 11:47:12,820 -> 01.Indexing Points,8.98,59142,20.0,21
2017-12-14 11:47:23,921 -> 01.Indexing Points,9.31,78856,20.0,21
2017-12-14 11:47:27,706 -> 02.Indexing Centers,2.34,47664,20.0,21
2017-12-14 11:47:42,993 -> 02.Indexing Centers,12.41,95328,20.0,21
2017-12-14 11:48:01,602 -> 02.Indexing Centers,15.72,142992,20.0,21
2017-12-14 11:48:23,994 -> 02.Indexing Centers,19.23,190656,20.0,21
2017-12-14 11:48:36,870 -> 03.Joining datasets,12.87,2590,20.0,21
2017-12-14 11:48:50,192 -> 03.Joining datasets,13.32,5180,20.0,21
2017-12-14 11:49:04,323 -> 03.Joining datasets,14.13,7770,20.0,21
2017-12-14 11:49:19,202 -> 03.Joining datasets,14.88,10360,20.0,21
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/1 for 28 cores...
2017-12-14 11:49:27,675 -> Starting session...
2017-12-14 11:49:39,388 -> 01.Indexing Points,4.95,19714,20.0,28
2017-12-14 11:49:48,946 -> 01.Indexing Points,6.93,39428,20.0,28
2017-12-14 11:49:57,718 -> 01.Indexing Points,7.33,59142,20.0,28
2017-12-14 11:50:07,352 -> 01.Indexing Points,7.93,78856,20.0,28
2017-12-14 11:50:11,148 -> 02.Indexing Centers,2.26,47664,20.0,28
2017-12-14 11:50:23,994 -> 02.Indexing Centers,10.41,95328,20.0,28
2017-12-14 11:50:40,135 -> 02.Indexing Centers,13.62,142992,20.0,28
2017-12-14 11:51:00,481 -> 02.Indexing Centers,17.87,190656,20.0,28
2017-12-14 11:51:11,129 -> 03.Joining datasets,10.65,2555,20.0,28
2017-12-14 11:51:22,421 -> 03.Joining datasets,11.29,5110,20.0,28
2017-12-14 11:51:34,177 -> 03.Joining datasets,11.76,7665,20.0,28
2017-12-14 11:51:46,612 -> 03.Joining datasets,12.43,10220,20.0,28
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
