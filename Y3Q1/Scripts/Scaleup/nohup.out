acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1 ...
Running in 7 cores and 256 partitions.  Setting mu = 10 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/10/13 09:38:29 INFO SparkContext: Running Spark version 2.1.0
17/10/13 09:38:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/10/13 09:38:30 INFO SecurityManager: Changing view acls to: acald013
17/10/13 09:38:30 INFO SecurityManager: Changing modify acls to: acald013
17/10/13 09:38:30 INFO SecurityManager: Changing view acls groups to: 
17/10/13 09:38:30 INFO SecurityManager: Changing modify acls groups to: 
17/10/13 09:38:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/10/13 09:38:30 INFO Utils: Successfully started service 'sparkDriver' on port 39182.
17/10/13 09:38:30 INFO SparkEnv: Registering MapOutputTracker
17/10/13 09:38:30 INFO SparkEnv: Registering BlockManagerMaster
17/10/13 09:38:30 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/10/13 09:38:30 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/10/13 09:38:30 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-437b175b-8aaf-4116-80e7-209863b3b083
17/10/13 09:38:30 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/10/13 09:38:31 INFO SparkEnv: Registering OutputCommitCoordinator
17/10/13 09:38:31 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/10/13 09:38:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/10/13 09:38:31 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:39182/jars/pflock_2.11-1.0.jar with timestamp 1507912711538
17/10/13 09:38:31 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/10/13 09:38:31 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/10/13 09:38:31 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20171013093831-0000
17/10/13 09:38:31 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34702.
17/10/13 09:38:31 INFO NettyBlockTransferService: Server created on 169.235.27.138:34702
17/10/13 09:38:31 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/10/13 09:38:31 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 34702, None)
17/10/13 09:38:31 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:34702 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 34702, None)
17/10/13 09:38:31 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 34702, None)
17/10/13 09:38:31 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 34702, None)
17/10/13 09:38:31 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20171013093831-0000/0 on worker-20171013093823-169.235.27.134-34336 (169.235.27.134:34336) with 7 cores
17/10/13 09:38:31 INFO StandaloneSchedulerBackend: Granted executor ID app-20171013093831-0000/0 on hostPort 169.235.27.134:34336 with 7 cores, 12.0 GB RAM
17/10/13 09:38:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20171013093831-0000/0 is now RUNNING
17/10/13 09:38:32 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20171013093831-0000
17/10/13 09:38:32 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/10/13 09:38:32 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y3Q1/Scripts/Scaleup/spark-warehouse/'.
Running app-20171013093831-0000 on 7 cores and 256 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       10.0        20K     32.545      5.102     37.647      25626         36          7        256    09:39:11.621
    PFlock       20.0        20K     21.397      5.537     26.934      52128        216          7        256    09:39:38.657
    PFlock       30.0        20K     23.325     46.906     70.231      78194        500          7        256    09:40:48.948
    PFlock       40.0        20K     25.834    126.394    152.228     105552        900          7        256    09:43:21.232
    PFlock       50.0        20K     28.277    132.061    160.338     133668       1217          7        256    09:46:01.622
