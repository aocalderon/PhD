acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 18:25:37,643 -> Starting session,1.63,0
2017-12-09 18:25:37,644 -> Setting variables,0.00,0
2017-12-09 18:25:42,097 -> Reading datasets,4.45,0
2017-12-09 18:25:42,117 -> Points partitions: 2
2017-12-09 18:25:42,130 -> Centers partitions: 2
2017-12-09 18:25:49,761 -> 01.Indexing points,7.58,78857,50.0,7
2017-12-09 18:25:57,985 -> 02.Indexing centers,8.22,502168,50.0,7
2017-12-09 18:25:57,997 -> 1024
2017-12-09 18:25:58,006 -> 1024
2017-12-09 18:26:20,211 -> 03.Joining datasets,22.20,502159,50.0,7
Done!!! Sat Dec  9 18:26:20 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 18:26:22,969 -> Starting session,1.51,0
2017-12-09 18:26:22,970 -> Setting variables,0.00,0
2017-12-09 18:26:27,209 -> Reading datasets,4.24,0
2017-12-09 18:26:27,223 -> Points partitions: 2
2017-12-09 18:26:27,232 -> Centers partitions: 2
2017-12-09 18:26:35,187 -> 01.Indexing points,7.75,78857,50.0,7
2017-12-09 18:26:43,829 -> 02.Indexing centers,8.64,502168,50.0,7
2017-12-09 18:26:43,839 -> 1024
2017-12-09 18:26:43,845 -> 1024
2017-12-09 18:27:06,930 -> 03.Joining datasets,23.08,502159,50.0,7
Done!!! Sat Dec  9 18:27:07 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 18:27:09,681 -> Starting session,1.59,0
2017-12-09 18:27:09,682 -> Setting variables,0.00,0
2017-12-09 18:27:13,994 -> Reading datasets,4.31,0
2017-12-09 18:27:14,009 -> Points partitions: 2
2017-12-09 18:27:14,019 -> Centers partitions: 2
2017-12-09 18:27:21,868 -> 01.Indexing points,7.81,78857,50.0,7
2017-12-09 18:27:30,250 -> 02.Indexing centers,8.38,502168,50.0,7
2017-12-09 18:27:30,258 -> 1024
2017-12-09 18:27:30,264 -> 1024
2017-12-09 18:27:52,439 -> 03.Joining datasets,22.17,502159,50.0,7
Done!!! Sat Dec  9 18:27:52 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 18:27:55,482 -> Starting session,1.86,0
2017-12-09 18:27:55,483 -> Setting variables,0.00,0
2017-12-09 18:27:59,695 -> Reading datasets,4.21,0
2017-12-09 18:27:59,706 -> Points partitions: 2
2017-12-09 18:27:59,713 -> Centers partitions: 2
2017-12-09 18:28:07,066 -> 01.Indexing points,7.31,78857,50.0,7
2017-12-09 18:28:14,999 -> 02.Indexing centers,7.93,502168,50.0,7
2017-12-09 18:28:15,011 -> 1024
2017-12-09 18:28:15,019 -> 1024
2017-12-09 18:28:36,442 -> 03.Joining datasets,21.42,502159,50.0,7
Done!!! Sat Dec  9 18:28:36 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 18:28:39,199 -> Starting session,1.54,0
2017-12-09 18:28:39,199 -> Setting variables,0.00,0
2017-12-09 18:28:43,533 -> Reading datasets,4.33,0
2017-12-09 18:28:43,549 -> Points partitions: 2
2017-12-09 18:28:43,558 -> Centers partitions: 2
2017-12-09 18:28:51,298 -> 01.Indexing points,7.70,78857,50.0,7
2017-12-09 18:28:59,699 -> 02.Indexing centers,8.40,502168,50.0,7
2017-12-09 18:28:59,711 -> 1024
2017-12-09 18:28:59,719 -> 1024
2017-12-09 18:29:22,306 -> 03.Joining datasets,22.59,502159,50.0,7
Done!!! Sat Dec  9 18:29:22 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 18:29:30,724 -> Starting session,1.67,0
2017-12-09 18:29:30,724 -> Setting variables,0.00,0
2017-12-09 18:29:35,837 -> Reading datasets,5.11,0
2017-12-09 18:29:35,851 -> Points partitions: 2
2017-12-09 18:29:35,861 -> Centers partitions: 2
2017-12-09 18:29:43,598 -> 01.Indexing points,7.70,78857,50.0,14
2017-12-09 18:29:51,738 -> 02.Indexing centers,8.14,502168,50.0,14
2017-12-09 18:29:51,746 -> 1024
2017-12-09 18:29:51,751 -> 1024
2017-12-09 18:30:09,566 -> 03.Joining datasets,17.81,502159,50.0,14
Done!!! Sat Dec  9 18:30:10 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 18:30:13,187 -> Starting session,1.62,0
2017-12-09 18:30:13,187 -> Setting variables,0.00,0
2017-12-09 18:30:18,193 -> Reading datasets,5.01,0
2017-12-09 18:30:18,210 -> Points partitions: 2
2017-12-09 18:30:18,220 -> Centers partitions: 2
2017-12-09 18:30:25,674 -> 01.Indexing points,7.41,78857,50.0,14
2017-12-09 18:30:34,465 -> 02.Indexing centers,8.79,502168,50.0,14
2017-12-09 18:30:34,472 -> 1024
2017-12-09 18:30:34,478 -> 1024
2017-12-09 18:30:52,012 -> 03.Joining datasets,17.53,502159,50.0,14
Done!!! Sat Dec  9 18:30:52 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 18:30:54,813 -> Starting session,1.68,0
2017-12-09 18:30:54,814 -> Setting variables,0.00,0
2017-12-09 18:30:59,954 -> Reading datasets,5.14,0
2017-12-09 18:30:59,965 -> Points partitions: 2
2017-12-09 18:30:59,973 -> Centers partitions: 2
2017-12-09 18:31:07,648 -> 01.Indexing points,7.64,78857,50.0,14
2017-12-09 18:31:16,063 -> 02.Indexing centers,8.41,502168,50.0,14
2017-12-09 18:31:16,071 -> 1024
2017-12-09 18:31:16,078 -> 1024
2017-12-09 18:31:34,318 -> 03.Joining datasets,18.24,502159,50.0,14
Done!!! Sat Dec  9 18:31:34 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 18:31:37,318 -> Starting session,1.79,0
2017-12-09 18:31:37,319 -> Setting variables,0.00,0
2017-12-09 18:31:42,531 -> Reading datasets,5.21,0
2017-12-09 18:31:42,541 -> Points partitions: 2
2017-12-09 18:31:42,548 -> Centers partitions: 2
2017-12-09 18:31:50,864 -> 01.Indexing points,8.27,78857,50.0,14
2017-12-09 18:31:59,233 -> 02.Indexing centers,8.37,502168,50.0,14
2017-12-09 18:31:59,241 -> 1024
2017-12-09 18:31:59,247 -> 1024
2017-12-09 18:32:17,568 -> 03.Joining datasets,18.32,502159,50.0,14
Done!!! Sat Dec  9 18:32:18 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 18:32:20,430 -> Starting session,1.57,0
2017-12-09 18:32:20,431 -> Setting variables,0.00,0
2017-12-09 18:32:25,500 -> Reading datasets,5.07,0
2017-12-09 18:32:25,511 -> Points partitions: 2
2017-12-09 18:32:25,519 -> Centers partitions: 2
2017-12-09 18:32:33,135 -> 01.Indexing points,7.58,78857,50.0,14
2017-12-09 18:32:41,171 -> 02.Indexing centers,8.03,502168,50.0,14
2017-12-09 18:32:41,182 -> 1024
2017-12-09 18:32:41,190 -> 1024
2017-12-09 18:32:58,985 -> 03.Joining datasets,17.94,502159,50.0,14
Done!!! Sat Dec  9 18:32:59 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 18:33:07,289 -> Starting session,1.79,0
2017-12-09 18:33:07,289 -> Setting variables,0.00,0
2017-12-09 18:33:13,521 -> Reading datasets,6.23,0
2017-12-09 18:33:13,542 -> Points partitions: 2
2017-12-09 18:33:13,555 -> Centers partitions: 2
2017-12-09 18:33:20,588 -> 01.Indexing points,6.97,78857,50.0,21
2017-12-09 18:33:28,639 -> 02.Indexing centers,8.05,502168,50.0,21
2017-12-09 18:33:28,650 -> 1024
2017-12-09 18:33:28,658 -> 1024
2017-12-09 18:33:42,283 -> 03.Joining datasets,13.62,502159,50.0,21
Done!!! Sat Dec  9 18:33:42 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 18:33:45,241 -> Starting session,1.69,0
2017-12-09 18:33:45,242 -> Setting variables,0.00,0
2017-12-09 18:33:51,025 -> Reading datasets,5.78,0
2017-12-09 18:33:51,044 -> Points partitions: 2
2017-12-09 18:33:51,056 -> Centers partitions: 2
2017-12-09 18:33:58,219 -> 01.Indexing points,7.11,78857,50.0,21
2017-12-09 18:34:05,680 -> 02.Indexing centers,7.46,502168,50.0,21
2017-12-09 18:34:05,690 -> 1024
2017-12-09 18:34:05,698 -> 1024
2017-12-09 18:34:19,191 -> 03.Joining datasets,13.49,502159,50.0,21
Done!!! Sat Dec  9 18:34:19 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 18:34:21,970 -> Starting session,1.62,0
2017-12-09 18:34:21,970 -> Setting variables,0.00,0
2017-12-09 18:34:27,816 -> Reading datasets,5.84,0
2017-12-09 18:34:27,836 -> Points partitions: 2
2017-12-09 18:34:27,848 -> Centers partitions: 2
2017-12-09 18:34:34,741 -> 01.Indexing points,6.84,78857,50.0,21
2017-12-09 18:34:42,702 -> 02.Indexing centers,7.96,502168,50.0,21
2017-12-09 18:34:42,710 -> 1024
2017-12-09 18:34:42,716 -> 1024
2017-12-09 18:34:57,119 -> 03.Joining datasets,14.40,502159,50.0,21
Done!!! Sat Dec  9 18:34:57 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 18:34:59,978 -> Starting session,1.56,0
2017-12-09 18:34:59,978 -> Setting variables,0.00,0
2017-12-09 18:35:05,124 -> Reading datasets,5.15,0
2017-12-09 18:35:05,135 -> Points partitions: 2
2017-12-09 18:35:05,143 -> Centers partitions: 2
2017-12-09 18:35:12,818 -> 01.Indexing points,7.64,78857,50.0,21
2017-12-09 18:35:20,529 -> 02.Indexing centers,7.71,502168,50.0,21
2017-12-09 18:35:20,540 -> 1024
2017-12-09 18:35:20,549 -> 1024
2017-12-09 18:35:33,326 -> 03.Joining datasets,12.78,502159,50.0,21
Done!!! Sat Dec  9 18:35:33 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 18:35:36,144 -> Starting session,1.60,0
2017-12-09 18:35:36,145 -> Setting variables,0.00,0
2017-12-09 18:35:41,128 -> Reading datasets,4.98,0
2017-12-09 18:35:41,141 -> Points partitions: 2
2017-12-09 18:35:41,152 -> Centers partitions: 2
2017-12-09 18:35:48,902 -> 01.Indexing points,7.71,78857,50.0,21
2017-12-09 18:35:56,616 -> 02.Indexing centers,7.71,502168,50.0,21
2017-12-09 18:35:56,624 -> 1024
2017-12-09 18:35:56,630 -> 1024
2017-12-09 18:36:10,395 -> 03.Joining datasets,13.77,502159,50.0,21
Done!!! Sat Dec  9 18:36:10 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 18:36:18,898 -> Starting session,1.92,0
2017-12-09 18:36:18,899 -> Setting variables,0.00,0
2017-12-09 18:36:26,108 -> Reading datasets,7.21,0
2017-12-09 18:36:26,122 -> Points partitions: 2
2017-12-09 18:36:26,134 -> Centers partitions: 2
2017-12-09 18:36:34,354 -> 01.Indexing points,8.17,78857,50.0,28
2017-12-09 18:36:43,044 -> 02.Indexing centers,8.69,502168,50.0,28
2017-12-09 18:36:43,052 -> 1024
2017-12-09 18:36:43,064 -> 1024
2017-12-09 18:36:55,847 -> 03.Joining datasets,12.78,502159,50.0,28
Done!!! Sat Dec  9 18:36:56 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 18:36:58,633 -> Starting session,1.51,0
2017-12-09 18:36:58,633 -> Setting variables,0.00,0
2017-12-09 18:37:05,803 -> Reading datasets,7.17,0
2017-12-09 18:37:05,821 -> Points partitions: 2
2017-12-09 18:37:05,833 -> Centers partitions: 2
2017-12-09 18:37:13,749 -> 01.Indexing points,7.87,78857,50.0,28
2017-12-09 18:37:23,769 -> 02.Indexing centers,10.02,502168,50.0,28
2017-12-09 18:37:23,777 -> 1024
2017-12-09 18:37:23,783 -> 1024
2017-12-09 18:37:37,318 -> 03.Joining datasets,13.53,502159,50.0,28
Done!!! Sat Dec  9 18:37:37 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 18:37:40,167 -> Starting session,1.61,0
2017-12-09 18:37:40,167 -> Setting variables,0.00,0
2017-12-09 18:37:46,254 -> Reading datasets,6.09,0
2017-12-09 18:37:46,269 -> Points partitions: 2
2017-12-09 18:37:46,278 -> Centers partitions: 2
2017-12-09 18:37:55,789 -> 01.Indexing points,9.47,78857,50.0,28
2017-12-09 18:38:07,558 -> 02.Indexing centers,11.77,502168,50.0,28
2017-12-09 18:38:07,570 -> 1024
2017-12-09 18:38:07,580 -> 1024
2017-12-09 18:38:20,962 -> 03.Joining datasets,13.38,502159,50.0,28
Done!!! Sat Dec  9 18:38:21 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 18:38:23,733 -> Starting session,1.54,0
2017-12-09 18:38:23,734 -> Setting variables,0.00,0
2017-12-09 18:38:31,104 -> Reading datasets,7.37,0
2017-12-09 18:38:31,122 -> Points partitions: 2
2017-12-09 18:38:31,133 -> Centers partitions: 2
2017-12-09 18:38:38,686 -> 01.Indexing points,8.05,78857,50.0,28
2017-12-09 18:38:50,078 -> 02.Indexing centers,11.39,502168,50.0,28
2017-12-09 18:38:50,091 -> 1024
2017-12-09 18:38:50,099 -> 1024
2017-12-09 18:39:02,269 -> 03.Joining datasets,12.17,502159,50.0,28
Done!!! Sat Dec  9 18:39:02 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 18:39:04,996 -> Starting session,1.63,0
2017-12-09 18:39:04,996 -> Setting variables,0.00,0
2017-12-09 18:39:12,347 -> Reading datasets,7.35,0
2017-12-09 18:39:12,370 -> Points partitions: 2
2017-12-09 18:39:12,387 -> Centers partitions: 2
2017-12-09 18:39:20,192 -> 01.Indexing points,7.74,78857,50.0,28
2017-12-09 18:39:30,779 -> 02.Indexing centers,10.58,502168,50.0,28
2017-12-09 18:39:30,788 -> 1024
2017-12-09 18:39:30,795 -> 1024
2017-12-09 18:39:42,971 -> 03.Joining datasets,12.18,502159,50.0,28
Done!!! Sat Dec  9 18:39:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 18:39:52,386 -> Starting session,1.77,0
2017-12-09 18:39:52,387 -> Setting variables,0.00,0
2017-12-09 18:39:56,792 -> Reading datasets,4.40,0
2017-12-09 18:39:56,805 -> Points partitions: 2
2017-12-09 18:39:56,816 -> Centers partitions: 2
2017-12-09 18:40:03,978 -> 01.Indexing points,7.12,59143,50.0,7
2017-12-09 18:40:11,222 -> 02.Indexing centers,7.24,376626,50.0,7
2017-12-09 18:40:11,233 -> 1024
2017-12-09 18:40:11,240 -> 1024
2017-12-09 18:40:30,659 -> 03.Joining datasets,19.49,376619,50.0,7
Done!!! Sat Dec  9 18:40:31 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 18:40:33,392 -> Starting session,1.54,0
2017-12-09 18:40:33,392 -> Setting variables,0.00,0
2017-12-09 18:40:37,789 -> Reading datasets,4.40,0
2017-12-09 18:40:37,804 -> Points partitions: 2
2017-12-09 18:40:37,813 -> Centers partitions: 2
2017-12-09 18:40:45,002 -> 01.Indexing points,7.15,59143,50.0,7
2017-12-09 18:40:52,259 -> 02.Indexing centers,7.25,376626,50.0,7
2017-12-09 18:40:52,274 -> 1024
2017-12-09 18:40:52,287 -> 1024
2017-12-09 18:41:13,207 -> 03.Joining datasets,20.92,376619,50.0,7
Done!!! Sat Dec  9 18:41:13 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 18:41:15,934 -> Starting session,1.52,0
2017-12-09 18:41:15,935 -> Setting variables,0.00,0
2017-12-09 18:41:20,148 -> Reading datasets,4.21,0
2017-12-09 18:41:20,162 -> Points partitions: 2
2017-12-09 18:41:20,171 -> Centers partitions: 2
2017-12-09 18:41:27,796 -> 01.Indexing points,7.58,59143,50.0,7
2017-12-09 18:41:35,171 -> 02.Indexing centers,7.37,376626,50.0,7
2017-12-09 18:41:35,186 -> 1024
2017-12-09 18:41:35,198 -> 1024
2017-12-09 18:41:56,491 -> 03.Joining datasets,21.29,376619,50.0,7
Done!!! Sat Dec  9 18:41:56 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 18:41:59,085 -> Starting session,1.50,0
2017-12-09 18:41:59,086 -> Setting variables,0.00,0
2017-12-09 18:42:03,791 -> Reading datasets,4.70,0
2017-12-09 18:42:03,810 -> Points partitions: 2
2017-12-09 18:42:03,821 -> Centers partitions: 2
2017-12-09 18:42:11,474 -> 01.Indexing points,7.60,59143,50.0,7
2017-12-09 18:42:18,882 -> 02.Indexing centers,7.41,376626,50.0,7
2017-12-09 18:42:18,895 -> 1024
2017-12-09 18:42:18,905 -> 1024
2017-12-09 18:42:39,749 -> 03.Joining datasets,20.84,376619,50.0,7
Done!!! Sat Dec  9 18:42:40 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 18:42:42,518 -> Starting session,1.54,0
2017-12-09 18:42:42,519 -> Setting variables,0.00,0
2017-12-09 18:42:46,921 -> Reading datasets,4.40,0
2017-12-09 18:42:46,931 -> Points partitions: 2
2017-12-09 18:42:46,939 -> Centers partitions: 2
2017-12-09 18:42:54,158 -> 01.Indexing points,7.18,59143,50.0,7
2017-12-09 18:43:01,450 -> 02.Indexing centers,7.29,376626,50.0,7
2017-12-09 18:43:01,462 -> 1024
2017-12-09 18:43:01,471 -> 1024
2017-12-09 18:43:22,012 -> 03.Joining datasets,20.54,376619,50.0,7
Done!!! Sat Dec  9 18:43:22 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 18:43:30,409 -> Starting session,1.68,0
2017-12-09 18:43:30,410 -> Setting variables,0.00,0
2017-12-09 18:43:35,359 -> Reading datasets,4.95,0
2017-12-09 18:43:35,373 -> Points partitions: 2
2017-12-09 18:43:35,384 -> Centers partitions: 2
2017-12-09 18:43:42,750 -> 01.Indexing points,7.33,59143,50.0,14
2017-12-09 18:43:50,114 -> 02.Indexing centers,7.36,376626,50.0,14
2017-12-09 18:43:50,122 -> 1024
2017-12-09 18:43:50,127 -> 1024
2017-12-09 18:44:06,492 -> 03.Joining datasets,16.36,376619,50.0,14
Done!!! Sat Dec  9 18:44:07 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 18:44:09,487 -> Starting session,1.78,0
2017-12-09 18:44:09,487 -> Setting variables,0.00,0
2017-12-09 18:44:14,378 -> Reading datasets,4.89,0
2017-12-09 18:44:14,392 -> Points partitions: 2
2017-12-09 18:44:14,403 -> Centers partitions: 2
2017-12-09 18:44:21,769 -> 01.Indexing points,7.32,59143,50.0,14
2017-12-09 18:44:29,200 -> 02.Indexing centers,7.43,376626,50.0,14
2017-12-09 18:44:29,213 -> 1024
2017-12-09 18:44:29,222 -> 1024
2017-12-09 18:44:46,197 -> 03.Joining datasets,16.98,376619,50.0,14
Done!!! Sat Dec  9 18:44:46 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 18:44:48,884 -> Starting session,1.58,0
2017-12-09 18:44:48,884 -> Setting variables,0.00,0
2017-12-09 18:44:53,825 -> Reading datasets,4.94,0
2017-12-09 18:44:53,847 -> Points partitions: 2
2017-12-09 18:44:53,859 -> Centers partitions: 2
2017-12-09 18:45:00,878 -> 01.Indexing points,6.97,59143,50.0,14
2017-12-09 18:45:08,203 -> 02.Indexing centers,7.32,376626,50.0,14
2017-12-09 18:45:08,211 -> 1024
2017-12-09 18:45:08,216 -> 1024
2017-12-09 18:45:24,350 -> 03.Joining datasets,16.13,376619,50.0,14
Done!!! Sat Dec  9 18:45:24 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 18:45:27,291 -> Starting session,1.72,0
2017-12-09 18:45:27,292 -> Setting variables,0.00,0
2017-12-09 18:45:32,266 -> Reading datasets,4.97,0
2017-12-09 18:45:32,279 -> Points partitions: 2
2017-12-09 18:45:32,287 -> Centers partitions: 2
2017-12-09 18:45:39,794 -> 01.Indexing points,7.42,59143,50.0,14
2017-12-09 18:45:46,764 -> 02.Indexing centers,6.97,376626,50.0,14
2017-12-09 18:45:46,774 -> 1024
2017-12-09 18:45:46,783 -> 1024
2017-12-09 18:46:03,713 -> 03.Joining datasets,16.93,376619,50.0,14
Done!!! Sat Dec  9 18:46:04 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 18:46:06,553 -> Starting session,1.62,0
2017-12-09 18:46:06,554 -> Setting variables,0.00,0
2017-12-09 18:46:11,711 -> Reading datasets,5.16,0
2017-12-09 18:46:11,721 -> Points partitions: 2
2017-12-09 18:46:11,729 -> Centers partitions: 2
2017-12-09 18:46:19,065 -> 01.Indexing points,7.30,59143,50.0,14
2017-12-09 18:46:26,609 -> 02.Indexing centers,7.54,376626,50.0,14
2017-12-09 18:46:26,618 -> 1024
2017-12-09 18:46:26,624 -> 1024
2017-12-09 18:46:43,711 -> 03.Joining datasets,17.09,376619,50.0,14
Done!!! Sat Dec  9 18:46:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 18:46:52,214 -> Starting session,1.75,0
2017-12-09 18:46:52,216 -> Setting variables,0.00,0
2017-12-09 18:46:57,240 -> Reading datasets,5.02,0
2017-12-09 18:46:57,251 -> Points partitions: 2
2017-12-09 18:46:57,259 -> Centers partitions: 2
2017-12-09 18:47:04,704 -> 01.Indexing points,7.41,59143,50.0,21
2017-12-09 18:47:11,718 -> 02.Indexing centers,7.01,376626,50.0,21
2017-12-09 18:47:11,733 -> 1024
2017-12-09 18:47:11,742 -> 1024
2017-12-09 18:47:24,714 -> 03.Joining datasets,12.97,376619,50.0,21
Done!!! Sat Dec  9 18:47:26 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 18:47:28,371 -> Starting session,1.66,0
2017-12-09 18:47:28,372 -> Setting variables,0.00,0
2017-12-09 18:47:34,267 -> Reading datasets,5.89,0
2017-12-09 18:47:34,280 -> Points partitions: 2
2017-12-09 18:47:34,288 -> Centers partitions: 2
2017-12-09 18:47:40,898 -> 01.Indexing points,6.57,59143,50.0,21
2017-12-09 18:47:47,678 -> 02.Indexing centers,6.78,376626,50.0,21
2017-12-09 18:47:47,688 -> 1024
2017-12-09 18:47:47,696 -> 1024
2017-12-09 18:48:00,551 -> 03.Joining datasets,12.85,376619,50.0,21
Done!!! Sat Dec  9 18:48:01 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 18:48:03,816 -> Starting session,1.53,0
2017-12-09 18:48:03,817 -> Setting variables,0.00,0
2017-12-09 18:48:09,797 -> Reading datasets,5.98,0
2017-12-09 18:48:09,809 -> Points partitions: 2
2017-12-09 18:48:09,817 -> Centers partitions: 2
2017-12-09 18:48:16,126 -> 01.Indexing points,6.27,59143,50.0,21
2017-12-09 18:48:23,299 -> 02.Indexing centers,7.17,376626,50.0,21
2017-12-09 18:48:23,311 -> 1024
2017-12-09 18:48:23,321 -> 1024
2017-12-09 18:48:36,169 -> 03.Joining datasets,12.85,376619,50.0,21
Done!!! Sat Dec  9 18:48:36 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 18:48:39,085 -> Starting session,1.85,0
2017-12-09 18:48:39,086 -> Setting variables,0.00,0
2017-12-09 18:48:44,984 -> Reading datasets,5.90,0
2017-12-09 18:48:45,004 -> Points partitions: 2
2017-12-09 18:48:45,017 -> Centers partitions: 2
2017-12-09 18:48:51,395 -> 01.Indexing points,6.33,59143,50.0,21
2017-12-09 18:48:58,147 -> 02.Indexing centers,6.75,376626,50.0,21
2017-12-09 18:48:58,156 -> 1024
2017-12-09 18:48:58,162 -> 1024
2017-12-09 18:49:11,603 -> 03.Joining datasets,13.44,376619,50.0,21
Done!!! Sat Dec  9 18:49:12 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 18:49:14,379 -> Starting session,1.59,0
2017-12-09 18:49:14,380 -> Setting variables,0.00,0
2017-12-09 18:49:20,241 -> Reading datasets,5.86,0
2017-12-09 18:49:20,263 -> Points partitions: 2
2017-12-09 18:49:20,279 -> Centers partitions: 2
2017-12-09 18:49:27,132 -> 01.Indexing points,6.80,59143,50.0,21
2017-12-09 18:49:33,866 -> 02.Indexing centers,6.73,376626,50.0,21
2017-12-09 18:49:33,876 -> 1024
2017-12-09 18:49:33,882 -> 1024
2017-12-09 18:49:46,757 -> 03.Joining datasets,12.87,376619,50.0,21
Done!!! Sat Dec  9 18:49:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 18:49:56,166 -> Starting session,1.84,0
2017-12-09 18:49:56,167 -> Setting variables,0.00,0
2017-12-09 18:50:03,565 -> Reading datasets,7.40,0
2017-12-09 18:50:03,577 -> Points partitions: 2
2017-12-09 18:50:03,584 -> Centers partitions: 2
2017-12-09 18:50:11,401 -> 01.Indexing points,7.78,59143,50.0,28
2017-12-09 18:50:17,939 -> 02.Indexing centers,6.54,376626,50.0,28
2017-12-09 18:50:17,950 -> 1024
2017-12-09 18:50:17,959 -> 1024
2017-12-09 18:50:29,173 -> 03.Joining datasets,11.21,376619,50.0,28
Done!!! Sat Dec  9 18:50:29 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 18:50:31,938 -> Starting session,1.54,0
2017-12-09 18:50:31,939 -> Setting variables,0.00,0
2017-12-09 18:50:39,272 -> Reading datasets,7.33,0
2017-12-09 18:50:39,292 -> Points partitions: 2
2017-12-09 18:50:39,306 -> Centers partitions: 2
2017-12-09 18:50:45,705 -> 01.Indexing points,6.34,59143,50.0,28
2017-12-09 18:50:52,524 -> 02.Indexing centers,6.82,376626,50.0,28
2017-12-09 18:50:52,538 -> 1024
2017-12-09 18:50:52,547 -> 1024
2017-12-09 18:51:04,531 -> 03.Joining datasets,11.98,376619,50.0,28
Done!!! Sat Dec  9 18:51:05 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 18:51:07,363 -> Starting session,1.71,0
2017-12-09 18:51:07,363 -> Setting variables,0.00,0
2017-12-09 18:51:14,270 -> Reading datasets,6.91,0
2017-12-09 18:51:14,290 -> Points partitions: 2
2017-12-09 18:51:14,303 -> Centers partitions: 2
2017-12-09 18:51:21,470 -> 01.Indexing points,7.11,59143,50.0,28
2017-12-09 18:51:29,524 -> 02.Indexing centers,8.05,376626,50.0,28
2017-12-09 18:51:29,532 -> 1024
2017-12-09 18:51:29,539 -> 1024
2017-12-09 18:51:41,392 -> 03.Joining datasets,11.85,376619,50.0,28
Done!!! Sat Dec  9 18:51:41 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 18:51:44,206 -> Starting session,1.59,0
2017-12-09 18:51:44,206 -> Setting variables,0.00,0
2017-12-09 18:51:51,411 -> Reading datasets,7.20,0
2017-12-09 18:51:51,430 -> Points partitions: 2
2017-12-09 18:51:51,446 -> Centers partitions: 2
2017-12-09 18:51:59,322 -> 01.Indexing points,7.82,59143,50.0,28
2017-12-09 18:52:06,973 -> 02.Indexing centers,7.65,376626,50.0,28
2017-12-09 18:52:06,981 -> 1024
2017-12-09 18:52:06,988 -> 1024
2017-12-09 18:52:18,582 -> 03.Joining datasets,11.59,376619,50.0,28
Done!!! Sat Dec  9 18:52:19 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 18:52:21,270 -> Starting session,1.55,0
2017-12-09 18:52:21,270 -> Setting variables,0.00,0
2017-12-09 18:52:28,295 -> Reading datasets,7.02,0
2017-12-09 18:52:28,314 -> Points partitions: 2
2017-12-09 18:52:28,326 -> Centers partitions: 2
2017-12-09 18:52:36,194 -> 01.Indexing points,7.81,59143,50.0,28
2017-12-09 18:52:42,756 -> 02.Indexing centers,6.56,376626,50.0,28
2017-12-09 18:52:42,767 -> 1024
2017-12-09 18:52:42,774 -> 1024
2017-12-09 18:52:54,575 -> 03.Joining datasets,11.80,376619,50.0,28
Done!!! Sat Dec  9 18:52:55 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 18:53:03,022 -> Starting session,1.47,0
2017-12-09 18:53:03,022 -> Setting variables,0.00,0
2017-12-09 18:53:07,245 -> Reading datasets,4.22,0
2017-12-09 18:53:07,259 -> Points partitions: 2
2017-12-09 18:53:07,270 -> Centers partitions: 2
2017-12-09 18:53:14,531 -> 01.Indexing points,7.22,39429,50.0,7
2017-12-09 18:53:20,837 -> 02.Indexing centers,6.31,251084,50.0,7
2017-12-09 18:53:20,852 -> 1024
2017-12-09 18:53:20,862 -> 1024
2017-12-09 18:53:39,272 -> 03.Joining datasets,18.49,251080,50.0,7
Done!!! Sat Dec  9 18:53:39 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 18:53:42,122 -> Starting session,1.56,0
2017-12-09 18:53:42,123 -> Setting variables,0.00,0
2017-12-09 18:53:46,448 -> Reading datasets,4.33,0
2017-12-09 18:53:46,460 -> Points partitions: 2
2017-12-09 18:53:46,468 -> Centers partitions: 2
2017-12-09 18:53:53,643 -> 01.Indexing points,7.14,39429,50.0,7
2017-12-09 18:53:59,960 -> 02.Indexing centers,6.32,251084,50.0,7
2017-12-09 18:53:59,970 -> 1024
2017-12-09 18:53:59,978 -> 1024
2017-12-09 18:54:18,927 -> 03.Joining datasets,18.95,251080,50.0,7
Done!!! Sat Dec  9 18:54:19 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 18:54:21,814 -> Starting session,1.70,0
2017-12-09 18:54:21,814 -> Setting variables,0.00,0
2017-12-09 18:54:25,922 -> Reading datasets,4.11,0
2017-12-09 18:54:25,933 -> Points partitions: 2
2017-12-09 18:54:25,940 -> Centers partitions: 2
2017-12-09 18:54:33,256 -> 01.Indexing points,7.28,39429,50.0,7
2017-12-09 18:54:39,552 -> 02.Indexing centers,6.29,251084,50.0,7
2017-12-09 18:54:39,565 -> 1024
2017-12-09 18:54:39,574 -> 1024
2017-12-09 18:54:57,742 -> 03.Joining datasets,18.17,251080,50.0,7
Done!!! Sat Dec  9 18:54:58 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 18:55:00,567 -> Starting session,1.56,0
2017-12-09 18:55:00,567 -> Setting variables,0.00,0
2017-12-09 18:55:04,835 -> Reading datasets,4.27,0
2017-12-09 18:55:04,851 -> Points partitions: 2
2017-12-09 18:55:04,861 -> Centers partitions: 2
2017-12-09 18:55:12,358 -> 01.Indexing points,7.45,39429,50.0,7
2017-12-09 18:55:18,586 -> 02.Indexing centers,6.23,251084,50.0,7
2017-12-09 18:55:18,600 -> 1024
2017-12-09 18:55:18,608 -> 1024
2017-12-09 18:55:36,975 -> 03.Joining datasets,18.37,251080,50.0,7
Done!!! Sat Dec  9 18:55:37 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 18:55:39,646 -> Starting session,1.50,0
2017-12-09 18:55:39,647 -> Setting variables,0.00,0
2017-12-09 18:55:43,865 -> Reading datasets,4.22,0
2017-12-09 18:55:43,878 -> Points partitions: 2
2017-12-09 18:55:43,886 -> Centers partitions: 2
2017-12-09 18:55:51,185 -> 01.Indexing points,7.26,39429,50.0,7
2017-12-09 18:55:57,615 -> 02.Indexing centers,6.43,251084,50.0,7
2017-12-09 18:55:57,628 -> 1024
2017-12-09 18:55:57,636 -> 1024
2017-12-09 18:56:16,668 -> 03.Joining datasets,19.03,251080,50.0,7
Done!!! Sat Dec  9 18:56:17 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 18:56:25,129 -> Starting session,1.75,0
2017-12-09 18:56:25,130 -> Setting variables,0.00,0
2017-12-09 18:56:29,921 -> Reading datasets,4.79,0
2017-12-09 18:56:29,931 -> Points partitions: 2
2017-12-09 18:56:29,940 -> Centers partitions: 2
2017-12-09 18:56:36,904 -> 01.Indexing points,6.92,39429,50.0,14
2017-12-09 18:56:43,186 -> 02.Indexing centers,6.28,251084,50.0,14
2017-12-09 18:56:43,196 -> 1024
2017-12-09 18:56:43,202 -> 1024
2017-12-09 18:56:58,335 -> 03.Joining datasets,15.13,251080,50.0,14
Done!!! Sat Dec  9 18:56:59 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 18:57:01,879 -> Starting session,1.49,0
2017-12-09 18:57:01,880 -> Setting variables,0.00,0
2017-12-09 18:57:06,721 -> Reading datasets,4.84,0
2017-12-09 18:57:06,738 -> Points partitions: 2
2017-12-09 18:57:06,749 -> Centers partitions: 2
2017-12-09 18:57:14,026 -> 01.Indexing points,7.23,39429,50.0,14
2017-12-09 18:57:20,936 -> 02.Indexing centers,6.91,251084,50.0,14
2017-12-09 18:57:20,945 -> 1024
2017-12-09 18:57:20,953 -> 1024
2017-12-09 18:57:35,683 -> 03.Joining datasets,14.73,251080,50.0,14
Done!!! Sat Dec  9 18:57:36 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 18:57:38,505 -> Starting session,1.58,0
2017-12-09 18:57:38,506 -> Setting variables,0.00,0
2017-12-09 18:57:43,189 -> Reading datasets,4.68,0
2017-12-09 18:57:43,199 -> Points partitions: 2
2017-12-09 18:57:43,206 -> Centers partitions: 2
2017-12-09 18:57:50,273 -> 01.Indexing points,7.03,39429,50.0,14
2017-12-09 18:57:56,573 -> 02.Indexing centers,6.30,251084,50.0,14
2017-12-09 18:57:56,581 -> 1024
2017-12-09 18:57:56,587 -> 1024
2017-12-09 18:58:11,795 -> 03.Joining datasets,15.21,251080,50.0,14
Done!!! Sat Dec  9 18:58:12 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 18:58:14,567 -> Starting session,1.55,0
2017-12-09 18:58:14,568 -> Setting variables,0.00,0
2017-12-09 18:58:19,465 -> Reading datasets,4.90,0
2017-12-09 18:58:19,475 -> Points partitions: 2
2017-12-09 18:58:19,483 -> Centers partitions: 2
2017-12-09 18:58:26,558 -> 01.Indexing points,7.04,39429,50.0,14
2017-12-09 18:58:32,708 -> 02.Indexing centers,6.15,251084,50.0,14
2017-12-09 18:58:32,720 -> 1024
2017-12-09 18:58:32,731 -> 1024
2017-12-09 18:58:47,800 -> 03.Joining datasets,15.07,251080,50.0,14
Done!!! Sat Dec  9 18:58:48 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 18:58:50,578 -> Starting session,1.55,0
2017-12-09 18:58:50,579 -> Setting variables,0.00,0
2017-12-09 18:58:55,505 -> Reading datasets,4.93,0
2017-12-09 18:58:55,519 -> Points partitions: 2
2017-12-09 18:58:55,530 -> Centers partitions: 2
2017-12-09 18:59:02,653 -> 01.Indexing points,7.08,39429,50.0,14
2017-12-09 18:59:09,125 -> 02.Indexing centers,6.47,251084,50.0,14
2017-12-09 18:59:09,133 -> 1024
2017-12-09 18:59:09,139 -> 1024
2017-12-09 18:59:24,313 -> 03.Joining datasets,15.25,251080,50.0,14
Done!!! Sat Dec  9 18:59:25 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 18:59:33,688 -> Starting session,1.76,0
2017-12-09 18:59:33,689 -> Setting variables,0.00,0
2017-12-09 18:59:38,718 -> Reading datasets,5.03,0
2017-12-09 18:59:38,729 -> Points partitions: 2
2017-12-09 18:59:38,737 -> Centers partitions: 2
2017-12-09 18:59:46,198 -> 01.Indexing points,7.42,39429,50.0,21
2017-12-09 18:59:52,197 -> 02.Indexing centers,6.00,251084,50.0,21
2017-12-09 18:59:52,209 -> 1024
2017-12-09 18:59:52,219 -> 1024
2017-12-09 19:00:03,662 -> 03.Joining datasets,11.44,251080,50.0,21
Done!!! Sat Dec  9 19:00:04 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 19:00:06,396 -> Starting session,1.60,0
2017-12-09 19:00:06,397 -> Setting variables,0.00,0
2017-12-09 19:00:12,027 -> Reading datasets,5.63,0
2017-12-09 19:00:12,044 -> Points partitions: 2
2017-12-09 19:00:12,055 -> Centers partitions: 2
2017-12-09 19:00:18,615 -> 01.Indexing points,6.51,39429,50.0,21
2017-12-09 19:00:24,569 -> 02.Indexing centers,5.95,251084,50.0,21
2017-12-09 19:00:24,579 -> 1024
2017-12-09 19:00:24,586 -> 1024
2017-12-09 19:00:36,367 -> 03.Joining datasets,11.78,251080,50.0,21
Done!!! Sat Dec  9 19:00:36 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 19:00:39,266 -> Starting session,1.68,0
2017-12-09 19:00:39,266 -> Setting variables,0.00,0
2017-12-09 19:00:45,093 -> Reading datasets,5.83,0
2017-12-09 19:00:45,108 -> Points partitions: 2
2017-12-09 19:00:45,118 -> Centers partitions: 2
2017-12-09 19:00:51,851 -> 01.Indexing points,6.57,39429,50.0,21
2017-12-09 19:00:57,768 -> 02.Indexing centers,5.91,251084,50.0,21
2017-12-09 19:00:57,781 -> 1024
2017-12-09 19:00:57,791 -> 1024
2017-12-09 19:01:09,181 -> 03.Joining datasets,11.39,251080,50.0,21
Done!!! Sat Dec  9 19:01:10 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 19:01:12,743 -> Starting session,1.65,0
2017-12-09 19:01:12,744 -> Setting variables,0.00,0
2017-12-09 19:01:17,458 -> Reading datasets,4.71,0
2017-12-09 19:01:17,471 -> Points partitions: 2
2017-12-09 19:01:17,478 -> Centers partitions: 2
2017-12-09 19:01:24,677 -> 01.Indexing points,7.16,39429,50.0,21
2017-12-09 19:01:30,203 -> 02.Indexing centers,5.52,251084,50.0,21
2017-12-09 19:01:30,212 -> 1024
2017-12-09 19:01:30,219 -> 1024
2017-12-09 19:01:41,662 -> 03.Joining datasets,11.44,251080,50.0,21
Done!!! Sat Dec  9 19:01:42 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 19:01:44,430 -> Starting session,1.59,0
2017-12-09 19:01:44,430 -> Setting variables,0.00,0
2017-12-09 19:01:50,149 -> Reading datasets,5.72,0
2017-12-09 19:01:50,167 -> Points partitions: 2
2017-12-09 19:01:50,179 -> Centers partitions: 2
2017-12-09 19:01:56,561 -> 01.Indexing points,6.33,39429,50.0,21
2017-12-09 19:02:02,173 -> 02.Indexing centers,5.61,251084,50.0,21
2017-12-09 19:02:02,181 -> 1024
2017-12-09 19:02:02,187 -> 1024
2017-12-09 19:02:13,386 -> 03.Joining datasets,11.32,251080,50.0,21
Done!!! Sat Dec  9 19:02:13 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 19:02:21,874 -> Starting session,1.71,0
2017-12-09 19:02:21,874 -> Setting variables,0.00,0
2017-12-09 19:02:28,884 -> Reading datasets,7.01,0
2017-12-09 19:02:28,901 -> Points partitions: 2
2017-12-09 19:02:28,911 -> Centers partitions: 2
2017-12-09 19:02:35,122 -> 01.Indexing points,6.17,39429,50.0,28
2017-12-09 19:02:41,225 -> 02.Indexing centers,6.10,251084,50.0,28
2017-12-09 19:02:41,233 -> 1024
2017-12-09 19:02:41,240 -> 1024
2017-12-09 19:02:51,481 -> 03.Joining datasets,10.24,251080,50.0,28
Done!!! Sat Dec  9 19:02:52 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 19:02:54,403 -> Starting session,1.65,0
2017-12-09 19:02:54,403 -> Setting variables,0.00,0
2017-12-09 19:02:59,296 -> Reading datasets,4.89,0
2017-12-09 19:02:59,307 -> Points partitions: 2
2017-12-09 19:02:59,314 -> Centers partitions: 2
2017-12-09 19:03:08,394 -> 01.Indexing points,9.04,39429,50.0,28
2017-12-09 19:03:14,272 -> 02.Indexing centers,5.88,251084,50.0,28
2017-12-09 19:03:14,289 -> 1024
2017-12-09 19:03:14,299 -> 1024
2017-12-09 19:03:25,224 -> 03.Joining datasets,10.93,251080,50.0,28
Done!!! Sat Dec  9 19:03:25 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 19:03:28,109 -> Starting session,1.58,0
2017-12-09 19:03:28,109 -> Setting variables,0.00,0
2017-12-09 19:03:34,045 -> Reading datasets,5.94,0
2017-12-09 19:03:34,067 -> Points partitions: 2
2017-12-09 19:03:34,080 -> Centers partitions: 2
2017-12-09 19:03:42,612 -> 01.Indexing points,8.47,39429,50.0,28
2017-12-09 19:03:48,567 -> 02.Indexing centers,5.95,251084,50.0,28
2017-12-09 19:03:48,578 -> 1024
2017-12-09 19:03:48,586 -> 1024
2017-12-09 19:03:59,347 -> 03.Joining datasets,10.76,251080,50.0,28
Done!!! Sat Dec  9 19:04:00 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 19:04:03,015 -> Starting session,1.63,0
2017-12-09 19:04:03,016 -> Setting variables,0.00,0
2017-12-09 19:04:10,354 -> Reading datasets,7.34,0
2017-12-09 19:04:10,378 -> Points partitions: 2
2017-12-09 19:04:10,395 -> Centers partitions: 2
2017-12-09 19:04:17,572 -> 01.Indexing points,7.12,39429,50.0,28
2017-12-09 19:04:25,246 -> 02.Indexing centers,7.67,251084,50.0,28
2017-12-09 19:04:25,257 -> 1024
2017-12-09 19:04:25,266 -> 1024
2017-12-09 19:04:35,216 -> 03.Joining datasets,9.95,251080,50.0,28
Done!!! Sat Dec  9 19:04:35 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 19:04:38,001 -> Starting session,1.56,0
2017-12-09 19:04:38,002 -> Setting variables,0.00,0
2017-12-09 19:04:44,987 -> Reading datasets,6.99,0
2017-12-09 19:04:45,004 -> Points partitions: 2
2017-12-09 19:04:45,015 -> Centers partitions: 2
2017-12-09 19:04:52,279 -> 01.Indexing points,7.21,39429,50.0,28
2017-12-09 19:04:58,571 -> 02.Indexing centers,6.29,251084,50.0,28
2017-12-09 19:04:58,580 -> 1024
2017-12-09 19:04:58,589 -> 1024
2017-12-09 19:05:09,322 -> 03.Joining datasets,10.73,251080,50.0,28
Done!!! Sat Dec  9 19:05:10 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 19:05:18,556 -> Starting session,1.43,0
2017-12-09 19:05:18,556 -> Setting variables,0.00,0
2017-12-09 19:05:22,868 -> Reading datasets,4.31,0
2017-12-09 19:05:22,884 -> Points partitions: 2
2017-12-09 19:05:22,896 -> Centers partitions: 2
2017-12-09 19:05:29,606 -> 01.Indexing points,6.67,19715,50.0,7
2017-12-09 19:05:35,229 -> 02.Indexing centers,5.62,125542,50.0,7
2017-12-09 19:05:35,242 -> 992
2017-12-09 19:05:35,251 -> 1024
2017-12-09 19:05:50,717 -> 03.Joining datasets,15.47,125540,50.0,7
Done!!! Sat Dec  9 19:05:51 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 19:05:53,475 -> Starting session,1.57,0
2017-12-09 19:05:53,475 -> Setting variables,0.00,0
2017-12-09 19:05:57,592 -> Reading datasets,4.12,0
2017-12-09 19:05:57,604 -> Points partitions: 2
2017-12-09 19:05:57,615 -> Centers partitions: 2
2017-12-09 19:06:04,506 -> 01.Indexing points,6.85,19715,50.0,7
2017-12-09 19:06:10,008 -> 02.Indexing centers,5.50,125542,50.0,7
2017-12-09 19:06:10,020 -> 992
2017-12-09 19:06:10,028 -> 1024
2017-12-09 19:06:25,483 -> 03.Joining datasets,15.45,125540,50.0,7
Done!!! Sat Dec  9 19:06:25 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 19:06:28,133 -> Starting session,1.56,0
2017-12-09 19:06:28,134 -> Setting variables,0.00,0
2017-12-09 19:06:32,451 -> Reading datasets,4.32,0
2017-12-09 19:06:32,467 -> Points partitions: 2
2017-12-09 19:06:32,476 -> Centers partitions: 2
2017-12-09 19:06:39,080 -> 01.Indexing points,6.57,19715,50.0,7
2017-12-09 19:06:44,342 -> 02.Indexing centers,5.26,125542,50.0,7
2017-12-09 19:06:44,351 -> 992
2017-12-09 19:06:44,358 -> 1024
2017-12-09 19:06:59,827 -> 03.Joining datasets,15.47,125540,50.0,7
Done!!! Sat Dec  9 19:07:00 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 19:07:02,575 -> Starting session,1.58,0
2017-12-09 19:07:02,576 -> Setting variables,0.00,0
2017-12-09 19:07:06,714 -> Reading datasets,4.14,0
2017-12-09 19:07:06,731 -> Points partitions: 2
2017-12-09 19:07:06,744 -> Centers partitions: 2
2017-12-09 19:07:13,472 -> 01.Indexing points,6.68,19715,50.0,7
2017-12-09 19:07:18,950 -> 02.Indexing centers,5.48,125542,50.0,7
2017-12-09 19:07:18,963 -> 992
2017-12-09 19:07:18,972 -> 1024
2017-12-09 19:07:34,506 -> 03.Joining datasets,15.53,125540,50.0,7
Done!!! Sat Dec  9 19:07:34 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 19:07:37,408 -> Starting session,1.72,0
2017-12-09 19:07:37,408 -> Setting variables,0.00,0
2017-12-09 19:07:41,637 -> Reading datasets,4.23,0
2017-12-09 19:07:41,651 -> Points partitions: 2
2017-12-09 19:07:41,661 -> Centers partitions: 2
2017-12-09 19:07:48,297 -> 01.Indexing points,6.59,19715,50.0,7
2017-12-09 19:07:53,626 -> 02.Indexing centers,5.33,125542,50.0,7
2017-12-09 19:07:53,640 -> 992
2017-12-09 19:07:53,651 -> 1024
2017-12-09 19:08:08,956 -> 03.Joining datasets,15.30,125540,50.0,7
Done!!! Sat Dec  9 19:08:09 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 19:08:17,434 -> Starting session,1.72,0
2017-12-09 19:08:17,435 -> Setting variables,0.00,0
2017-12-09 19:08:22,097 -> Reading datasets,4.66,0
2017-12-09 19:08:22,111 -> Points partitions: 2
2017-12-09 19:08:22,122 -> Centers partitions: 2
2017-12-09 19:08:29,261 -> 01.Indexing points,7.09,19715,50.0,14
2017-12-09 19:08:34,724 -> 02.Indexing centers,5.46,125542,50.0,14
2017-12-09 19:08:34,732 -> 992
2017-12-09 19:08:34,738 -> 1024
2017-12-09 19:08:47,871 -> 03.Joining datasets,13.27,125540,50.0,14
Done!!! Sat Dec  9 19:08:48 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 19:08:50,693 -> Starting session,1.59,0
2017-12-09 19:08:50,693 -> Setting variables,0.00,0
2017-12-09 19:08:55,509 -> Reading datasets,4.82,0
2017-12-09 19:08:55,523 -> Points partitions: 2
2017-12-09 19:08:55,533 -> Centers partitions: 2
2017-12-09 19:09:02,306 -> 01.Indexing points,6.74,19715,50.0,14
2017-12-09 19:09:07,324 -> 02.Indexing centers,5.02,125542,50.0,14
2017-12-09 19:09:07,334 -> 992
2017-12-09 19:09:07,342 -> 1024
2017-12-09 19:09:20,393 -> 03.Joining datasets,13.05,125540,50.0,14
Done!!! Sat Dec  9 19:09:20 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 19:09:23,164 -> Starting session,1.55,0
2017-12-09 19:09:23,165 -> Setting variables,0.00,0
2017-12-09 19:09:28,076 -> Reading datasets,4.91,0
2017-12-09 19:09:28,086 -> Points partitions: 2
2017-12-09 19:09:28,094 -> Centers partitions: 2
2017-12-09 19:09:34,813 -> 01.Indexing points,6.68,19715,50.0,14
2017-12-09 19:09:40,172 -> 02.Indexing centers,5.36,125542,50.0,14
2017-12-09 19:09:40,180 -> 992
2017-12-09 19:09:40,185 -> 1024
2017-12-09 19:09:52,546 -> 03.Joining datasets,12.36,125540,50.0,14
Done!!! Sat Dec  9 19:09:53 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 19:09:55,406 -> Starting session,1.62,0
2017-12-09 19:09:55,406 -> Setting variables,0.00,0
2017-12-09 19:09:59,979 -> Reading datasets,4.57,0
2017-12-09 19:09:59,992 -> Points partitions: 2
2017-12-09 19:10:00,004 -> Centers partitions: 2
2017-12-09 19:10:07,202 -> 01.Indexing points,7.15,19715,50.0,14
2017-12-09 19:10:12,541 -> 02.Indexing centers,5.34,125542,50.0,14
2017-12-09 19:10:12,549 -> 992
2017-12-09 19:10:12,555 -> 1024
2017-12-09 19:10:24,686 -> 03.Joining datasets,12.13,125540,50.0,14
Done!!! Sat Dec  9 19:10:25 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 19:10:27,484 -> Starting session,1.57,0
2017-12-09 19:10:27,485 -> Setting variables,0.00,0
2017-12-09 19:10:32,208 -> Reading datasets,4.72,0
2017-12-09 19:10:32,218 -> Points partitions: 2
2017-12-09 19:10:32,227 -> Centers partitions: 2
2017-12-09 19:10:39,255 -> 01.Indexing points,6.99,19715,50.0,14
2017-12-09 19:10:44,388 -> 02.Indexing centers,5.13,125542,50.0,14
2017-12-09 19:10:44,396 -> 992
2017-12-09 19:10:44,401 -> 1024
2017-12-09 19:10:56,733 -> 03.Joining datasets,12.33,125540,50.0,14
Done!!! Sat Dec  9 19:10:57 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 19:11:05,031 -> Starting session,1.55,0
2017-12-09 19:11:05,032 -> Setting variables,0.00,0
2017-12-09 19:11:10,184 -> Reading datasets,5.15,0
2017-12-09 19:11:10,201 -> Points partitions: 2
2017-12-09 19:11:10,213 -> Centers partitions: 2
2017-12-09 19:11:18,401 -> 01.Indexing points,8.14,19715,50.0,21
2017-12-09 19:11:23,090 -> 02.Indexing centers,4.69,125542,50.0,21
2017-12-09 19:11:23,101 -> 992
2017-12-09 19:11:23,111 -> 1024
2017-12-09 19:11:33,940 -> 03.Joining datasets,10.83,125540,50.0,21
Done!!! Sat Dec  9 19:11:34 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 19:11:36,699 -> Starting session,1.50,0
2017-12-09 19:11:36,699 -> Setting variables,0.00,0
2017-12-09 19:11:41,457 -> Reading datasets,4.76,0
2017-12-09 19:11:41,470 -> Points partitions: 2
2017-12-09 19:11:41,480 -> Centers partitions: 2
2017-12-09 19:11:49,599 -> 01.Indexing points,8.07,19715,50.0,21
2017-12-09 19:11:55,406 -> 02.Indexing centers,4.93,125542,50.0,21
2017-12-09 19:11:55,417 -> 992
2017-12-09 19:11:55,427 -> 1024
2017-12-09 19:12:09,334 -> 03.Joining datasets,13.91,125540,50.0,21
Done!!! Sat Dec  9 19:12:09 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 19:12:12,152 -> Starting session,1.62,0
2017-12-09 19:12:12,153 -> Setting variables,0.00,0
2017-12-09 19:12:17,993 -> Reading datasets,5.84,0
2017-12-09 19:12:18,013 -> Points partitions: 2
2017-12-09 19:12:18,023 -> Centers partitions: 2
2017-12-09 19:12:25,283 -> 01.Indexing points,7.22,19715,50.0,21
2017-12-09 19:12:30,804 -> 02.Indexing centers,5.52,125542,50.0,21
2017-12-09 19:12:30,813 -> 992
2017-12-09 19:12:30,819 -> 1024
2017-12-09 19:12:41,996 -> 03.Joining datasets,11.18,125540,50.0,21
Done!!! Sat Dec  9 19:12:42 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 19:12:45,006 -> Starting session,1.85,0
2017-12-09 19:12:45,006 -> Setting variables,0.00,0
2017-12-09 19:12:51,068 -> Reading datasets,6.06,0
2017-12-09 19:12:51,087 -> Points partitions: 2
2017-12-09 19:12:51,098 -> Centers partitions: 2
2017-12-09 19:12:57,972 -> 01.Indexing points,6.82,19715,50.0,21
2017-12-09 19:13:02,941 -> 02.Indexing centers,4.97,125542,50.0,21
2017-12-09 19:13:02,952 -> 992
2017-12-09 19:13:02,960 -> 1024
2017-12-09 19:13:15,126 -> 03.Joining datasets,12.17,125540,50.0,21
Done!!! Sat Dec  9 19:13:15 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 19:13:18,016 -> Starting session,1.66,0
2017-12-09 19:13:18,017 -> Setting variables,0.00,0
2017-12-09 19:13:23,620 -> Reading datasets,5.60,0
2017-12-09 19:13:23,640 -> Points partitions: 2
2017-12-09 19:13:23,653 -> Centers partitions: 2
2017-12-09 19:13:29,161 -> 01.Indexing points,5.45,19715,50.0,21
2017-12-09 19:13:33,784 -> 02.Indexing centers,4.62,125542,50.0,21
2017-12-09 19:13:33,796 -> 992
2017-12-09 19:13:33,803 -> 1024
2017-12-09 19:13:43,504 -> 03.Joining datasets,9.70,125540,50.0,21
Done!!! Sat Dec  9 19:13:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 19:13:52,050 -> Starting session,1.85,0
2017-12-09 19:13:52,052 -> Setting variables,0.00,0
2017-12-09 19:13:58,782 -> Reading datasets,6.73,0
2017-12-09 19:13:58,793 -> Points partitions: 2
2017-12-09 19:13:58,802 -> Centers partitions: 2
2017-12-09 19:14:05,201 -> 01.Indexing points,6.36,19715,50.0,28
2017-12-09 19:14:10,776 -> 02.Indexing centers,5.57,125542,50.0,28
2017-12-09 19:14:10,789 -> 992
2017-12-09 19:14:10,798 -> 1024
2017-12-09 19:14:23,036 -> 03.Joining datasets,12.24,125540,50.0,28
Done!!! Sat Dec  9 19:14:23 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 19:14:25,901 -> Starting session,1.64,0
2017-12-09 19:14:25,901 -> Setting variables,0.00,0
2017-12-09 19:14:32,774 -> Reading datasets,6.87,0
2017-12-09 19:14:32,785 -> Points partitions: 2
2017-12-09 19:14:32,794 -> Centers partitions: 2
2017-12-09 19:14:41,059 -> 01.Indexing points,8.23,19715,50.0,28
2017-12-09 19:14:46,202 -> 02.Indexing centers,5.14,125542,50.0,28
2017-12-09 19:14:46,214 -> 992
2017-12-09 19:14:46,223 -> 1024
2017-12-09 19:14:56,325 -> 03.Joining datasets,10.10,125540,50.0,28
Done!!! Sat Dec  9 19:14:56 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 19:14:59,166 -> Starting session,1.55,0
2017-12-09 19:14:59,167 -> Setting variables,0.00,0
2017-12-09 19:15:05,351 -> Reading datasets,6.18,0
2017-12-09 19:15:05,370 -> Points partitions: 2
2017-12-09 19:15:05,380 -> Centers partitions: 2
2017-12-09 19:15:12,367 -> 01.Indexing points,6.94,19715,50.0,28
2017-12-09 19:15:17,773 -> 02.Indexing centers,5.40,125542,50.0,28
2017-12-09 19:15:17,781 -> 992
2017-12-09 19:15:17,787 -> 1024
2017-12-09 19:15:29,001 -> 03.Joining datasets,11.21,125540,50.0,28
Done!!! Sat Dec  9 19:15:29 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 19:15:31,787 -> Starting session,1.53,0
2017-12-09 19:15:31,788 -> Setting variables,0.00,0
2017-12-09 19:15:36,645 -> Reading datasets,4.86,0
2017-12-09 19:15:36,656 -> Points partitions: 2
2017-12-09 19:15:36,663 -> Centers partitions: 2
2017-12-09 19:15:46,537 -> 01.Indexing points,9.84,19715,50.0,28
2017-12-09 19:15:52,717 -> 02.Indexing centers,6.18,125542,50.0,28
2017-12-09 19:15:52,729 -> 992
2017-12-09 19:15:52,738 -> 1024
2017-12-09 19:16:04,557 -> 03.Joining datasets,11.82,125540,50.0,28
Done!!! Sat Dec  9 19:16:05 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 19:16:07,264 -> Starting session,1.57,0
2017-12-09 19:16:07,265 -> Setting variables,0.00,0
2017-12-09 19:16:13,384 -> Reading datasets,6.12,0
2017-12-09 19:16:13,410 -> Points partitions: 2
2017-12-09 19:16:13,425 -> Centers partitions: 2
2017-12-09 19:16:22,682 -> 01.Indexing points,9.21,19715,50.0,28
2017-12-09 19:16:28,193 -> 02.Indexing centers,5.51,125542,50.0,28
2017-12-09 19:16:28,203 -> 992
2017-12-09 19:16:28,210 -> 1024
2017-12-09 19:16:40,232 -> 03.Joining datasets,12.02,125540,50.0,28
Done!!! Sat Dec  9 19:16:40 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 19:16:48,467 -> Starting session,1.43,0
2017-12-09 19:16:48,467 -> Setting variables,0.00,0
2017-12-09 19:16:52,965 -> Reading datasets,4.50,0
2017-12-09 19:16:52,979 -> Points partitions: 2
2017-12-09 19:16:52,991 -> Centers partitions: 2
2017-12-09 19:17:00,447 -> 01.Indexing points,7.41,78857,40.0,7
2017-12-09 19:17:07,877 -> 02.Indexing centers,7.43,395352,40.0,7
2017-12-09 19:17:07,889 -> 1024
2017-12-09 19:17:07,897 -> 1024
2017-12-09 19:17:28,476 -> 03.Joining datasets,20.58,395346,40.0,7
Done!!! Sat Dec  9 19:17:28 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 19:17:31,116 -> Starting session,1.51,0
2017-12-09 19:17:31,117 -> Setting variables,0.00,0
2017-12-09 19:17:35,386 -> Reading datasets,4.27,0
2017-12-09 19:17:35,401 -> Points partitions: 2
2017-12-09 19:17:35,411 -> Centers partitions: 2
2017-12-09 19:17:42,836 -> 01.Indexing points,7.39,78857,40.0,7
2017-12-09 19:17:50,002 -> 02.Indexing centers,7.21,395352,40.0,7
2017-12-09 19:17:50,012 -> 1024
2017-12-09 19:17:50,020 -> 1024
2017-12-09 19:18:10,664 -> 03.Joining datasets,20.64,395346,40.0,7
Done!!! Sat Dec  9 19:18:11 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 19:18:13,384 -> Starting session,1.54,0
2017-12-09 19:18:13,384 -> Setting variables,0.00,0
2017-12-09 19:18:17,800 -> Reading datasets,4.41,0
2017-12-09 19:18:17,813 -> Points partitions: 2
2017-12-09 19:18:17,822 -> Centers partitions: 2
2017-12-09 19:18:25,200 -> 01.Indexing points,7.34,78857,40.0,7
2017-12-09 19:18:32,244 -> 02.Indexing centers,7.04,395352,40.0,7
2017-12-09 19:18:32,253 -> 1024
2017-12-09 19:18:32,259 -> 1024
2017-12-09 19:18:52,840 -> 03.Joining datasets,20.58,395346,40.0,7
Done!!! Sat Dec  9 19:18:53 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 19:18:55,705 -> Starting session,1.63,0
2017-12-09 19:18:55,705 -> Setting variables,0.00,0
2017-12-09 19:19:00,173 -> Reading datasets,4.47,0
2017-12-09 19:19:00,189 -> Points partitions: 2
2017-12-09 19:19:00,201 -> Centers partitions: 2
2017-12-09 19:19:08,075 -> 01.Indexing points,7.83,78857,40.0,7
2017-12-09 19:19:15,405 -> 02.Indexing centers,7.33,395352,40.0,7
2017-12-09 19:19:15,414 -> 1024
2017-12-09 19:19:15,421 -> 1024
2017-12-09 19:19:37,262 -> 03.Joining datasets,21.84,395346,40.0,7
Done!!! Sat Dec  9 19:19:37 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 19:19:39,986 -> Starting session,1.52,0
2017-12-09 19:19:39,987 -> Setting variables,0.00,0
2017-12-09 19:19:44,477 -> Reading datasets,4.49,0
2017-12-09 19:19:44,492 -> Points partitions: 2
2017-12-09 19:19:44,501 -> Centers partitions: 2
2017-12-09 19:19:52,284 -> 01.Indexing points,7.74,78857,40.0,7
2017-12-09 19:19:59,782 -> 02.Indexing centers,7.50,395352,40.0,7
2017-12-09 19:19:59,795 -> 1024
2017-12-09 19:19:59,804 -> 1024
2017-12-09 19:20:20,545 -> 03.Joining datasets,20.74,395346,40.0,7
Done!!! Sat Dec  9 19:20:21 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 19:20:28,842 -> Starting session,1.65,0
2017-12-09 19:20:28,843 -> Setting variables,0.00,0
2017-12-09 19:20:33,766 -> Reading datasets,4.92,0
2017-12-09 19:20:33,778 -> Points partitions: 2
2017-12-09 19:20:33,788 -> Centers partitions: 2
2017-12-09 19:20:41,427 -> 01.Indexing points,7.60,78857,40.0,14
2017-12-09 19:20:48,858 -> 02.Indexing centers,7.43,395352,40.0,14
2017-12-09 19:20:48,869 -> 1024
2017-12-09 19:20:48,876 -> 1024
2017-12-09 19:21:05,630 -> 03.Joining datasets,16.75,395346,40.0,14
Done!!! Sat Dec  9 19:21:06 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 19:21:08,501 -> Starting session,1.62,0
2017-12-09 19:21:08,502 -> Setting variables,0.00,0
2017-12-09 19:21:13,732 -> Reading datasets,5.23,0
2017-12-09 19:21:13,744 -> Points partitions: 2
2017-12-09 19:21:13,752 -> Centers partitions: 2
2017-12-09 19:21:21,509 -> 01.Indexing points,7.72,78857,40.0,14
2017-12-09 19:21:28,996 -> 02.Indexing centers,7.49,395352,40.0,14
2017-12-09 19:21:29,008 -> 1024
2017-12-09 19:21:29,017 -> 1024
2017-12-09 19:21:46,701 -> 03.Joining datasets,17.68,395346,40.0,14
Done!!! Sat Dec  9 19:21:47 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 19:21:49,635 -> Starting session,1.74,0
2017-12-09 19:21:49,635 -> Setting variables,0.00,0
2017-12-09 19:21:54,692 -> Reading datasets,5.06,0
2017-12-09 19:21:54,704 -> Points partitions: 2
2017-12-09 19:21:54,713 -> Centers partitions: 2
2017-12-09 19:22:02,423 -> 01.Indexing points,7.67,78857,40.0,14
2017-12-09 19:22:09,933 -> 02.Indexing centers,7.51,395352,40.0,14
2017-12-09 19:22:09,947 -> 1024
2017-12-09 19:22:09,960 -> 1024
2017-12-09 19:22:27,085 -> 03.Joining datasets,17.12,395346,40.0,14
Done!!! Sat Dec  9 19:22:27 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 19:22:29,940 -> Starting session,1.65,0
2017-12-09 19:22:29,940 -> Setting variables,0.00,0
2017-12-09 19:22:34,915 -> Reading datasets,4.97,0
2017-12-09 19:22:34,929 -> Points partitions: 2
2017-12-09 19:22:34,939 -> Centers partitions: 2
2017-12-09 19:22:42,483 -> 01.Indexing points,7.50,78857,40.0,14
2017-12-09 19:22:49,705 -> 02.Indexing centers,7.22,395352,40.0,14
2017-12-09 19:22:49,713 -> 1024
2017-12-09 19:22:49,719 -> 1024
2017-12-09 19:23:05,855 -> 03.Joining datasets,16.14,395346,40.0,14
Done!!! Sat Dec  9 19:23:06 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 19:23:08,683 -> Starting session,1.64,0
2017-12-09 19:23:08,684 -> Setting variables,0.00,0
2017-12-09 19:23:13,552 -> Reading datasets,4.87,0
2017-12-09 19:23:13,566 -> Points partitions: 2
2017-12-09 19:23:13,575 -> Centers partitions: 2
2017-12-09 19:23:21,403 -> 01.Indexing points,7.79,78857,40.0,14
2017-12-09 19:23:29,021 -> 02.Indexing centers,7.62,395352,40.0,14
2017-12-09 19:23:29,029 -> 1024
2017-12-09 19:23:29,037 -> 1024
2017-12-09 19:23:46,227 -> 03.Joining datasets,17.19,395346,40.0,14
Done!!! Sat Dec  9 19:23:46 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 19:23:54,665 -> Starting session,1.87,0
2017-12-09 19:23:54,666 -> Setting variables,0.00,0
2017-12-09 19:23:59,742 -> Reading datasets,5.08,0
2017-12-09 19:23:59,753 -> Points partitions: 2
2017-12-09 19:23:59,761 -> Centers partitions: 2
2017-12-09 19:24:07,570 -> 01.Indexing points,7.66,78857,40.0,21
2017-12-09 19:24:14,690 -> 02.Indexing centers,7.12,395352,40.0,21
2017-12-09 19:24:14,698 -> 1024
2017-12-09 19:24:14,704 -> 1024
2017-12-09 19:24:27,962 -> 03.Joining datasets,13.32,395346,40.0,21
Done!!! Sat Dec  9 19:24:28 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 19:24:30,631 -> Starting session,1.55,0
2017-12-09 19:24:30,632 -> Setting variables,0.00,0
2017-12-09 19:24:36,496 -> Reading datasets,5.86,0
2017-12-09 19:24:36,510 -> Points partitions: 2
2017-12-09 19:24:36,520 -> Centers partitions: 2
2017-12-09 19:24:43,325 -> 01.Indexing points,6.76,78857,40.0,21
2017-12-09 19:24:49,989 -> 02.Indexing centers,6.66,395352,40.0,21
2017-12-09 19:24:50,001 -> 1024
2017-12-09 19:24:50,010 -> 1024
2017-12-09 19:25:02,739 -> 03.Joining datasets,12.73,395346,40.0,21
Done!!! Sat Dec  9 19:25:03 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 19:25:05,681 -> Starting session,1.73,0
2017-12-09 19:25:05,681 -> Setting variables,0.00,0
2017-12-09 19:25:11,630 -> Reading datasets,5.95,0
2017-12-09 19:25:11,655 -> Points partitions: 2
2017-12-09 19:25:11,672 -> Centers partitions: 2
2017-12-09 19:25:18,438 -> 01.Indexing points,6.71,78857,40.0,21
2017-12-09 19:25:24,823 -> 02.Indexing centers,6.38,395352,40.0,21
2017-12-09 19:25:24,830 -> 1024
2017-12-09 19:25:24,836 -> 1024
2017-12-09 19:25:37,542 -> 03.Joining datasets,12.71,395346,40.0,21
Done!!! Sat Dec  9 19:25:38 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 19:25:40,390 -> Starting session,1.61,0
2017-12-09 19:25:40,391 -> Setting variables,0.00,0
2017-12-09 19:25:46,334 -> Reading datasets,6.02,0
2017-12-09 19:25:46,351 -> Points partitions: 2
2017-12-09 19:25:46,362 -> Centers partitions: 2
2017-12-09 19:25:53,348 -> 01.Indexing points,6.94,78857,40.0,21
2017-12-09 19:25:59,854 -> 02.Indexing centers,6.50,395352,40.0,21
2017-12-09 19:25:59,862 -> 1024
2017-12-09 19:25:59,868 -> 1024
2017-12-09 19:26:12,825 -> 03.Joining datasets,12.96,395346,40.0,21
Done!!! Sat Dec  9 19:26:13 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 19:26:15,741 -> Starting session,1.67,0
2017-12-09 19:26:15,742 -> Setting variables,0.00,0
2017-12-09 19:26:21,639 -> Reading datasets,5.90,0
2017-12-09 19:26:21,660 -> Points partitions: 2
2017-12-09 19:26:21,673 -> Centers partitions: 2
2017-12-09 19:26:29,294 -> 01.Indexing points,7.54,78857,40.0,21
2017-12-09 19:26:36,222 -> 02.Indexing centers,6.93,395352,40.0,21
2017-12-09 19:26:36,234 -> 1024
2017-12-09 19:26:36,243 -> 1024
2017-12-09 19:26:49,806 -> 03.Joining datasets,13.56,395346,40.0,21
Done!!! Sat Dec  9 19:26:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 19:26:59,117 -> Starting session,1.69,0
2017-12-09 19:26:59,117 -> Setting variables,0.00,0
2017-12-09 19:27:06,238 -> Reading datasets,7.12,0
2017-12-09 19:27:06,266 -> Points partitions: 2
2017-12-09 19:27:06,282 -> Centers partitions: 2
2017-12-09 19:27:14,100 -> 01.Indexing points,7.77,78857,40.0,28
2017-12-09 19:27:20,293 -> 02.Indexing centers,6.19,395352,40.0,28
2017-12-09 19:27:20,304 -> 1024
2017-12-09 19:27:20,313 -> 1024
2017-12-09 19:27:32,877 -> 03.Joining datasets,12.56,395346,40.0,28
Done!!! Sat Dec  9 19:27:33 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 19:27:35,802 -> Starting session,1.68,0
2017-12-09 19:27:35,802 -> Setting variables,0.00,0
2017-12-09 19:27:40,832 -> Reading datasets,5.03,0
2017-12-09 19:27:40,843 -> Points partitions: 2
2017-12-09 19:27:40,850 -> Centers partitions: 2
2017-12-09 19:27:51,282 -> 01.Indexing points,10.39,78857,40.0,28
2017-12-09 19:28:00,942 -> 02.Indexing centers,9.66,395352,40.0,28
2017-12-09 19:28:00,949 -> 1024
2017-12-09 19:28:00,955 -> 1024
2017-12-09 19:28:12,665 -> 03.Joining datasets,11.71,395346,40.0,28
Done!!! Sat Dec  9 19:28:13 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 19:28:15,435 -> Starting session,1.55,0
2017-12-09 19:28:15,438 -> Setting variables,0.00,0
2017-12-09 19:28:20,569 -> Reading datasets,5.13,0
2017-12-09 19:28:20,586 -> Points partitions: 2
2017-12-09 19:28:20,598 -> Centers partitions: 2
2017-12-09 19:28:30,860 -> 01.Indexing points,10.22,78857,40.0,28
2017-12-09 19:28:39,408 -> 02.Indexing centers,8.55,395352,40.0,28
2017-12-09 19:28:39,419 -> 1024
2017-12-09 19:28:39,426 -> 1024
2017-12-09 19:28:50,799 -> 03.Joining datasets,11.37,395346,40.0,28
Done!!! Sat Dec  9 19:28:51 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 19:28:53,565 -> Starting session,1.53,0
2017-12-09 19:28:53,565 -> Setting variables,0.00,0
2017-12-09 19:29:00,790 -> Reading datasets,7.22,0
2017-12-09 19:29:00,812 -> Points partitions: 2
2017-12-09 19:29:00,825 -> Centers partitions: 2
2017-12-09 19:29:08,508 -> 01.Indexing points,7.63,78857,40.0,28
2017-12-09 19:29:15,510 -> 02.Indexing centers,7.00,395352,40.0,28
2017-12-09 19:29:15,520 -> 1024
2017-12-09 19:29:15,527 -> 1024
2017-12-09 19:29:26,944 -> 03.Joining datasets,11.42,395346,40.0,28
Done!!! Sat Dec  9 19:29:27 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 19:29:29,653 -> Starting session,1.58,0
2017-12-09 19:29:29,654 -> Setting variables,0.00,0
2017-12-09 19:29:36,940 -> Reading datasets,7.29,0
2017-12-09 19:29:36,954 -> Points partitions: 2
2017-12-09 19:29:36,965 -> Centers partitions: 2
2017-12-09 19:29:45,254 -> 01.Indexing points,8.24,78857,40.0,28
2017-12-09 19:29:54,505 -> 02.Indexing centers,9.25,395352,40.0,28
2017-12-09 19:29:54,514 -> 1024
2017-12-09 19:29:54,520 -> 1024
2017-12-09 19:30:06,278 -> 03.Joining datasets,11.76,395346,40.0,28
Done!!! Sat Dec  9 19:30:06 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 19:30:14,789 -> Starting session,1.56,0
2017-12-09 19:30:14,790 -> Setting variables,0.00,0
2017-12-09 19:30:18,992 -> Reading datasets,4.20,0
2017-12-09 19:30:19,010 -> Points partitions: 2
2017-12-09 19:30:19,021 -> Centers partitions: 2
2017-12-09 19:30:26,131 -> 01.Indexing points,7.07,59143,40.0,7
2017-12-09 19:30:32,615 -> 02.Indexing centers,6.48,296514,40.0,7
2017-12-09 19:30:32,627 -> 1024
2017-12-09 19:30:32,635 -> 1024
2017-12-09 19:30:51,776 -> 03.Joining datasets,19.14,296510,40.0,7
Done!!! Sat Dec  9 19:30:52 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 19:30:54,500 -> Starting session,1.53,0
2017-12-09 19:30:54,501 -> Setting variables,0.00,0
2017-12-09 19:30:58,797 -> Reading datasets,4.30,0
2017-12-09 19:30:58,808 -> Points partitions: 2
2017-12-09 19:30:58,815 -> Centers partitions: 2
2017-12-09 19:31:06,153 -> 01.Indexing points,7.27,59143,40.0,7
2017-12-09 19:31:12,665 -> 02.Indexing centers,6.51,296514,40.0,7
2017-12-09 19:31:12,675 -> 1024
2017-12-09 19:31:12,682 -> 1024
2017-12-09 19:31:31,862 -> 03.Joining datasets,19.18,296510,40.0,7
Done!!! Sat Dec  9 19:31:32 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 19:31:34,576 -> Starting session,1.57,0
2017-12-09 19:31:34,576 -> Setting variables,0.00,0
2017-12-09 19:31:38,884 -> Reading datasets,4.31,0
2017-12-09 19:31:38,895 -> Points partitions: 2
2017-12-09 19:31:38,902 -> Centers partitions: 2
2017-12-09 19:31:46,291 -> 01.Indexing points,7.35,59143,40.0,7
2017-12-09 19:31:52,629 -> 02.Indexing centers,6.34,296514,40.0,7
2017-12-09 19:31:52,642 -> 1024
2017-12-09 19:31:52,651 -> 1024
2017-12-09 19:32:12,417 -> 03.Joining datasets,19.77,296510,40.0,7
Done!!! Sat Dec  9 19:32:12 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 19:32:15,038 -> Starting session,1.51,0
2017-12-09 19:32:15,039 -> Setting variables,0.00,0
2017-12-09 19:32:19,352 -> Reading datasets,4.31,0
2017-12-09 19:32:19,364 -> Points partitions: 2
2017-12-09 19:32:19,371 -> Centers partitions: 2
2017-12-09 19:32:26,505 -> 01.Indexing points,7.10,59143,40.0,7
2017-12-09 19:32:33,146 -> 02.Indexing centers,6.64,296514,40.0,7
2017-12-09 19:32:33,158 -> 1024
2017-12-09 19:32:33,167 -> 1024
2017-12-09 19:32:52,364 -> 03.Joining datasets,19.31,296510,40.0,7
Done!!! Sat Dec  9 19:32:52 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 19:32:55,118 -> Starting session,1.56,0
2017-12-09 19:32:55,118 -> Setting variables,0.00,0
2017-12-09 19:32:59,397 -> Reading datasets,4.28,0
2017-12-09 19:32:59,414 -> Points partitions: 2
2017-12-09 19:32:59,426 -> Centers partitions: 2
2017-12-09 19:33:06,634 -> 01.Indexing points,7.17,59143,40.0,7
2017-12-09 19:33:13,185 -> 02.Indexing centers,6.55,296514,40.0,7
2017-12-09 19:33:13,196 -> 1024
2017-12-09 19:33:13,203 -> 1024
2017-12-09 19:33:32,307 -> 03.Joining datasets,19.10,296510,40.0,7
Done!!! Sat Dec  9 19:33:32 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 19:33:40,688 -> Starting session,1.64,0
2017-12-09 19:33:40,689 -> Setting variables,0.00,0
2017-12-09 19:33:45,614 -> Reading datasets,4.92,0
2017-12-09 19:33:45,625 -> Points partitions: 2
2017-12-09 19:33:45,633 -> Centers partitions: 2
2017-12-09 19:33:52,707 -> 01.Indexing points,7.03,59143,40.0,14
2017-12-09 19:33:59,212 -> 02.Indexing centers,6.50,296514,40.0,14
2017-12-09 19:33:59,223 -> 1024
2017-12-09 19:33:59,233 -> 1024
2017-12-09 19:34:15,751 -> 03.Joining datasets,16.52,296510,40.0,14
Done!!! Sat Dec  9 19:34:16 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 19:34:18,569 -> Starting session,1.60,0
2017-12-09 19:34:18,569 -> Setting variables,0.00,0
2017-12-09 19:34:23,595 -> Reading datasets,5.03,0
2017-12-09 19:34:23,607 -> Points partitions: 2
2017-12-09 19:34:23,615 -> Centers partitions: 2
2017-12-09 19:34:31,042 -> 01.Indexing points,7.39,59143,40.0,14
2017-12-09 19:34:37,742 -> 02.Indexing centers,6.70,296514,40.0,14
2017-12-09 19:34:37,750 -> 1024
2017-12-09 19:34:37,757 -> 1024
2017-12-09 19:34:54,225 -> 03.Joining datasets,16.47,296510,40.0,14
Done!!! Sat Dec  9 19:34:54 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 19:34:56,998 -> Starting session,1.66,0
2017-12-09 19:34:56,998 -> Setting variables,0.00,0
2017-12-09 19:35:02,042 -> Reading datasets,5.04,0
2017-12-09 19:35:02,053 -> Points partitions: 2
2017-12-09 19:35:02,060 -> Centers partitions: 2
2017-12-09 19:35:09,559 -> 01.Indexing points,7.46,59143,40.0,14
2017-12-09 19:35:15,927 -> 02.Indexing centers,6.37,296514,40.0,14
2017-12-09 19:35:15,935 -> 1024
2017-12-09 19:35:15,942 -> 1024
2017-12-09 19:35:31,608 -> 03.Joining datasets,15.67,296510,40.0,14
Done!!! Sat Dec  9 19:35:32 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 19:35:35,168 -> Starting session,1.55,0
2017-12-09 19:35:35,168 -> Setting variables,0.00,0
2017-12-09 19:35:40,045 -> Reading datasets,4.88,0
2017-12-09 19:35:40,055 -> Points partitions: 2
2017-12-09 19:35:40,062 -> Centers partitions: 2
2017-12-09 19:35:47,219 -> 01.Indexing points,7.12,59143,40.0,14
2017-12-09 19:35:53,529 -> 02.Indexing centers,6.31,296514,40.0,14
2017-12-09 19:35:53,540 -> 1024
2017-12-09 19:35:53,547 -> 1024
2017-12-09 19:36:08,889 -> 03.Joining datasets,15.88,296510,40.0,14
Done!!! Sat Dec  9 19:36:09 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 19:36:11,805 -> Starting session,1.64,0
2017-12-09 19:36:11,806 -> Setting variables,0.00,0
2017-12-09 19:36:16,607 -> Reading datasets,4.80,0
2017-12-09 19:36:16,619 -> Points partitions: 2
2017-12-09 19:36:16,626 -> Centers partitions: 2
2017-12-09 19:36:23,586 -> 01.Indexing points,6.92,59143,40.0,14
2017-12-09 19:36:29,719 -> 02.Indexing centers,6.13,296514,40.0,14
2017-12-09 19:36:29,727 -> 1024
2017-12-09 19:36:29,733 -> 1024
2017-12-09 19:36:45,199 -> 03.Joining datasets,15.47,296510,40.0,14
Done!!! Sat Dec  9 19:36:46 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 19:36:54,695 -> Starting session,1.92,0
2017-12-09 19:36:54,695 -> Setting variables,0.00,0
2017-12-09 19:37:00,581 -> Reading datasets,5.89,0
2017-12-09 19:37:00,594 -> Points partitions: 2
2017-12-09 19:37:00,603 -> Centers partitions: 2
2017-12-09 19:37:07,282 -> 01.Indexing points,6.64,59143,40.0,21
2017-12-09 19:37:13,197 -> 02.Indexing centers,5.91,296514,40.0,21
2017-12-09 19:37:13,208 -> 1024
2017-12-09 19:37:13,218 -> 1024
2017-12-09 19:37:26,281 -> 03.Joining datasets,13.06,296510,40.0,21
Done!!! Sat Dec  9 19:37:27 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 19:37:29,980 -> Starting session,1.64,0
2017-12-09 19:37:29,980 -> Setting variables,0.00,0
2017-12-09 19:37:35,977 -> Reading datasets,6.00,0
2017-12-09 19:37:36,000 -> Points partitions: 2
2017-12-09 19:37:36,017 -> Centers partitions: 2
2017-12-09 19:37:42,932 -> 01.Indexing points,6.86,59143,40.0,21
2017-12-09 19:37:48,704 -> 02.Indexing centers,5.77,296514,40.0,21
2017-12-09 19:37:48,711 -> 1024
2017-12-09 19:37:48,717 -> 1024
2017-12-09 19:38:00,883 -> 03.Joining datasets,12.17,296510,40.0,21
Done!!! Sat Dec  9 19:38:01 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 19:38:03,729 -> Starting session,1.62,0
2017-12-09 19:38:03,730 -> Setting variables,0.00,0
2017-12-09 19:38:09,608 -> Reading datasets,5.88,0
2017-12-09 19:38:09,619 -> Points partitions: 2
2017-12-09 19:38:09,626 -> Centers partitions: 2
2017-12-09 19:38:16,684 -> 01.Indexing points,7.02,59143,40.0,21
2017-12-09 19:38:22,648 -> 02.Indexing centers,5.96,296514,40.0,21
2017-12-09 19:38:22,655 -> 1024
2017-12-09 19:38:22,661 -> 1024
2017-12-09 19:38:34,640 -> 03.Joining datasets,11.98,296510,40.0,21
Done!!! Sat Dec  9 19:38:35 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 19:38:37,482 -> Starting session,1.63,0
2017-12-09 19:38:37,483 -> Setting variables,0.00,0
2017-12-09 19:38:43,384 -> Reading datasets,5.90,0
2017-12-09 19:38:43,399 -> Points partitions: 2
2017-12-09 19:38:43,412 -> Centers partitions: 2
2017-12-09 19:38:49,717 -> 01.Indexing points,6.26,59143,40.0,21
2017-12-09 19:38:55,398 -> 02.Indexing centers,5.68,296514,40.0,21
2017-12-09 19:38:55,408 -> 1024
2017-12-09 19:38:55,419 -> 1024
2017-12-09 19:39:07,730 -> 03.Joining datasets,12.31,296510,40.0,21
Done!!! Sat Dec  9 19:39:09 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 19:39:11,244 -> Starting session,1.59,0
2017-12-09 19:39:11,245 -> Setting variables,0.00,0
2017-12-09 19:39:16,161 -> Reading datasets,4.92,0
2017-12-09 19:39:16,174 -> Points partitions: 2
2017-12-09 19:39:16,185 -> Centers partitions: 2
2017-12-09 19:39:24,249 -> 01.Indexing points,8.03,59143,40.0,21
2017-12-09 19:39:30,021 -> 02.Indexing centers,5.77,296514,40.0,21
2017-12-09 19:39:30,028 -> 1024
2017-12-09 19:39:30,034 -> 1024
2017-12-09 19:39:42,068 -> 03.Joining datasets,12.03,296510,40.0,21
Done!!! Sat Dec  9 19:39:43 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 19:39:51,310 -> Starting session,1.67,0
2017-12-09 19:39:51,310 -> Setting variables,0.00,0
2017-12-09 19:39:58,411 -> Reading datasets,7.10,0
2017-12-09 19:39:58,429 -> Points partitions: 2
2017-12-09 19:39:58,440 -> Centers partitions: 2
2017-12-09 19:40:06,042 -> 01.Indexing points,7.55,59143,40.0,28
2017-12-09 19:40:14,126 -> 02.Indexing centers,8.08,296514,40.0,28
2017-12-09 19:40:14,134 -> 1024
2017-12-09 19:40:14,140 -> 1024
2017-12-09 19:40:24,760 -> 03.Joining datasets,10.62,296510,40.0,28
Done!!! Sat Dec  9 19:40:25 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 19:40:27,435 -> Starting session,1.55,0
2017-12-09 19:40:27,435 -> Setting variables,0.00,0
2017-12-09 19:40:33,213 -> Reading datasets,5.78,0
2017-12-09 19:40:33,227 -> Points partitions: 2
2017-12-09 19:40:33,239 -> Centers partitions: 2
2017-12-09 19:40:42,789 -> 01.Indexing points,9.51,59143,40.0,28
2017-12-09 19:40:49,508 -> 02.Indexing centers,6.72,296514,40.0,28
2017-12-09 19:40:49,518 -> 1024
2017-12-09 19:40:49,526 -> 1024
2017-12-09 19:41:00,514 -> 03.Joining datasets,10.99,296510,40.0,28
Done!!! Sat Dec  9 19:41:01 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 19:41:03,172 -> Starting session,1.56,0
2017-12-09 19:41:03,173 -> Setting variables,0.00,0
2017-12-09 19:41:10,348 -> Reading datasets,7.18,0
2017-12-09 19:41:10,369 -> Points partitions: 2
2017-12-09 19:41:10,381 -> Centers partitions: 2
2017-12-09 19:41:18,343 -> 01.Indexing points,7.91,59143,40.0,28
2017-12-09 19:41:25,840 -> 02.Indexing centers,7.50,296514,40.0,28
2017-12-09 19:41:25,853 -> 1024
2017-12-09 19:41:25,861 -> 1024
2017-12-09 19:41:36,680 -> 03.Joining datasets,10.82,296510,40.0,28
Done!!! Sat Dec  9 19:41:37 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 19:41:40,249 -> Starting session,1.53,0
2017-12-09 19:41:40,249 -> Setting variables,0.00,0
2017-12-09 19:41:47,371 -> Reading datasets,7.12,0
2017-12-09 19:41:47,383 -> Points partitions: 2
2017-12-09 19:41:47,393 -> Centers partitions: 2
2017-12-09 19:41:55,143 -> 01.Indexing points,7.70,59143,40.0,28
2017-12-09 19:42:03,115 -> 02.Indexing centers,7.97,296514,40.0,28
2017-12-09 19:42:03,125 -> 1024
2017-12-09 19:42:03,133 -> 1024
2017-12-09 19:42:14,450 -> 03.Joining datasets,11.32,296510,40.0,28
Done!!! Sat Dec  9 19:42:15 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 19:42:18,167 -> Starting session,1.62,0
2017-12-09 19:42:18,168 -> Setting variables,0.00,0
2017-12-09 19:42:25,290 -> Reading datasets,7.12,0
2017-12-09 19:42:25,308 -> Points partitions: 2
2017-12-09 19:42:25,319 -> Centers partitions: 2
2017-12-09 19:42:32,524 -> 01.Indexing points,7.15,59143,40.0,28
2017-12-09 19:42:38,675 -> 02.Indexing centers,6.15,296514,40.0,28
2017-12-09 19:42:38,686 -> 1024
2017-12-09 19:42:38,694 -> 1024
2017-12-09 19:42:50,672 -> 03.Joining datasets,11.98,296510,40.0,28
Done!!! Sat Dec  9 19:42:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 19:42:59,831 -> Starting session,1.51,0
2017-12-09 19:42:59,831 -> Setting variables,0.00,0
2017-12-09 19:43:04,109 -> Reading datasets,4.28,0
2017-12-09 19:43:04,125 -> Points partitions: 2
2017-12-09 19:43:04,135 -> Centers partitions: 2
2017-12-09 19:43:11,531 -> 01.Indexing points,7.36,39429,40.0,7
2017-12-09 19:43:17,282 -> 02.Indexing centers,5.75,197676,40.0,7
2017-12-09 19:43:17,296 -> 1024
2017-12-09 19:43:17,306 -> 1024
2017-12-09 19:43:35,120 -> 03.Joining datasets,17.81,197673,40.0,7
Done!!! Sat Dec  9 19:43:35 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 19:43:37,888 -> Starting session,1.57,0
2017-12-09 19:43:37,888 -> Setting variables,0.00,0
2017-12-09 19:43:42,260 -> Reading datasets,4.37,0
2017-12-09 19:43:42,276 -> Points partitions: 2
2017-12-09 19:43:42,286 -> Centers partitions: 2
2017-12-09 19:43:49,909 -> 01.Indexing points,7.58,39429,40.0,7
2017-12-09 19:43:55,565 -> 02.Indexing centers,5.66,197676,40.0,7
2017-12-09 19:43:55,576 -> 1024
2017-12-09 19:43:55,584 -> 1024
2017-12-09 19:44:12,920 -> 03.Joining datasets,17.34,197673,40.0,7
Done!!! Sat Dec  9 19:44:13 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 19:44:15,686 -> Starting session,1.56,0
2017-12-09 19:44:15,686 -> Setting variables,0.00,0
2017-12-09 19:44:20,013 -> Reading datasets,4.33,0
2017-12-09 19:44:20,029 -> Points partitions: 2
2017-12-09 19:44:20,038 -> Centers partitions: 2
2017-12-09 19:44:27,248 -> 01.Indexing points,7.17,39429,40.0,7
2017-12-09 19:44:33,402 -> 02.Indexing centers,6.15,197676,40.0,7
2017-12-09 19:44:33,416 -> 1024
2017-12-09 19:44:33,426 -> 1024
2017-12-09 19:44:52,172 -> 03.Joining datasets,17.86,197673,40.0,7
Done!!! Sat Dec  9 19:44:52 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 19:44:54,965 -> Starting session,1.57,0
2017-12-09 19:44:54,966 -> Setting variables,0.00,0
2017-12-09 19:44:59,327 -> Reading datasets,4.36,0
2017-12-09 19:44:59,338 -> Points partitions: 2
2017-12-09 19:44:59,346 -> Centers partitions: 2
2017-12-09 19:45:06,305 -> 01.Indexing points,6.92,39429,40.0,7
2017-12-09 19:45:12,117 -> 02.Indexing centers,5.81,197676,40.0,7
2017-12-09 19:45:12,132 -> 1024
2017-12-09 19:45:12,143 -> 1024
2017-12-09 19:45:29,545 -> 03.Joining datasets,17.44,197673,40.0,7
Done!!! Sat Dec  9 19:45:30 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 19:45:32,297 -> Starting session,1.55,0
2017-12-09 19:45:32,298 -> Setting variables,0.00,0
2017-12-09 19:45:36,547 -> Reading datasets,4.25,0
2017-12-09 19:45:36,558 -> Points partitions: 2
2017-12-09 19:45:36,565 -> Centers partitions: 2
2017-12-09 19:45:43,693 -> 01.Indexing points,7.09,39429,40.0,7
2017-12-09 19:45:49,417 -> 02.Indexing centers,5.72,197676,40.0,7
2017-12-09 19:45:49,430 -> 1024
2017-12-09 19:45:49,439 -> 1024
2017-12-09 19:46:06,465 -> 03.Joining datasets,17.03,197673,40.0,7
Done!!! Sat Dec  9 19:46:06 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 19:46:14,901 -> Starting session,1.67,0
2017-12-09 19:46:14,903 -> Setting variables,0.00,0
2017-12-09 19:46:19,763 -> Reading datasets,4.86,0
2017-12-09 19:46:19,774 -> Points partitions: 2
2017-12-09 19:46:19,782 -> Centers partitions: 2
2017-12-09 19:46:26,710 -> 01.Indexing points,6.89,39429,40.0,14
2017-12-09 19:46:32,490 -> 02.Indexing centers,5.78,197676,40.0,14
2017-12-09 19:46:32,499 -> 1024
2017-12-09 19:46:32,507 -> 1024
2017-12-09 19:46:47,061 -> 03.Joining datasets,14.55,197673,40.0,14
Done!!! Sat Dec  9 19:46:47 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 19:46:49,881 -> Starting session,1.60,0
2017-12-09 19:46:49,882 -> Setting variables,0.00,0
2017-12-09 19:46:54,723 -> Reading datasets,4.84,0
2017-12-09 19:46:54,736 -> Points partitions: 2
2017-12-09 19:46:54,747 -> Centers partitions: 2
2017-12-09 19:47:01,706 -> 01.Indexing points,6.92,39429,40.0,14
2017-12-09 19:47:07,755 -> 02.Indexing centers,6.05,197676,40.0,14
2017-12-09 19:47:07,768 -> 1024
2017-12-09 19:47:07,779 -> 1024
2017-12-09 19:47:21,629 -> 03.Joining datasets,13.85,197673,40.0,14
Done!!! Sat Dec  9 19:47:22 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 19:47:24,555 -> Starting session,1.68,0
2017-12-09 19:47:24,556 -> Setting variables,0.00,0
2017-12-09 19:47:29,344 -> Reading datasets,4.79,0
2017-12-09 19:47:29,358 -> Points partitions: 2
2017-12-09 19:47:29,369 -> Centers partitions: 2
2017-12-09 19:47:36,505 -> 01.Indexing points,7.08,39429,40.0,14
2017-12-09 19:47:42,274 -> 02.Indexing centers,5.77,197676,40.0,14
2017-12-09 19:47:42,282 -> 1024
2017-12-09 19:47:42,289 -> 1024
2017-12-09 19:47:56,885 -> 03.Joining datasets,14.60,197673,40.0,14
Done!!! Sat Dec  9 19:47:57 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 19:47:59,893 -> Starting session,1.63,0
2017-12-09 19:47:59,893 -> Setting variables,0.00,0
2017-12-09 19:48:04,863 -> Reading datasets,4.97,0
2017-12-09 19:48:04,873 -> Points partitions: 2
2017-12-09 19:48:04,880 -> Centers partitions: 2
2017-12-09 19:48:11,954 -> 01.Indexing points,7.03,39429,40.0,14
2017-12-09 19:48:17,935 -> 02.Indexing centers,5.98,197676,40.0,14
2017-12-09 19:48:17,943 -> 1024
2017-12-09 19:48:17,949 -> 1024
2017-12-09 19:48:33,211 -> 03.Joining datasets,15.26,197673,40.0,14
Done!!! Sat Dec  9 19:48:33 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 19:48:36,087 -> Starting session,1.67,0
2017-12-09 19:48:36,087 -> Setting variables,0.00,0
2017-12-09 19:48:40,664 -> Reading datasets,4.58,0
2017-12-09 19:48:40,675 -> Points partitions: 2
2017-12-09 19:48:40,683 -> Centers partitions: 2
2017-12-09 19:48:47,611 -> 01.Indexing points,6.89,39429,40.0,14
2017-12-09 19:48:53,274 -> 02.Indexing centers,5.66,197676,40.0,14
2017-12-09 19:48:53,284 -> 1024
2017-12-09 19:48:53,290 -> 1024
2017-12-09 19:49:07,638 -> 03.Joining datasets,14.35,197673,40.0,14
Done!!! Sat Dec  9 19:49:08 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 19:49:16,815 -> Starting session,1.78,0
2017-12-09 19:49:16,817 -> Setting variables,0.00,0
2017-12-09 19:49:22,570 -> Reading datasets,5.75,0
2017-12-09 19:49:22,588 -> Points partitions: 2
2017-12-09 19:49:22,603 -> Centers partitions: 2
2017-12-09 19:49:29,279 -> 01.Indexing points,6.63,39429,40.0,21
2017-12-09 19:49:34,390 -> 02.Indexing centers,5.11,197676,40.0,21
2017-12-09 19:49:34,399 -> 1024
2017-12-09 19:49:34,406 -> 1024
2017-12-09 19:49:45,721 -> 03.Joining datasets,11.31,197673,40.0,21
Done!!! Sat Dec  9 19:49:46 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 19:49:48,579 -> Starting session,1.62,0
2017-12-09 19:49:48,580 -> Setting variables,0.00,0
2017-12-09 19:49:54,393 -> Reading datasets,5.81,0
2017-12-09 19:49:54,408 -> Points partitions: 2
2017-12-09 19:49:54,417 -> Centers partitions: 2
2017-12-09 19:50:00,581 -> 01.Indexing points,6.12,39429,40.0,21
2017-12-09 19:50:05,966 -> 02.Indexing centers,5.38,197676,40.0,21
2017-12-09 19:50:05,974 -> 1024
2017-12-09 19:50:05,980 -> 1024
2017-12-09 19:50:17,047 -> 03.Joining datasets,11.07,197673,40.0,21
Done!!! Sat Dec  9 19:50:17 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 19:50:19,912 -> Starting session,1.62,0
2017-12-09 19:50:19,912 -> Setting variables,0.00,0
2017-12-09 19:50:25,633 -> Reading datasets,5.72,0
2017-12-09 19:50:25,655 -> Points partitions: 2
2017-12-09 19:50:25,672 -> Centers partitions: 2
2017-12-09 19:50:31,848 -> 01.Indexing points,6.12,39429,40.0,21
2017-12-09 19:50:36,826 -> 02.Indexing centers,4.98,197676,40.0,21
2017-12-09 19:50:36,835 -> 1024
2017-12-09 19:50:36,842 -> 1024
2017-12-09 19:50:49,235 -> 03.Joining datasets,12.39,197673,40.0,21
Done!!! Sat Dec  9 19:50:49 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 19:50:52,047 -> Starting session,1.58,0
2017-12-09 19:50:52,047 -> Setting variables,0.00,0
2017-12-09 19:50:57,812 -> Reading datasets,5.77,0
2017-12-09 19:50:57,827 -> Points partitions: 2
2017-12-09 19:50:57,836 -> Centers partitions: 2
2017-12-09 19:51:04,399 -> 01.Indexing points,6.52,39429,40.0,21
2017-12-09 19:51:09,571 -> 02.Indexing centers,5.17,197676,40.0,21
2017-12-09 19:51:09,582 -> 1024
2017-12-09 19:51:09,590 -> 1024
2017-12-09 19:51:20,695 -> 03.Joining datasets,11.10,197673,40.0,21
Done!!! Sat Dec  9 19:51:21 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 19:51:23,611 -> Starting session,1.81,0
2017-12-09 19:51:23,612 -> Setting variables,0.00,0
2017-12-09 19:51:29,406 -> Reading datasets,5.79,0
2017-12-09 19:51:29,422 -> Points partitions: 2
2017-12-09 19:51:29,432 -> Centers partitions: 2
2017-12-09 19:51:35,193 -> 01.Indexing points,5.80,39429,40.0,21
2017-12-09 19:51:40,378 -> 02.Indexing centers,5.18,197676,40.0,21
2017-12-09 19:51:40,386 -> 1024
2017-12-09 19:51:40,392 -> 1024
2017-12-09 19:51:51,164 -> 03.Joining datasets,10.77,197673,40.0,21
Done!!! Sat Dec  9 19:51:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 19:51:59,825 -> Starting session,1.81,0
2017-12-09 19:51:59,825 -> Setting variables,0.00,0
2017-12-09 19:52:06,934 -> Reading datasets,7.11,0
2017-12-09 19:52:06,945 -> Points partitions: 2
2017-12-09 19:52:06,952 -> Centers partitions: 2
2017-12-09 19:52:14,636 -> 01.Indexing points,7.65,39429,40.0,28
2017-12-09 19:52:19,564 -> 02.Indexing centers,4.93,197676,40.0,28
2017-12-09 19:52:19,573 -> 1024
2017-12-09 19:52:19,579 -> 1024
2017-12-09 19:52:29,855 -> 03.Joining datasets,10.28,197673,40.0,28
Done!!! Sat Dec  9 19:52:30 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 19:52:32,647 -> Starting session,1.67,0
2017-12-09 19:52:32,648 -> Setting variables,0.00,0
2017-12-09 19:52:39,414 -> Reading datasets,6.77,0
2017-12-09 19:52:39,433 -> Points partitions: 2
2017-12-09 19:52:39,446 -> Centers partitions: 2
2017-12-09 19:52:47,075 -> 01.Indexing points,7.57,39429,40.0,28
2017-12-09 19:52:52,763 -> 02.Indexing centers,5.69,197676,40.0,28
2017-12-09 19:52:52,773 -> 1024
2017-12-09 19:52:52,783 -> 1024
2017-12-09 19:53:03,439 -> 03.Joining datasets,10.66,197673,40.0,28
Done!!! Sat Dec  9 19:53:03 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 19:53:06,143 -> Starting session,1.59,0
2017-12-09 19:53:06,144 -> Setting variables,0.00,0
2017-12-09 19:53:13,237 -> Reading datasets,7.09,0
2017-12-09 19:53:13,256 -> Points partitions: 2
2017-12-09 19:53:13,267 -> Centers partitions: 2
2017-12-09 19:53:20,554 -> 01.Indexing points,7.22,39429,40.0,28
2017-12-09 19:53:26,169 -> 02.Indexing centers,5.61,197676,40.0,28
2017-12-09 19:53:26,176 -> 1024
2017-12-09 19:53:26,182 -> 1024
2017-12-09 19:53:35,966 -> 03.Joining datasets,9.78,197673,40.0,28
Done!!! Sat Dec  9 19:53:37 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 19:53:39,540 -> Starting session,1.68,0
2017-12-09 19:53:39,540 -> Setting variables,0.00,0
2017-12-09 19:53:45,366 -> Reading datasets,5.83,0
2017-12-09 19:53:45,377 -> Points partitions: 2
2017-12-09 19:53:45,385 -> Centers partitions: 2
2017-12-09 19:53:53,739 -> 01.Indexing points,8.32,39429,40.0,28
2017-12-09 19:53:58,586 -> 02.Indexing centers,4.85,197676,40.0,28
2017-12-09 19:53:58,595 -> 1024
2017-12-09 19:53:58,601 -> 1024
2017-12-09 19:54:08,491 -> 03.Joining datasets,9.89,197673,40.0,28
Done!!! Sat Dec  9 19:54:08 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 19:54:11,209 -> Starting session,1.50,0
2017-12-09 19:54:11,209 -> Setting variables,0.00,0
2017-12-09 19:54:18,074 -> Reading datasets,6.86,0
2017-12-09 19:54:18,093 -> Points partitions: 2
2017-12-09 19:54:18,104 -> Centers partitions: 2
2017-12-09 19:54:25,208 -> 01.Indexing points,7.06,39429,40.0,28
2017-12-09 19:54:31,528 -> 02.Indexing centers,6.32,197676,40.0,28
2017-12-09 19:54:31,537 -> 1024
2017-12-09 19:54:31,544 -> 1024
2017-12-09 19:54:41,266 -> 03.Joining datasets,9.72,197673,40.0,28
Done!!! Sat Dec  9 19:54:41 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 19:54:49,615 -> Starting session,1.44,0
2017-12-09 19:54:49,616 -> Setting variables,0.00,0
2017-12-09 19:54:53,978 -> Reading datasets,4.36,0
2017-12-09 19:54:53,989 -> Points partitions: 2
2017-12-09 19:54:53,996 -> Centers partitions: 2
2017-12-09 19:55:00,623 -> 01.Indexing points,6.59,19715,40.0,7
2017-12-09 19:55:05,819 -> 02.Indexing centers,5.19,98838,40.0,7
2017-12-09 19:55:05,832 -> 992
2017-12-09 19:55:05,841 -> 1024
2017-12-09 19:55:20,533 -> 03.Joining datasets,14.77,98837,40.0,7
Done!!! Sat Dec  9 19:55:21 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 19:55:23,313 -> Starting session,1.57,0
2017-12-09 19:55:23,370 -> Setting variables,0.00,0
2017-12-09 19:55:27,532 -> Reading datasets,4.16,0
2017-12-09 19:55:27,546 -> Points partitions: 2
2017-12-09 19:55:27,555 -> Centers partitions: 2
2017-12-09 19:55:34,293 -> 01.Indexing points,6.70,19715,40.0,7
2017-12-09 19:55:39,652 -> 02.Indexing centers,5.36,98838,40.0,7
2017-12-09 19:55:39,664 -> 992
2017-12-09 19:55:39,672 -> 1024
2017-12-09 19:55:54,381 -> 03.Joining datasets,14.71,98837,40.0,7
Done!!! Sat Dec  9 19:55:54 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 19:55:57,133 -> Starting session,1.58,0
2017-12-09 19:55:57,133 -> Setting variables,0.00,0
2017-12-09 19:56:01,216 -> Reading datasets,4.08,0
2017-12-09 19:56:01,227 -> Points partitions: 2
2017-12-09 19:56:01,235 -> Centers partitions: 2
2017-12-09 19:56:07,921 -> 01.Indexing points,6.65,19715,40.0,7
2017-12-09 19:56:13,357 -> 02.Indexing centers,5.43,98838,40.0,7
2017-12-09 19:56:13,370 -> 992
2017-12-09 19:56:13,379 -> 1024
2017-12-09 19:56:28,526 -> 03.Joining datasets,15.15,98837,40.0,7
Done!!! Sat Dec  9 19:56:28 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 19:56:31,205 -> Starting session,1.52,0
2017-12-09 19:56:31,205 -> Setting variables,0.00,0
2017-12-09 19:56:35,306 -> Reading datasets,4.10,0
2017-12-09 19:56:35,320 -> Points partitions: 2
2017-12-09 19:56:35,330 -> Centers partitions: 2
2017-12-09 19:56:41,773 -> 01.Indexing points,6.40,19715,40.0,7
2017-12-09 19:56:46,627 -> 02.Indexing centers,4.85,98838,40.0,7
2017-12-09 19:56:46,635 -> 992
2017-12-09 19:56:46,642 -> 1024
2017-12-09 19:57:01,351 -> 03.Joining datasets,14.71,98837,40.0,7
Done!!! Sat Dec  9 19:57:01 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 19:57:04,096 -> Starting session,1.64,0
2017-12-09 19:57:04,097 -> Setting variables,0.00,0
2017-12-09 19:57:08,254 -> Reading datasets,4.16,0
2017-12-09 19:57:08,265 -> Points partitions: 2
2017-12-09 19:57:08,272 -> Centers partitions: 2
2017-12-09 19:57:14,878 -> 01.Indexing points,6.57,19715,40.0,7
2017-12-09 19:57:19,756 -> 02.Indexing centers,4.88,98838,40.0,7
2017-12-09 19:57:19,766 -> 992
2017-12-09 19:57:19,775 -> 1024
2017-12-09 19:57:34,407 -> 03.Joining datasets,14.63,98837,40.0,7
Done!!! Sat Dec  9 19:57:34 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 19:57:42,864 -> Starting session,1.72,0
2017-12-09 19:57:42,864 -> Setting variables,0.00,0
2017-12-09 19:57:47,627 -> Reading datasets,4.76,0
2017-12-09 19:57:47,638 -> Points partitions: 2
2017-12-09 19:57:47,645 -> Centers partitions: 2
2017-12-09 19:57:54,545 -> 01.Indexing points,6.86,19715,40.0,14
2017-12-09 19:57:59,663 -> 02.Indexing centers,5.12,98838,40.0,14
2017-12-09 19:57:59,672 -> 992
2017-12-09 19:57:59,679 -> 1024
2017-12-09 19:58:12,594 -> 03.Joining datasets,12.91,98837,40.0,14
Done!!! Sat Dec  9 19:58:13 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 19:58:15,436 -> Starting session,1.60,0
2017-12-09 19:58:15,436 -> Setting variables,0.00,0
2017-12-09 19:58:20,349 -> Reading datasets,4.91,0
2017-12-09 19:58:20,362 -> Points partitions: 2
2017-12-09 19:58:20,370 -> Centers partitions: 2
2017-12-09 19:58:27,184 -> 01.Indexing points,6.77,19715,40.0,14
2017-12-09 19:58:32,202 -> 02.Indexing centers,5.02,98838,40.0,14
2017-12-09 19:58:32,212 -> 992
2017-12-09 19:58:32,219 -> 1024
2017-12-09 19:58:43,637 -> 03.Joining datasets,11.42,98837,40.0,14
Done!!! Sat Dec  9 19:58:44 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 19:58:46,462 -> Starting session,1.62,0
2017-12-09 19:58:46,462 -> Setting variables,0.00,0
2017-12-09 19:58:51,444 -> Reading datasets,4.98,0
2017-12-09 19:58:51,454 -> Points partitions: 2
2017-12-09 19:58:51,462 -> Centers partitions: 2
2017-12-09 19:58:58,674 -> 01.Indexing points,7.17,19715,40.0,14
2017-12-09 19:59:03,478 -> 02.Indexing centers,4.80,98838,40.0,14
2017-12-09 19:59:03,485 -> 992
2017-12-09 19:59:03,491 -> 1024
2017-12-09 19:59:15,261 -> 03.Joining datasets,11.77,98837,40.0,14
Done!!! Sat Dec  9 19:59:15 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 19:59:18,063 -> Starting session,1.54,0
2017-12-09 19:59:18,063 -> Setting variables,0.00,0
2017-12-09 19:59:22,883 -> Reading datasets,4.82,0
2017-12-09 19:59:22,894 -> Points partitions: 2
2017-12-09 19:59:22,902 -> Centers partitions: 2
2017-12-09 19:59:29,773 -> 01.Indexing points,6.83,19715,40.0,14
2017-12-09 19:59:34,458 -> 02.Indexing centers,4.68,98838,40.0,14
2017-12-09 19:59:34,467 -> 992
2017-12-09 19:59:34,473 -> 1024
2017-12-09 19:59:46,853 -> 03.Joining datasets,12.38,98837,40.0,14
Done!!! Sat Dec  9 19:59:47 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 19:59:49,574 -> Starting session,1.55,0
2017-12-09 19:59:49,575 -> Setting variables,0.00,0
2017-12-09 19:59:54,490 -> Reading datasets,4.91,0
2017-12-09 19:59:54,501 -> Points partitions: 2
2017-12-09 19:59:54,509 -> Centers partitions: 2
2017-12-09 20:00:01,621 -> 01.Indexing points,7.07,19715,40.0,14
2017-12-09 20:00:06,376 -> 02.Indexing centers,4.75,98838,40.0,14
2017-12-09 20:00:06,387 -> 992
2017-12-09 20:00:06,396 -> 1024
2017-12-09 20:00:18,461 -> 03.Joining datasets,12.06,98837,40.0,14
Done!!! Sat Dec  9 20:00:18 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 20:00:26,674 -> Starting session,1.62,0
2017-12-09 20:00:26,675 -> Setting variables,0.00,0
2017-12-09 20:00:31,519 -> Reading datasets,4.84,0
2017-12-09 20:00:31,530 -> Points partitions: 2
2017-12-09 20:00:31,537 -> Centers partitions: 2
2017-12-09 20:00:39,602 -> 01.Indexing points,8.03,19715,40.0,21
2017-12-09 20:00:43,971 -> 02.Indexing centers,4.37,98838,40.0,21
2017-12-09 20:00:43,983 -> 992
2017-12-09 20:00:43,991 -> 1024
2017-12-09 20:00:54,700 -> 03.Joining datasets,10.71,98837,40.0,21
Done!!! Sat Dec  9 20:00:55 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 20:00:58,383 -> Starting session,1.79,0
2017-12-09 20:00:58,383 -> Setting variables,0.00,0
2017-12-09 20:01:03,120 -> Reading datasets,4.74,0
2017-12-09 20:01:03,130 -> Points partitions: 2
2017-12-09 20:01:03,138 -> Centers partitions: 2
2017-12-09 20:01:09,068 -> 01.Indexing points,5.89,19715,40.0,21
2017-12-09 20:01:13,745 -> 02.Indexing centers,4.68,98838,40.0,21
2017-12-09 20:01:13,752 -> 992
2017-12-09 20:01:13,758 -> 1024
2017-12-09 20:01:26,670 -> 03.Joining datasets,12.91,98837,40.0,21
Done!!! Sat Dec  9 20:01:27 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 20:01:29,642 -> Starting session,1.74,0
2017-12-09 20:01:29,643 -> Setting variables,0.00,0
2017-12-09 20:01:35,244 -> Reading datasets,5.60,0
2017-12-09 20:01:35,263 -> Points partitions: 2
2017-12-09 20:01:35,275 -> Centers partitions: 2
2017-12-09 20:01:42,781 -> 01.Indexing points,7.45,19715,40.0,21
2017-12-09 20:01:47,157 -> 02.Indexing centers,4.37,98838,40.0,21
2017-12-09 20:01:47,169 -> 992
2017-12-09 20:01:47,178 -> 1024
2017-12-09 20:01:57,191 -> 03.Joining datasets,10.01,98837,40.0,21
Done!!! Sat Dec  9 20:01:57 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 20:01:59,949 -> Starting session,1.50,0
2017-12-09 20:01:59,949 -> Setting variables,0.00,0
2017-12-09 20:02:05,505 -> Reading datasets,5.56,0
2017-12-09 20:02:05,529 -> Points partitions: 2
2017-12-09 20:02:05,547 -> Centers partitions: 2
2017-12-09 20:02:13,048 -> 01.Indexing points,7.44,19715,40.0,21
2017-12-09 20:02:17,396 -> 02.Indexing centers,4.35,98838,40.0,21
2017-12-09 20:02:17,408 -> 992
2017-12-09 20:02:17,417 -> 1024
2017-12-09 20:02:30,199 -> 03.Joining datasets,12.96,98837,40.0,21
Done!!! Sat Dec  9 20:02:30 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 20:02:32,992 -> Starting session,1.61,0
2017-12-09 20:02:32,993 -> Setting variables,0.00,0
2017-12-09 20:02:38,555 -> Reading datasets,5.56,0
2017-12-09 20:02:38,571 -> Points partitions: 2
2017-12-09 20:02:38,581 -> Centers partitions: 2
2017-12-09 20:02:46,175 -> 01.Indexing points,7.54,19715,40.0,21
2017-12-09 20:02:50,742 -> 02.Indexing centers,4.57,98838,40.0,21
2017-12-09 20:02:50,754 -> 992
2017-12-09 20:02:50,763 -> 1024
2017-12-09 20:03:01,347 -> 03.Joining datasets,10.58,98837,40.0,21
Done!!! Sat Dec  9 20:03:01 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 20:03:09,661 -> Starting session,1.70,0
2017-12-09 20:03:09,661 -> Setting variables,0.00,0
2017-12-09 20:03:16,675 -> Reading datasets,7.01,0
2017-12-09 20:03:16,692 -> Points partitions: 2
2017-12-09 20:03:16,705 -> Centers partitions: 2
2017-12-09 20:03:24,977 -> 01.Indexing points,8.23,19715,40.0,28
2017-12-09 20:03:29,481 -> 02.Indexing centers,4.50,98838,40.0,28
2017-12-09 20:03:29,488 -> 992
2017-12-09 20:03:29,494 -> 1024
2017-12-09 20:03:39,861 -> 03.Joining datasets,10.37,98837,40.0,28
Done!!! Sat Dec  9 20:03:40 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 20:03:42,728 -> Starting session,1.65,0
2017-12-09 20:03:42,728 -> Setting variables,0.00,0
2017-12-09 20:03:49,533 -> Reading datasets,6.80,0
2017-12-09 20:03:49,545 -> Points partitions: 2
2017-12-09 20:03:49,554 -> Centers partitions: 2
2017-12-09 20:03:57,797 -> 01.Indexing points,8.20,19715,40.0,28
2017-12-09 20:04:03,163 -> 02.Indexing centers,5.36,98838,40.0,28
2017-12-09 20:04:03,179 -> 992
2017-12-09 20:04:03,190 -> 1024
2017-12-09 20:04:14,234 -> 03.Joining datasets,11.04,98837,40.0,28
Done!!! Sat Dec  9 20:04:14 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 20:04:17,022 -> Starting session,1.55,0
2017-12-09 20:04:17,023 -> Setting variables,0.00,0
2017-12-09 20:04:24,090 -> Reading datasets,7.07,0
2017-12-09 20:04:24,102 -> Points partitions: 2
2017-12-09 20:04:24,109 -> Centers partitions: 2
2017-12-09 20:04:32,131 -> 01.Indexing points,7.98,19715,40.0,28
2017-12-09 20:04:37,926 -> 02.Indexing centers,5.79,98838,40.0,28
2017-12-09 20:04:37,938 -> 992
2017-12-09 20:04:37,947 -> 1024
2017-12-09 20:04:47,955 -> 03.Joining datasets,10.01,98837,40.0,28
Done!!! Sat Dec  9 20:04:48 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 20:04:50,743 -> Starting session,1.61,0
2017-12-09 20:04:50,744 -> Setting variables,0.00,0
2017-12-09 20:04:57,779 -> Reading datasets,7.03,0
2017-12-09 20:04:57,796 -> Points partitions: 2
2017-12-09 20:04:57,806 -> Centers partitions: 2
2017-12-09 20:05:03,990 -> 01.Indexing points,6.13,19715,40.0,28
2017-12-09 20:05:09,465 -> 02.Indexing centers,5.47,98838,40.0,28
2017-12-09 20:05:09,474 -> 992
2017-12-09 20:05:09,480 -> 1024
2017-12-09 20:05:20,722 -> 03.Joining datasets,11.24,98837,40.0,28
Done!!! Sat Dec  9 20:05:21 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 20:05:23,483 -> Starting session,1.58,0
2017-12-09 20:05:23,483 -> Setting variables,0.00,0
2017-12-09 20:05:30,204 -> Reading datasets,6.72,0
2017-12-09 20:05:30,221 -> Points partitions: 2
2017-12-09 20:05:30,232 -> Centers partitions: 2
2017-12-09 20:05:39,052 -> 01.Indexing points,8.77,19715,40.0,28
2017-12-09 20:05:44,462 -> 02.Indexing centers,5.41,98838,40.0,28
2017-12-09 20:05:44,470 -> 992
2017-12-09 20:05:44,476 -> 1024
2017-12-09 20:05:53,545 -> 03.Joining datasets,9.07,98837,40.0,28
Done!!! Sat Dec  9 20:05:54 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 20:06:02,006 -> Starting session,1.48,0
2017-12-09 20:06:02,007 -> Setting variables,0.00,0
2017-12-09 20:06:06,273 -> Reading datasets,4.27,0
2017-12-09 20:06:06,286 -> Points partitions: 2
2017-12-09 20:06:06,294 -> Centers partitions: 2
2017-12-09 20:06:13,744 -> 01.Indexing points,7.41,78857,30.0,7
2017-12-09 20:06:20,308 -> 02.Indexing centers,6.56,289496,30.0,7
2017-12-09 20:06:20,322 -> 1024
2017-12-09 20:06:20,331 -> 1024
2017-12-09 20:06:39,650 -> 03.Joining datasets,19.32,289486,30.0,7
Done!!! Sat Dec  9 20:06:40 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 20:06:42,459 -> Starting session,1.61,0
2017-12-09 20:06:42,459 -> Setting variables,0.00,0
2017-12-09 20:06:46,702 -> Reading datasets,4.24,0
2017-12-09 20:06:46,715 -> Points partitions: 2
2017-12-09 20:06:46,724 -> Centers partitions: 2
2017-12-09 20:06:54,701 -> 01.Indexing points,7.89,78857,30.0,7
2017-12-09 20:07:01,246 -> 02.Indexing centers,6.54,289496,30.0,7
2017-12-09 20:07:01,257 -> 1024
2017-12-09 20:07:01,268 -> 1024
2017-12-09 20:07:20,734 -> 03.Joining datasets,19.47,289486,30.0,7
Done!!! Sat Dec  9 20:07:21 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 20:07:23,477 -> Starting session,1.60,0
2017-12-09 20:07:23,478 -> Setting variables,0.00,0
2017-12-09 20:07:27,795 -> Reading datasets,4.32,0
2017-12-09 20:07:27,808 -> Points partitions: 2
2017-12-09 20:07:27,816 -> Centers partitions: 2
2017-12-09 20:07:35,435 -> 01.Indexing points,7.58,78857,30.0,7
2017-12-09 20:07:41,907 -> 02.Indexing centers,6.47,289496,30.0,7
2017-12-09 20:07:41,920 -> 1024
2017-12-09 20:07:41,928 -> 1024
2017-12-09 20:08:01,453 -> 03.Joining datasets,19.52,289486,30.0,7
Done!!! Sat Dec  9 20:08:01 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 20:08:04,251 -> Starting session,1.74,0
2017-12-09 20:08:04,251 -> Setting variables,0.00,0
2017-12-09 20:08:08,529 -> Reading datasets,4.28,0
2017-12-09 20:08:08,542 -> Points partitions: 2
2017-12-09 20:08:08,554 -> Centers partitions: 2
2017-12-09 20:08:16,468 -> 01.Indexing points,7.87,78857,30.0,7
2017-12-09 20:08:22,730 -> 02.Indexing centers,6.26,289496,30.0,7
2017-12-09 20:08:22,742 -> 1024
2017-12-09 20:08:22,750 -> 1024
2017-12-09 20:08:42,662 -> 03.Joining datasets,19.98,289486,30.0,7
Done!!! Sat Dec  9 20:08:43 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 20:08:45,149 -> Starting session,1.39,0
2017-12-09 20:08:45,161 -> Setting variables,0.00,0
2017-12-09 20:08:49,422 -> Reading datasets,4.26,0
2017-12-09 20:08:49,433 -> Points partitions: 2
2017-12-09 20:08:49,440 -> Centers partitions: 2
2017-12-09 20:08:57,007 -> 01.Indexing points,7.53,78857,30.0,7
2017-12-09 20:09:03,563 -> 02.Indexing centers,6.55,289496,30.0,7
2017-12-09 20:09:03,574 -> 1024
2017-12-09 20:09:03,582 -> 1024
2017-12-09 20:09:23,108 -> 03.Joining datasets,19.53,289486,30.0,7
Done!!! Sat Dec  9 20:09:23 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 20:09:31,435 -> Starting session,1.72,0
2017-12-09 20:09:31,435 -> Setting variables,0.00,0
2017-12-09 20:09:36,116 -> Reading datasets,4.68,0
2017-12-09 20:09:36,131 -> Points partitions: 2
2017-12-09 20:09:36,142 -> Centers partitions: 2
2017-12-09 20:09:44,188 -> 01.Indexing points,8.01,78857,30.0,14
2017-12-09 20:09:50,761 -> 02.Indexing centers,6.57,289496,30.0,14
2017-12-09 20:09:50,772 -> 1024
2017-12-09 20:09:50,778 -> 1024
2017-12-09 20:10:06,353 -> 03.Joining datasets,15.57,289486,30.0,14
Done!!! Sat Dec  9 20:10:06 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 20:10:09,215 -> Starting session,1.61,0
2017-12-09 20:10:09,216 -> Setting variables,0.00,0
2017-12-09 20:10:14,172 -> Reading datasets,4.96,0
2017-12-09 20:10:14,184 -> Points partitions: 2
2017-12-09 20:10:14,192 -> Centers partitions: 2
2017-12-09 20:10:22,254 -> 01.Indexing points,8.02,78857,30.0,14
2017-12-09 20:10:29,073 -> 02.Indexing centers,6.82,289496,30.0,14
2017-12-09 20:10:29,085 -> 1024
2017-12-09 20:10:29,094 -> 1024
2017-12-09 20:10:45,967 -> 03.Joining datasets,16.87,289486,30.0,14
Done!!! Sat Dec  9 20:10:46 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 20:10:48,820 -> Starting session,1.65,0
2017-12-09 20:10:48,821 -> Setting variables,0.00,0
2017-12-09 20:10:53,784 -> Reading datasets,4.96,0
2017-12-09 20:10:53,797 -> Points partitions: 2
2017-12-09 20:10:53,806 -> Centers partitions: 2
2017-12-09 20:11:01,724 -> 01.Indexing points,7.88,78857,30.0,14
2017-12-09 20:11:08,102 -> 02.Indexing centers,6.38,289496,30.0,14
2017-12-09 20:11:08,110 -> 1024
2017-12-09 20:11:08,116 -> 1024
2017-12-09 20:11:22,821 -> 03.Joining datasets,14.70,289486,30.0,14
Done!!! Sat Dec  9 20:11:23 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 20:11:25,761 -> Starting session,1.83,0
2017-12-09 20:11:25,761 -> Setting variables,0.00,0
2017-12-09 20:11:30,828 -> Reading datasets,5.07,0
2017-12-09 20:11:30,839 -> Points partitions: 2
2017-12-09 20:11:30,846 -> Centers partitions: 2
2017-12-09 20:11:38,353 -> 01.Indexing points,7.46,78857,30.0,14
2017-12-09 20:11:44,730 -> 02.Indexing centers,6.38,289496,30.0,14
2017-12-09 20:11:44,741 -> 1024
2017-12-09 20:11:44,749 -> 1024
2017-12-09 20:12:00,153 -> 03.Joining datasets,15.40,289486,30.0,14
Done!!! Sat Dec  9 20:12:00 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 20:12:03,017 -> Starting session,1.61,0
2017-12-09 20:12:03,018 -> Setting variables,0.00,0
2017-12-09 20:12:07,829 -> Reading datasets,4.81,0
2017-12-09 20:12:07,842 -> Points partitions: 2
2017-12-09 20:12:07,850 -> Centers partitions: 2
2017-12-09 20:12:15,575 -> 01.Indexing points,7.69,78857,30.0,14
2017-12-09 20:12:21,726 -> 02.Indexing centers,6.15,289496,30.0,14
2017-12-09 20:12:21,734 -> 1024
2017-12-09 20:12:21,740 -> 1024
2017-12-09 20:12:36,831 -> 03.Joining datasets,15.09,289486,30.0,14
Done!!! Sat Dec  9 20:12:37 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 20:12:45,248 -> Starting session,1.63,0
2017-12-09 20:12:45,248 -> Setting variables,0.00,0
2017-12-09 20:12:51,050 -> Reading datasets,5.80,0
2017-12-09 20:12:51,075 -> Points partitions: 2
2017-12-09 20:12:51,091 -> Centers partitions: 2
2017-12-09 20:12:58,189 -> 01.Indexing points,7.04,78857,30.0,21
2017-12-09 20:13:04,092 -> 02.Indexing centers,5.90,289496,30.0,21
2017-12-09 20:13:04,103 -> 1024
2017-12-09 20:13:04,110 -> 1024
2017-12-09 20:13:16,218 -> 03.Joining datasets,12.11,289486,30.0,21
Done!!! Sat Dec  9 20:13:16 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 20:13:18,882 -> Starting session,1.55,0
2017-12-09 20:13:18,882 -> Setting variables,0.00,0
2017-12-09 20:13:25,044 -> Reading datasets,6.16,0
2017-12-09 20:13:25,058 -> Points partitions: 2
2017-12-09 20:13:25,066 -> Centers partitions: 2
2017-12-09 20:13:31,934 -> 01.Indexing points,6.82,78857,30.0,21
2017-12-09 20:13:37,743 -> 02.Indexing centers,5.81,289496,30.0,21
2017-12-09 20:13:37,751 -> 1024
2017-12-09 20:13:37,757 -> 1024
2017-12-09 20:13:50,102 -> 03.Joining datasets,12.34,289486,30.0,21
Done!!! Sat Dec  9 20:13:50 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 20:13:52,847 -> Starting session,1.58,0
2017-12-09 20:13:52,847 -> Setting variables,0.00,0
2017-12-09 20:13:58,669 -> Reading datasets,5.82,0
2017-12-09 20:13:58,688 -> Points partitions: 2
2017-12-09 20:13:58,700 -> Centers partitions: 2
2017-12-09 20:14:05,731 -> 01.Indexing points,6.97,78857,30.0,21
2017-12-09 20:14:11,784 -> 02.Indexing centers,6.05,289496,30.0,21
2017-12-09 20:14:11,792 -> 1024
2017-12-09 20:14:11,798 -> 1024
2017-12-09 20:14:23,541 -> 03.Joining datasets,11.74,289486,30.0,21
Done!!! Sat Dec  9 20:14:24 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 20:14:27,207 -> Starting session,1.63,0
2017-12-09 20:14:27,208 -> Setting variables,0.00,0
2017-12-09 20:14:33,085 -> Reading datasets,5.88,0
2017-12-09 20:14:33,108 -> Points partitions: 2
2017-12-09 20:14:33,122 -> Centers partitions: 2
2017-12-09 20:14:39,884 -> 01.Indexing points,6.71,78857,30.0,21
2017-12-09 20:14:45,407 -> 02.Indexing centers,5.52,289496,30.0,21
2017-12-09 20:14:45,415 -> 1024
2017-12-09 20:14:45,421 -> 1024
2017-12-09 20:14:56,871 -> 03.Joining datasets,11.57,289486,30.0,21
Done!!! Sat Dec  9 20:14:57 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 20:14:59,594 -> Starting session,1.55,0
2017-12-09 20:14:59,594 -> Setting variables,0.00,0
2017-12-09 20:15:05,657 -> Reading datasets,6.06,0
2017-12-09 20:15:05,668 -> Points partitions: 2
2017-12-09 20:15:05,675 -> Centers partitions: 2
2017-12-09 20:15:12,519 -> 01.Indexing points,6.81,78857,30.0,21
2017-12-09 20:15:18,536 -> 02.Indexing centers,6.02,289496,30.0,21
2017-12-09 20:15:18,547 -> 1024
2017-12-09 20:15:18,555 -> 1024
2017-12-09 20:15:30,513 -> 03.Joining datasets,11.96,289486,30.0,21
Done!!! Sat Dec  9 20:15:31 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 20:15:38,768 -> Starting session,1.55,0
2017-12-09 20:15:38,768 -> Setting variables,0.00,0
2017-12-09 20:15:44,774 -> Reading datasets,6.01,0
2017-12-09 20:15:44,785 -> Points partitions: 2
2017-12-09 20:15:44,792 -> Centers partitions: 2
2017-12-09 20:15:54,157 -> 01.Indexing points,9.33,78857,30.0,28
2017-12-09 20:16:02,106 -> 02.Indexing centers,7.95,289496,30.0,28
2017-12-09 20:16:02,114 -> 1024
2017-12-09 20:16:02,121 -> 1024
2017-12-09 20:16:13,215 -> 03.Joining datasets,11.09,289486,30.0,28
Done!!! Sat Dec  9 20:16:14 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 20:16:16,855 -> Starting session,1.60,0
2017-12-09 20:16:16,856 -> Setting variables,0.00,0
2017-12-09 20:16:24,071 -> Reading datasets,7.21,0
2017-12-09 20:16:24,086 -> Points partitions: 2
2017-12-09 20:16:24,094 -> Centers partitions: 2
2017-12-09 20:16:31,375 -> 01.Indexing points,7.24,78857,30.0,28
2017-12-09 20:16:39,580 -> 02.Indexing centers,8.20,289496,30.0,28
2017-12-09 20:16:39,587 -> 1024
2017-12-09 20:16:39,593 -> 1024
2017-12-09 20:16:50,426 -> 03.Joining datasets,10.83,289486,30.0,28
Done!!! Sat Dec  9 20:16:50 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 20:16:53,164 -> Starting session,1.52,0
2017-12-09 20:16:53,164 -> Setting variables,0.00,0
2017-12-09 20:16:59,425 -> Reading datasets,6.26,0
2017-12-09 20:16:59,436 -> Points partitions: 2
2017-12-09 20:16:59,444 -> Centers partitions: 2
2017-12-09 20:17:08,347 -> 01.Indexing points,8.86,78857,30.0,28
2017-12-09 20:17:14,990 -> 02.Indexing centers,6.64,289496,30.0,28
2017-12-09 20:17:14,997 -> 1024
2017-12-09 20:17:15,003 -> 1024
2017-12-09 20:17:25,792 -> 03.Joining datasets,10.79,289486,30.0,28
Done!!! Sat Dec  9 20:17:26 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 20:17:28,635 -> Starting session,1.54,0
2017-12-09 20:17:28,636 -> Setting variables,0.00,0
2017-12-09 20:17:35,778 -> Reading datasets,7.14,0
2017-12-09 20:17:35,795 -> Points partitions: 2
2017-12-09 20:17:35,808 -> Centers partitions: 2
2017-12-09 20:17:43,703 -> 01.Indexing points,7.85,78857,30.0,28
2017-12-09 20:17:49,216 -> 02.Indexing centers,5.51,289496,30.0,28
2017-12-09 20:17:49,228 -> 1024
2017-12-09 20:17:49,236 -> 1024
2017-12-09 20:18:00,522 -> 03.Joining datasets,11.34,289486,30.0,28
Done!!! Sat Dec  9 20:18:01 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 20:18:03,324 -> Starting session,1.62,0
2017-12-09 20:18:03,325 -> Setting variables,0.00,0
2017-12-09 20:18:10,491 -> Reading datasets,7.17,0
2017-12-09 20:18:10,503 -> Points partitions: 2
2017-12-09 20:18:10,511 -> Centers partitions: 2
2017-12-09 20:18:18,289 -> 01.Indexing points,7.64,78857,30.0,28
2017-12-09 20:18:25,880 -> 02.Indexing centers,7.59,289496,30.0,28
2017-12-09 20:18:25,891 -> 1024
2017-12-09 20:18:25,927 -> 1024
2017-12-09 20:18:37,083 -> 03.Joining datasets,11.16,289486,30.0,28
Done!!! Sat Dec  9 20:18:37 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 20:18:45,272 -> Starting session,1.37,0
2017-12-09 20:18:45,273 -> Setting variables,0.00,0
2017-12-09 20:18:49,378 -> Reading datasets,4.10,0
2017-12-09 20:18:49,393 -> Points partitions: 2
2017-12-09 20:18:49,404 -> Centers partitions: 2
2017-12-09 20:18:56,896 -> 01.Indexing points,7.45,59143,30.0,7
2017-12-09 20:19:02,742 -> 02.Indexing centers,5.84,217122,30.0,7
2017-12-09 20:19:02,752 -> 1024
2017-12-09 20:19:02,759 -> 1024
2017-12-09 20:19:20,231 -> 03.Joining datasets,17.47,217115,30.0,7
Done!!! Sat Dec  9 20:19:20 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 20:19:22,668 -> Starting session,1.37,0
2017-12-09 20:19:22,669 -> Setting variables,0.00,0
2017-12-09 20:19:26,944 -> Reading datasets,4.28,0
2017-12-09 20:19:26,956 -> Points partitions: 2
2017-12-09 20:19:26,964 -> Centers partitions: 2
2017-12-09 20:19:34,453 -> 01.Indexing points,7.45,59143,30.0,7
2017-12-09 20:19:40,186 -> 02.Indexing centers,5.73,217122,30.0,7
2017-12-09 20:19:40,203 -> 1024
2017-12-09 20:19:40,217 -> 1024
2017-12-09 20:19:58,472 -> 03.Joining datasets,18.25,217115,30.0,7
Done!!! Sat Dec  9 20:19:58 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 20:20:00,970 -> Starting session,1.44,0
2017-12-09 20:20:00,970 -> Setting variables,0.00,0
2017-12-09 20:20:05,165 -> Reading datasets,4.19,0
2017-12-09 20:20:05,176 -> Points partitions: 2
2017-12-09 20:20:05,183 -> Centers partitions: 2
2017-12-09 20:20:12,789 -> 01.Indexing points,7.57,59143,30.0,7
2017-12-09 20:20:18,664 -> 02.Indexing centers,5.87,217122,30.0,7
2017-12-09 20:20:18,676 -> 1024
2017-12-09 20:20:18,685 -> 1024
2017-12-09 20:20:36,196 -> 03.Joining datasets,17.51,217115,30.0,7
Done!!! Sat Dec  9 20:20:36 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 20:20:38,856 -> Starting session,1.58,0
2017-12-09 20:20:38,857 -> Setting variables,0.00,0
2017-12-09 20:20:43,057 -> Reading datasets,4.20,0
2017-12-09 20:20:43,070 -> Points partitions: 2
2017-12-09 20:20:43,081 -> Centers partitions: 2
2017-12-09 20:20:50,808 -> 01.Indexing points,7.69,59143,30.0,7
2017-12-09 20:20:56,581 -> 02.Indexing centers,5.77,217122,30.0,7
2017-12-09 20:20:56,593 -> 1024
2017-12-09 20:20:56,602 -> 1024
2017-12-09 20:21:15,139 -> 03.Joining datasets,18.54,217115,30.0,7
Done!!! Sat Dec  9 20:21:15 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 20:21:17,677 -> Starting session,1.38,0
2017-12-09 20:21:17,678 -> Setting variables,0.00,0
2017-12-09 20:21:21,924 -> Reading datasets,4.25,0
2017-12-09 20:21:21,937 -> Points partitions: 2
2017-12-09 20:21:21,946 -> Centers partitions: 2
2017-12-09 20:21:29,217 -> 01.Indexing points,7.23,59143,30.0,7
2017-12-09 20:21:35,077 -> 02.Indexing centers,5.86,217122,30.0,7
2017-12-09 20:21:35,087 -> 1024
2017-12-09 20:21:35,095 -> 1024
2017-12-09 20:21:52,360 -> 03.Joining datasets,17.27,217115,30.0,7
Done!!! Sat Dec  9 20:21:52 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 20:22:00,713 -> Starting session,1.72,0
2017-12-09 20:22:00,715 -> Setting variables,0.00,0
2017-12-09 20:22:05,571 -> Reading datasets,4.86,0
2017-12-09 20:22:05,582 -> Points partitions: 2
2017-12-09 20:22:05,589 -> Centers partitions: 2
2017-12-09 20:22:13,122 -> 01.Indexing points,7.49,59143,30.0,14
2017-12-09 20:22:18,795 -> 02.Indexing centers,5.67,217122,30.0,14
2017-12-09 20:22:18,806 -> 1024
2017-12-09 20:22:18,815 -> 1024
2017-12-09 20:22:33,594 -> 03.Joining datasets,14.78,217115,30.0,14
Done!!! Sat Dec  9 20:22:34 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 20:22:36,481 -> Starting session,1.66,0
2017-12-09 20:22:36,482 -> Setting variables,0.00,0
2017-12-09 20:22:41,348 -> Reading datasets,4.87,0
2017-12-09 20:22:41,362 -> Points partitions: 2
2017-12-09 20:22:41,373 -> Centers partitions: 2
2017-12-09 20:22:48,681 -> 01.Indexing points,7.27,59143,30.0,14
2017-12-09 20:22:54,498 -> 02.Indexing centers,5.90,217122,30.0,14
2017-12-09 20:22:54,511 -> 1024
2017-12-09 20:22:54,520 -> 1024
2017-12-09 20:23:09,579 -> 03.Joining datasets,15.06,217115,30.0,14
Done!!! Sat Dec  9 20:23:10 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 20:23:12,470 -> Starting session,1.68,0
2017-12-09 20:23:12,470 -> Setting variables,0.00,0
2017-12-09 20:23:17,076 -> Reading datasets,4.61,0
2017-12-09 20:23:17,087 -> Points partitions: 2
2017-12-09 20:23:17,095 -> Centers partitions: 2
2017-12-09 20:23:24,654 -> 01.Indexing points,7.52,59143,30.0,14
2017-12-09 20:23:30,637 -> 02.Indexing centers,5.98,217122,30.0,14
2017-12-09 20:23:30,646 -> 1024
2017-12-09 20:23:30,653 -> 1024
2017-12-09 20:23:46,068 -> 03.Joining datasets,15.41,217115,30.0,14
Done!!! Sat Dec  9 20:23:46 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 20:23:48,929 -> Starting session,1.62,0
2017-12-09 20:23:48,930 -> Setting variables,0.00,0
2017-12-09 20:23:53,983 -> Reading datasets,5.05,0
2017-12-09 20:23:53,993 -> Points partitions: 2
2017-12-09 20:23:54,000 -> Centers partitions: 2
2017-12-09 20:24:01,779 -> 01.Indexing points,7.74,59143,30.0,14
2017-12-09 20:24:07,760 -> 02.Indexing centers,5.98,217122,30.0,14
2017-12-09 20:24:07,771 -> 1024
2017-12-09 20:24:07,780 -> 1024
2017-12-09 20:24:22,868 -> 03.Joining datasets,15.09,217115,30.0,14
Done!!! Sat Dec  9 20:24:23 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 20:24:25,748 -> Starting session,1.65,0
2017-12-09 20:24:25,749 -> Setting variables,0.00,0
2017-12-09 20:24:30,645 -> Reading datasets,4.90,0
2017-12-09 20:24:30,658 -> Points partitions: 2
2017-12-09 20:24:30,669 -> Centers partitions: 2
2017-12-09 20:24:38,598 -> 01.Indexing points,7.89,59143,30.0,14
2017-12-09 20:24:44,481 -> 02.Indexing centers,5.88,217122,30.0,14
2017-12-09 20:24:44,491 -> 1024
2017-12-09 20:24:44,498 -> 1024
2017-12-09 20:24:59,969 -> 03.Joining datasets,15.47,217115,30.0,14
Done!!! Sat Dec  9 20:25:00 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 20:25:08,087 -> Starting session,1.71,0
2017-12-09 20:25:08,088 -> Setting variables,0.00,0
2017-12-09 20:25:13,293 -> Reading datasets,5.20,0
2017-12-09 20:25:13,307 -> Points partitions: 2
2017-12-09 20:25:13,318 -> Centers partitions: 2
2017-12-09 20:25:21,289 -> 01.Indexing points,7.93,59143,30.0,21
2017-12-09 20:25:26,161 -> 02.Indexing centers,4.87,217122,30.0,21
2017-12-09 20:25:26,170 -> 1024
2017-12-09 20:25:26,176 -> 1024
2017-12-09 20:25:37,352 -> 03.Joining datasets,11.18,217115,30.0,21
Done!!! Sat Dec  9 20:25:37 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 20:25:40,300 -> Starting session,1.65,0
2017-12-09 20:25:40,300 -> Setting variables,0.00,0
2017-12-09 20:25:45,477 -> Reading datasets,5.18,0
2017-12-09 20:25:45,491 -> Points partitions: 2
2017-12-09 20:25:45,502 -> Centers partitions: 2
2017-12-09 20:25:53,016 -> 01.Indexing points,7.47,59143,30.0,21
2017-12-09 20:25:58,182 -> 02.Indexing centers,5.16,217122,30.0,21
2017-12-09 20:25:58,191 -> 1024
2017-12-09 20:25:58,201 -> 1024
2017-12-09 20:26:09,169 -> 03.Joining datasets,10.97,217115,30.0,21
Done!!! Sat Dec  9 20:26:09 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 20:26:11,906 -> Starting session,1.63,0
2017-12-09 20:26:11,906 -> Setting variables,0.00,0
2017-12-09 20:26:18,133 -> Reading datasets,6.23,0
2017-12-09 20:26:18,148 -> Points partitions: 2
2017-12-09 20:26:18,158 -> Centers partitions: 2
2017-12-09 20:26:24,675 -> 01.Indexing points,6.48,59143,30.0,21
2017-12-09 20:26:29,844 -> 02.Indexing centers,5.17,217122,30.0,21
2017-12-09 20:26:29,851 -> 1024
2017-12-09 20:26:29,857 -> 1024
2017-12-09 20:26:41,780 -> 03.Joining datasets,11.92,217115,30.0,21
Done!!! Sat Dec  9 20:26:42 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 20:26:44,668 -> Starting session,1.76,0
2017-12-09 20:26:44,668 -> Setting variables,0.00,0
2017-12-09 20:26:50,886 -> Reading datasets,6.22,0
2017-12-09 20:26:50,898 -> Points partitions: 2
2017-12-09 20:26:50,906 -> Centers partitions: 2
2017-12-09 20:26:57,340 -> 01.Indexing points,6.34,59143,30.0,21
2017-12-09 20:27:03,029 -> 02.Indexing centers,5.69,217122,30.0,21
2017-12-09 20:27:03,041 -> 1024
2017-12-09 20:27:03,050 -> 1024
2017-12-09 20:27:14,761 -> 03.Joining datasets,11.71,217115,30.0,21
Done!!! Sat Dec  9 20:27:15 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 20:27:17,585 -> Starting session,1.61,0
2017-12-09 20:27:17,586 -> Setting variables,0.00,0
2017-12-09 20:27:23,617 -> Reading datasets,6.03,0
2017-12-09 20:27:23,628 -> Points partitions: 2
2017-12-09 20:27:23,635 -> Centers partitions: 2
2017-12-09 20:27:30,997 -> 01.Indexing points,7.28,59143,30.0,21
2017-12-09 20:27:36,145 -> 02.Indexing centers,5.15,217122,30.0,21
2017-12-09 20:27:36,155 -> 1024
2017-12-09 20:27:36,162 -> 1024
2017-12-09 20:27:47,327 -> 03.Joining datasets,11.16,217115,30.0,21
Done!!! Sat Dec  9 20:27:47 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 20:27:55,824 -> Starting session,1.87,0
2017-12-09 20:27:55,825 -> Setting variables,0.00,0
2017-12-09 20:28:03,122 -> Reading datasets,7.30,0
2017-12-09 20:28:03,132 -> Points partitions: 2
2017-12-09 20:28:03,140 -> Centers partitions: 2
2017-12-09 20:28:10,197 -> 01.Indexing points,6.98,59143,30.0,28
2017-12-09 20:28:15,299 -> 02.Indexing centers,5.10,217122,30.0,28
2017-12-09 20:28:15,309 -> 1024
2017-12-09 20:28:15,317 -> 1024
2017-12-09 20:28:26,428 -> 03.Joining datasets,11.11,217115,30.0,28
Done!!! Sat Dec  9 20:28:26 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 20:28:29,322 -> Starting session,1.69,0
2017-12-09 20:28:29,323 -> Setting variables,0.00,0
2017-12-09 20:28:35,526 -> Reading datasets,6.20,0
2017-12-09 20:28:35,541 -> Points partitions: 2
2017-12-09 20:28:35,551 -> Centers partitions: 2
2017-12-09 20:28:45,014 -> 01.Indexing points,9.42,59143,30.0,28
2017-12-09 20:28:50,161 -> 02.Indexing centers,5.15,217122,30.0,28
2017-12-09 20:28:50,168 -> 1024
2017-12-09 20:28:50,174 -> 1024
2017-12-09 20:29:00,574 -> 03.Joining datasets,10.40,217115,30.0,28
Done!!! Sat Dec  9 20:29:01 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 20:29:03,446 -> Starting session,1.65,0
2017-12-09 20:29:03,447 -> Setting variables,0.00,0
2017-12-09 20:29:08,504 -> Reading datasets,5.06,0
2017-12-09 20:29:08,515 -> Points partitions: 2
2017-12-09 20:29:08,523 -> Centers partitions: 2
2017-12-09 20:29:18,862 -> 01.Indexing points,10.30,59143,30.0,28
2017-12-09 20:29:24,577 -> 02.Indexing centers,5.71,217122,30.0,28
2017-12-09 20:29:24,586 -> 1024
2017-12-09 20:29:24,592 -> 1024
2017-12-09 20:29:34,306 -> 03.Joining datasets,9.71,217115,30.0,28
Done!!! Sat Dec  9 20:29:34 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 20:29:37,317 -> Starting session,1.74,0
2017-12-09 20:29:37,318 -> Setting variables,0.00,0
2017-12-09 20:29:43,258 -> Reading datasets,5.94,0
2017-12-09 20:29:43,269 -> Points partitions: 2
2017-12-09 20:29:43,277 -> Centers partitions: 2
2017-12-09 20:29:52,499 -> 01.Indexing points,9.18,59143,30.0,28
2017-12-09 20:29:57,959 -> 02.Indexing centers,5.46,217122,30.0,28
2017-12-09 20:29:57,969 -> 1024
2017-12-09 20:29:57,977 -> 1024
2017-12-09 20:30:08,286 -> 03.Joining datasets,10.31,217115,30.0,28
Done!!! Sat Dec  9 20:30:08 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 20:30:11,034 -> Starting session,1.67,0
2017-12-09 20:30:11,035 -> Setting variables,0.00,0
2017-12-09 20:30:18,464 -> Reading datasets,7.43,0
2017-12-09 20:30:18,478 -> Points partitions: 2
2017-12-09 20:30:18,487 -> Centers partitions: 2
2017-12-09 20:30:26,558 -> 01.Indexing points,8.03,59143,30.0,28
2017-12-09 20:30:32,173 -> 02.Indexing centers,5.61,217122,30.0,28
2017-12-09 20:30:32,181 -> 1024
2017-12-09 20:30:32,187 -> 1024
2017-12-09 20:30:43,554 -> 03.Joining datasets,11.37,217115,30.0,28
Done!!! Sat Dec  9 20:30:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 20:30:51,724 -> Starting session,1.49,0
2017-12-09 20:30:51,724 -> Setting variables,0.00,0
2017-12-09 20:30:55,936 -> Reading datasets,4.21,0
2017-12-09 20:30:55,950 -> Points partitions: 2
2017-12-09 20:30:55,960 -> Centers partitions: 2
2017-12-09 20:31:02,850 -> 01.Indexing points,6.85,39429,30.0,7
2017-12-09 20:31:07,959 -> 02.Indexing centers,5.11,144748,30.0,7
2017-12-09 20:31:07,967 -> 1024
2017-12-09 20:31:07,972 -> 1024
2017-12-09 20:31:23,514 -> 03.Joining datasets,15.54,144743,30.0,7
Done!!! Sat Dec  9 20:31:23 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 20:31:25,962 -> Starting session,1.45,0
2017-12-09 20:31:25,963 -> Setting variables,0.00,0
2017-12-09 20:31:30,236 -> Reading datasets,4.27,0
2017-12-09 20:31:30,249 -> Points partitions: 2
2017-12-09 20:31:30,258 -> Centers partitions: 2
2017-12-09 20:31:37,274 -> 01.Indexing points,6.98,39429,30.0,7
2017-12-09 20:31:42,731 -> 02.Indexing centers,5.46,144748,30.0,7
2017-12-09 20:31:42,739 -> 1024
2017-12-09 20:31:42,745 -> 1024
2017-12-09 20:31:58,906 -> 03.Joining datasets,16.16,144743,30.0,7
Done!!! Sat Dec  9 20:31:59 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 20:32:01,408 -> Starting session,1.50,0
2017-12-09 20:32:01,409 -> Setting variables,0.00,0
2017-12-09 20:32:05,534 -> Reading datasets,4.12,0
2017-12-09 20:32:05,544 -> Points partitions: 2
2017-12-09 20:32:05,552 -> Centers partitions: 2
2017-12-09 20:32:12,480 -> 01.Indexing points,6.89,39429,30.0,7
2017-12-09 20:32:17,838 -> 02.Indexing centers,5.36,144748,30.0,7
2017-12-09 20:32:17,846 -> 1024
2017-12-09 20:32:17,851 -> 1024
2017-12-09 20:32:33,790 -> 03.Joining datasets,15.94,144743,30.0,7
Done!!! Sat Dec  9 20:32:34 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 20:32:36,435 -> Starting session,1.66,0
2017-12-09 20:32:36,435 -> Setting variables,0.00,0
2017-12-09 20:32:40,377 -> Reading datasets,3.94,0
2017-12-09 20:32:40,390 -> Points partitions: 2
2017-12-09 20:32:40,400 -> Centers partitions: 2
2017-12-09 20:32:47,034 -> 01.Indexing points,6.59,39429,30.0,7
2017-12-09 20:32:52,329 -> 02.Indexing centers,5.29,144748,30.0,7
2017-12-09 20:32:52,336 -> 1024
2017-12-09 20:32:52,342 -> 1024
2017-12-09 20:33:08,282 -> 03.Joining datasets,15.94,144743,30.0,7
Done!!! Sat Dec  9 20:33:08 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 20:33:10,728 -> Starting session,1.41,0
2017-12-09 20:33:10,729 -> Setting variables,0.00,0
2017-12-09 20:33:14,894 -> Reading datasets,4.16,0
2017-12-09 20:33:14,907 -> Points partitions: 2
2017-12-09 20:33:14,917 -> Centers partitions: 2
2017-12-09 20:33:21,868 -> 01.Indexing points,7.03,39429,30.0,7
2017-12-09 20:33:27,198 -> 02.Indexing centers,5.33,144748,30.0,7
2017-12-09 20:33:27,207 -> 1024
2017-12-09 20:33:27,213 -> 1024
2017-12-09 20:33:43,459 -> 03.Joining datasets,16.25,144743,30.0,7
Done!!! Sat Dec  9 20:33:43 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 20:33:51,635 -> Starting session,1.76,0
2017-12-09 20:33:51,636 -> Setting variables,0.00,0
2017-12-09 20:33:56,520 -> Reading datasets,4.88,0
2017-12-09 20:33:56,533 -> Points partitions: 2
2017-12-09 20:33:56,541 -> Centers partitions: 2
2017-12-09 20:34:04,181 -> 01.Indexing points,7.60,39429,30.0,14
2017-12-09 20:34:09,549 -> 02.Indexing centers,5.37,144748,30.0,14
2017-12-09 20:34:09,559 -> 1024
2017-12-09 20:34:09,567 -> 1024
2017-12-09 20:34:22,892 -> 03.Joining datasets,13.32,144743,30.0,14
Done!!! Sat Dec  9 20:34:23 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 20:34:25,750 -> Starting session,1.76,0
2017-12-09 20:34:25,751 -> Setting variables,0.00,0
2017-12-09 20:34:30,656 -> Reading datasets,4.90,0
2017-12-09 20:34:30,668 -> Points partitions: 2
2017-12-09 20:34:30,677 -> Centers partitions: 2
2017-12-09 20:34:38,100 -> 01.Indexing points,7.38,39429,30.0,14
2017-12-09 20:34:44,001 -> 02.Indexing centers,5.90,144748,30.0,14
2017-12-09 20:34:44,012 -> 1024
2017-12-09 20:34:44,017 -> 1024
2017-12-09 20:34:58,258 -> 03.Joining datasets,14.24,144743,30.0,14
Done!!! Sat Dec  9 20:34:58 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 20:35:01,205 -> Starting session,1.71,0
2017-12-09 20:35:01,206 -> Setting variables,0.00,0
2017-12-09 20:35:06,475 -> Reading datasets,5.27,0
2017-12-09 20:35:06,485 -> Points partitions: 2
2017-12-09 20:35:06,493 -> Centers partitions: 2
2017-12-09 20:35:14,194 -> 01.Indexing points,7.67,39429,30.0,14
2017-12-09 20:35:19,766 -> 02.Indexing centers,5.57,144748,30.0,14
2017-12-09 20:35:19,776 -> 1024
2017-12-09 20:35:19,783 -> 1024
2017-12-09 20:35:33,832 -> 03.Joining datasets,14.05,144743,30.0,14
Done!!! Sat Dec  9 20:35:34 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 20:35:36,693 -> Starting session,1.65,0
2017-12-09 20:35:36,694 -> Setting variables,0.00,0
2017-12-09 20:35:41,654 -> Reading datasets,4.96,0
2017-12-09 20:35:41,667 -> Points partitions: 2
2017-12-09 20:35:41,676 -> Centers partitions: 2
2017-12-09 20:35:49,250 -> 01.Indexing points,7.53,39429,30.0,14
2017-12-09 20:35:54,623 -> 02.Indexing centers,5.37,144748,30.0,14
2017-12-09 20:35:54,632 -> 1024
2017-12-09 20:35:54,640 -> 1024
2017-12-09 20:36:08,608 -> 03.Joining datasets,13.97,144743,30.0,14
Done!!! Sat Dec  9 20:36:09 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 20:36:11,276 -> Starting session,1.58,0
2017-12-09 20:36:11,276 -> Setting variables,0.00,0
2017-12-09 20:36:16,316 -> Reading datasets,5.04,0
2017-12-09 20:36:16,326 -> Points partitions: 2
2017-12-09 20:36:16,334 -> Centers partitions: 2
2017-12-09 20:36:23,827 -> 01.Indexing points,7.45,39429,30.0,14
2017-12-09 20:36:29,357 -> 02.Indexing centers,5.53,144748,30.0,14
2017-12-09 20:36:29,368 -> 1024
2017-12-09 20:36:29,377 -> 1024
2017-12-09 20:36:44,039 -> 03.Joining datasets,13.74,144743,30.0,14
Done!!! Sat Dec  9 20:36:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 20:36:52,366 -> Starting session,1.68,0
2017-12-09 20:36:52,368 -> Setting variables,0.00,0
2017-12-09 20:36:57,748 -> Reading datasets,5.38,0
2017-12-09 20:36:57,760 -> Points partitions: 2
2017-12-09 20:36:57,767 -> Centers partitions: 2
2017-12-09 20:37:05,434 -> 01.Indexing points,7.63,39429,30.0,21
2017-12-09 20:37:10,234 -> 02.Indexing centers,4.80,144748,30.0,21
2017-12-09 20:37:10,243 -> 1024
2017-12-09 20:37:10,251 -> 1024
2017-12-09 20:37:20,980 -> 03.Joining datasets,10.73,144743,30.0,21
Done!!! Sat Dec  9 20:37:21 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 20:37:23,882 -> Starting session,1.81,0
2017-12-09 20:37:23,883 -> Setting variables,0.00,0
2017-12-09 20:37:28,694 -> Reading datasets,4.81,0
2017-12-09 20:37:28,705 -> Points partitions: 2
2017-12-09 20:37:28,714 -> Centers partitions: 2
2017-12-09 20:37:36,429 -> 01.Indexing points,7.68,39429,30.0,21
2017-12-09 20:37:41,498 -> 02.Indexing centers,5.07,144748,30.0,21
2017-12-09 20:37:41,508 -> 1024
2017-12-09 20:37:41,516 -> 1024
2017-12-09 20:37:53,142 -> 03.Joining datasets,11.63,144743,30.0,21
Done!!! Sat Dec  9 20:37:54 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 20:37:56,766 -> Starting session,1.62,0
2017-12-09 20:37:56,767 -> Setting variables,0.00,0
2017-12-09 20:38:02,668 -> Reading datasets,5.90,0
2017-12-09 20:38:02,681 -> Points partitions: 2
2017-12-09 20:38:02,691 -> Centers partitions: 2
2017-12-09 20:38:09,826 -> 01.Indexing points,7.08,39429,30.0,21
2017-12-09 20:38:15,404 -> 02.Indexing centers,5.58,144748,30.0,21
2017-12-09 20:38:15,412 -> 1024
2017-12-09 20:38:15,418 -> 1024
2017-12-09 20:38:25,697 -> 03.Joining datasets,10.28,144743,30.0,21
Done!!! Sat Dec  9 20:38:26 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 20:38:28,510 -> Starting session,1.55,0
2017-12-09 20:38:28,511 -> Setting variables,0.00,0
2017-12-09 20:38:34,608 -> Reading datasets,6.10,0
2017-12-09 20:38:34,622 -> Points partitions: 2
2017-12-09 20:38:34,632 -> Centers partitions: 2
2017-12-09 20:38:41,428 -> 01.Indexing points,6.75,39429,30.0,21
2017-12-09 20:38:46,261 -> 02.Indexing centers,4.83,144748,30.0,21
2017-12-09 20:38:46,271 -> 1024
2017-12-09 20:38:46,279 -> 1024
2017-12-09 20:38:56,325 -> 03.Joining datasets,10.05,144743,30.0,21
Done!!! Sat Dec  9 20:38:56 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 20:38:59,148 -> Starting session,1.60,0
2017-12-09 20:38:59,148 -> Setting variables,0.00,0
2017-12-09 20:39:04,132 -> Reading datasets,4.98,0
2017-12-09 20:39:04,145 -> Points partitions: 2
2017-12-09 20:39:04,154 -> Centers partitions: 2
2017-12-09 20:39:11,817 -> 01.Indexing points,7.62,39429,30.0,21
2017-12-09 20:39:17,056 -> 02.Indexing centers,5.24,144748,30.0,21
2017-12-09 20:39:17,065 -> 1024
2017-12-09 20:39:17,071 -> 1024
2017-12-09 20:39:28,606 -> 03.Joining datasets,11.54,144743,30.0,21
Done!!! Sat Dec  9 20:39:29 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 20:39:37,036 -> Starting session,1.78,0
2017-12-09 20:39:37,037 -> Setting variables,0.00,0
2017-12-09 20:39:43,337 -> Reading datasets,6.30,0
2017-12-09 20:39:43,348 -> Points partitions: 2
2017-12-09 20:39:43,356 -> Centers partitions: 2
2017-12-09 20:39:52,079 -> 01.Indexing points,8.68,39429,30.0,28
2017-12-09 20:39:57,150 -> 02.Indexing centers,5.07,144748,30.0,28
2017-12-09 20:39:57,162 -> 1024
2017-12-09 20:39:57,170 -> 1024
2017-12-09 20:40:06,357 -> 03.Joining datasets,9.19,144743,30.0,28
Done!!! Sat Dec  9 20:40:06 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 20:40:09,310 -> Starting session,1.71,0
2017-12-09 20:40:09,310 -> Setting variables,0.00,0
2017-12-09 20:40:15,567 -> Reading datasets,6.26,0
2017-12-09 20:40:15,582 -> Points partitions: 2
2017-12-09 20:40:15,594 -> Centers partitions: 2
2017-12-09 20:40:24,784 -> 01.Indexing points,9.14,39429,30.0,28
2017-12-09 20:40:30,526 -> 02.Indexing centers,5.74,144748,30.0,28
2017-12-09 20:40:30,536 -> 1024
2017-12-09 20:40:30,544 -> 1024
2017-12-09 20:40:42,657 -> 03.Joining datasets,12.11,144743,30.0,28
Done!!! Sat Dec  9 20:40:43 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 20:40:46,321 -> Starting session,1.80,0
2017-12-09 20:40:46,321 -> Setting variables,0.00,0
2017-12-09 20:40:52,362 -> Reading datasets,6.04,0
2017-12-09 20:40:52,374 -> Points partitions: 2
2017-12-09 20:40:52,383 -> Centers partitions: 2
2017-12-09 20:40:59,177 -> 01.Indexing points,6.75,39429,30.0,28
2017-12-09 20:41:05,315 -> 02.Indexing centers,6.14,144748,30.0,28
2017-12-09 20:41:05,323 -> 1024
2017-12-09 20:41:05,329 -> 1024
2017-12-09 20:41:14,739 -> 03.Joining datasets,9.41,144743,30.0,28
Done!!! Sat Dec  9 20:41:15 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 20:41:17,584 -> Starting session,1.60,0
2017-12-09 20:41:17,584 -> Setting variables,0.00,0
2017-12-09 20:41:24,784 -> Reading datasets,7.20,0
2017-12-09 20:41:24,797 -> Points partitions: 2
2017-12-09 20:41:24,807 -> Centers partitions: 2
2017-12-09 20:41:32,755 -> 01.Indexing points,7.90,39429,30.0,28
2017-12-09 20:41:37,433 -> 02.Indexing centers,4.68,144748,30.0,28
2017-12-09 20:41:37,442 -> 1024
2017-12-09 20:41:37,450 -> 1024
2017-12-09 20:41:47,018 -> 03.Joining datasets,9.57,144743,30.0,28
Done!!! Sat Dec  9 20:41:48 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 20:41:50,752 -> Starting session,1.70,0
2017-12-09 20:41:50,752 -> Setting variables,0.00,0
2017-12-09 20:41:58,057 -> Reading datasets,7.30,0
2017-12-09 20:41:58,069 -> Points partitions: 2
2017-12-09 20:41:58,076 -> Centers partitions: 2
2017-12-09 20:42:05,552 -> 01.Indexing points,7.44,39429,30.0,28
2017-12-09 20:42:10,157 -> 02.Indexing centers,4.60,144748,30.0,28
2017-12-09 20:42:10,167 -> 1024
2017-12-09 20:42:10,176 -> 1024
2017-12-09 20:42:19,724 -> 03.Joining datasets,9.55,144743,30.0,28
Done!!! Sat Dec  9 20:42:20 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 20:42:27,931 -> Starting session,1.59,0
2017-12-09 20:42:27,932 -> Setting variables,0.00,0
2017-12-09 20:42:31,935 -> Reading datasets,4.00,0
2017-12-09 20:42:31,947 -> Points partitions: 2
2017-12-09 20:42:31,954 -> Centers partitions: 2
2017-12-09 20:42:38,497 -> 01.Indexing points,6.51,19715,30.0,7
2017-12-09 20:42:43,168 -> 02.Indexing centers,4.67,72374,30.0,7
2017-12-09 20:42:43,175 -> 992
2017-12-09 20:42:43,181 -> 1024
2017-12-09 20:42:56,782 -> 03.Joining datasets,13.60,72372,30.0,7
Done!!! Sat Dec  9 20:42:57 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 20:42:59,225 -> Starting session,1.46,0
2017-12-09 20:42:59,225 -> Setting variables,0.00,0
2017-12-09 20:43:03,278 -> Reading datasets,4.05,0
2017-12-09 20:43:03,291 -> Points partitions: 2
2017-12-09 20:43:03,302 -> Centers partitions: 2
2017-12-09 20:43:10,482 -> 01.Indexing points,7.13,19715,30.0,7
2017-12-09 20:43:15,331 -> 02.Indexing centers,4.85,72374,30.0,7
2017-12-09 20:43:15,340 -> 992
2017-12-09 20:43:15,345 -> 1024
2017-12-09 20:43:29,410 -> 03.Joining datasets,14.06,72372,30.0,7
Done!!! Sat Dec  9 20:43:29 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 20:43:31,915 -> Starting session,1.52,0
2017-12-09 20:43:31,916 -> Setting variables,0.00,0
2017-12-09 20:43:36,054 -> Reading datasets,4.14,0
2017-12-09 20:43:36,067 -> Points partitions: 2
2017-12-09 20:43:36,077 -> Centers partitions: 2
2017-12-09 20:43:42,991 -> 01.Indexing points,6.87,19715,30.0,7
2017-12-09 20:43:47,800 -> 02.Indexing centers,4.81,72374,30.0,7
2017-12-09 20:43:47,808 -> 992
2017-12-09 20:43:47,814 -> 1024
2017-12-09 20:44:02,138 -> 03.Joining datasets,14.32,72372,30.0,7
Done!!! Sat Dec  9 20:44:02 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 20:44:04,693 -> Starting session,1.58,0
2017-12-09 20:44:04,694 -> Setting variables,0.00,0
2017-12-09 20:44:08,929 -> Reading datasets,4.23,0
2017-12-09 20:44:08,942 -> Points partitions: 2
2017-12-09 20:44:08,950 -> Centers partitions: 2
2017-12-09 20:44:15,297 -> 01.Indexing points,6.31,19715,30.0,7
2017-12-09 20:44:19,960 -> 02.Indexing centers,4.66,72374,30.0,7
2017-12-09 20:44:19,967 -> 992
2017-12-09 20:44:19,973 -> 1024
2017-12-09 20:44:33,313 -> 03.Joining datasets,13.34,72372,30.0,7
Done!!! Sat Dec  9 20:44:33 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 20:44:35,901 -> Starting session,1.57,0
2017-12-09 20:44:35,902 -> Setting variables,0.00,0
2017-12-09 20:44:40,115 -> Reading datasets,4.21,0
2017-12-09 20:44:40,129 -> Points partitions: 2
2017-12-09 20:44:40,139 -> Centers partitions: 2
2017-12-09 20:44:47,121 -> 01.Indexing points,6.94,19715,30.0,7
2017-12-09 20:44:51,886 -> 02.Indexing centers,4.76,72374,30.0,7
2017-12-09 20:44:51,893 -> 992
2017-12-09 20:44:51,899 -> 1024
2017-12-09 20:45:05,914 -> 03.Joining datasets,14.20,72372,30.0,7
Done!!! Sat Dec  9 20:45:06 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 20:45:14,345 -> Starting session,1.88,0
2017-12-09 20:45:14,346 -> Setting variables,0.00,0
2017-12-09 20:45:19,226 -> Reading datasets,4.88,0
2017-12-09 20:45:19,237 -> Points partitions: 2
2017-12-09 20:45:19,244 -> Centers partitions: 2
2017-12-09 20:45:26,943 -> 01.Indexing points,7.66,19715,30.0,14
2017-12-09 20:45:31,768 -> 02.Indexing centers,4.82,72374,30.0,14
2017-12-09 20:45:31,779 -> 992
2017-12-09 20:45:31,787 -> 1024
2017-12-09 20:45:43,284 -> 03.Joining datasets,11.50,72372,30.0,14
Done!!! Sat Dec  9 20:45:43 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 20:45:46,018 -> Starting session,1.66,0
2017-12-09 20:45:46,019 -> Setting variables,0.00,0
2017-12-09 20:45:50,960 -> Reading datasets,4.94,0
2017-12-09 20:45:50,972 -> Points partitions: 2
2017-12-09 20:45:50,979 -> Centers partitions: 2
2017-12-09 20:45:58,279 -> 01.Indexing points,7.26,19715,30.0,14
2017-12-09 20:46:03,117 -> 02.Indexing centers,4.84,72374,30.0,14
2017-12-09 20:46:03,127 -> 992
2017-12-09 20:46:03,137 -> 1024
2017-12-09 20:46:15,302 -> 03.Joining datasets,12.16,72372,30.0,14
Done!!! Sat Dec  9 20:46:15 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 20:46:18,206 -> Starting session,1.70,0
2017-12-09 20:46:18,207 -> Setting variables,0.00,0
2017-12-09 20:46:23,072 -> Reading datasets,4.87,0
2017-12-09 20:46:23,084 -> Points partitions: 2
2017-12-09 20:46:23,094 -> Centers partitions: 2
2017-12-09 20:46:30,498 -> 01.Indexing points,7.36,19715,30.0,14
2017-12-09 20:46:35,408 -> 02.Indexing centers,4.91,72374,30.0,14
2017-12-09 20:46:35,419 -> 992
2017-12-09 20:46:35,428 -> 1024
2017-12-09 20:46:48,058 -> 03.Joining datasets,12.63,72372,30.0,14
Done!!! Sat Dec  9 20:46:48 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 20:46:50,981 -> Starting session,1.70,0
2017-12-09 20:46:50,981 -> Setting variables,0.00,0
2017-12-09 20:46:56,040 -> Reading datasets,5.06,0
2017-12-09 20:46:56,052 -> Points partitions: 2
2017-12-09 20:46:56,060 -> Centers partitions: 2
2017-12-09 20:47:03,676 -> 01.Indexing points,7.57,19715,30.0,14
2017-12-09 20:47:08,349 -> 02.Indexing centers,4.67,72374,30.0,14
2017-12-09 20:47:08,357 -> 992
2017-12-09 20:47:08,362 -> 1024
2017-12-09 20:47:20,610 -> 03.Joining datasets,12.25,72372,30.0,14
Done!!! Sat Dec  9 20:47:21 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 20:47:23,626 -> Starting session,1.83,0
2017-12-09 20:47:23,627 -> Setting variables,0.00,0
2017-12-09 20:47:28,483 -> Reading datasets,4.92,0
2017-12-09 20:47:28,492 -> Points partitions: 2
2017-12-09 20:47:28,501 -> Centers partitions: 2
2017-12-09 20:47:36,072 -> 01.Indexing points,7.53,19715,30.0,14
2017-12-09 20:47:41,512 -> 02.Indexing centers,5.44,72374,30.0,14
2017-12-09 20:47:41,520 -> 992
2017-12-09 20:47:41,526 -> 1024
2017-12-09 20:47:53,323 -> 03.Joining datasets,11.80,72372,30.0,14
Done!!! Sat Dec  9 20:47:54 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 20:48:02,424 -> Starting session,1.79,0
2017-12-09 20:48:02,424 -> Setting variables,0.00,0
2017-12-09 20:48:07,380 -> Reading datasets,4.96,0
2017-12-09 20:48:07,393 -> Points partitions: 2
2017-12-09 20:48:07,404 -> Centers partitions: 2
2017-12-09 20:48:16,216 -> 01.Indexing points,8.77,19715,30.0,21
2017-12-09 20:48:20,886 -> 02.Indexing centers,4.67,72374,30.0,21
2017-12-09 20:48:20,893 -> 992
2017-12-09 20:48:20,900 -> 1024
2017-12-09 20:48:31,579 -> 03.Joining datasets,10.68,72372,30.0,21
Done!!! Sat Dec  9 20:48:32 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 20:48:34,756 -> Starting session,1.96,0
2017-12-09 20:48:34,757 -> Setting variables,0.00,0
2017-12-09 20:48:40,119 -> Reading datasets,5.36,0
2017-12-09 20:48:40,133 -> Points partitions: 2
2017-12-09 20:48:40,144 -> Centers partitions: 2
2017-12-09 20:48:46,308 -> 01.Indexing points,6.12,19715,30.0,21
2017-12-09 20:48:51,371 -> 02.Indexing centers,5.06,72374,30.0,21
2017-12-09 20:48:51,378 -> 992
2017-12-09 20:48:51,384 -> 1024
2017-12-09 20:49:03,253 -> 03.Joining datasets,11.87,72372,30.0,21
Done!!! Sat Dec  9 20:49:03 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 20:49:06,218 -> Starting session,1.63,0
2017-12-09 20:49:06,218 -> Setting variables,0.00,0
2017-12-09 20:49:12,090 -> Reading datasets,5.87,0
2017-12-09 20:49:12,101 -> Points partitions: 2
2017-12-09 20:49:12,109 -> Centers partitions: 2
2017-12-09 20:49:19,908 -> 01.Indexing points,7.76,19715,30.0,21
2017-12-09 20:49:24,551 -> 02.Indexing centers,4.64,72374,30.0,21
2017-12-09 20:49:24,563 -> 992
2017-12-09 20:49:24,573 -> 1024
2017-12-09 20:49:35,307 -> 03.Joining datasets,10.73,72372,30.0,21
Done!!! Sat Dec  9 20:49:35 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 20:49:38,092 -> Starting session,1.66,0
2017-12-09 20:49:38,093 -> Setting variables,0.00,0
2017-12-09 20:49:44,054 -> Reading datasets,5.96,0
2017-12-09 20:49:44,064 -> Points partitions: 2
2017-12-09 20:49:44,070 -> Centers partitions: 2
2017-12-09 20:49:52,023 -> 01.Indexing points,7.92,19715,30.0,21
2017-12-09 20:49:56,660 -> 02.Indexing centers,4.64,72374,30.0,21
2017-12-09 20:49:56,672 -> 992
2017-12-09 20:49:56,681 -> 1024
2017-12-09 20:50:09,193 -> 03.Joining datasets,12.51,72372,30.0,21
Done!!! Sat Dec  9 20:50:09 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 20:50:12,028 -> Starting session,1.63,0
2017-12-09 20:50:12,029 -> Setting variables,0.00,0
2017-12-09 20:50:17,845 -> Reading datasets,5.82,0
2017-12-09 20:50:17,860 -> Points partitions: 2
2017-12-09 20:50:17,871 -> Centers partitions: 2
2017-12-09 20:50:25,323 -> 01.Indexing points,7.41,19715,30.0,21
2017-12-09 20:50:29,701 -> 02.Indexing centers,4.38,72374,30.0,21
2017-12-09 20:50:29,708 -> 992
2017-12-09 20:50:29,714 -> 1024
2017-12-09 20:50:39,999 -> 03.Joining datasets,10.28,72372,30.0,21
Done!!! Sat Dec  9 20:50:41 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 20:50:49,394 -> Starting session,1.97,0
2017-12-09 20:50:49,394 -> Setting variables,0.00,0
2017-12-09 20:50:56,387 -> Reading datasets,6.99,0
2017-12-09 20:50:56,401 -> Points partitions: 2
2017-12-09 20:50:56,411 -> Centers partitions: 2
2017-12-09 20:51:05,312 -> 01.Indexing points,8.86,19715,30.0,28
2017-12-09 20:51:10,913 -> 02.Indexing centers,5.60,72374,30.0,28
2017-12-09 20:51:10,921 -> 992
2017-12-09 20:51:10,926 -> 1024
2017-12-09 20:51:21,521 -> 03.Joining datasets,10.60,72372,30.0,28
Done!!! Sat Dec  9 20:51:22 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 20:51:24,312 -> Starting session,1.67,0
2017-12-09 20:51:24,313 -> Setting variables,0.00,0
2017-12-09 20:51:31,319 -> Reading datasets,7.01,0
2017-12-09 20:51:31,333 -> Points partitions: 2
2017-12-09 20:51:31,344 -> Centers partitions: 2
2017-12-09 20:51:37,735 -> 01.Indexing points,6.34,19715,30.0,28
2017-12-09 20:51:42,792 -> 02.Indexing centers,5.06,72374,30.0,28
2017-12-09 20:51:42,802 -> 992
2017-12-09 20:51:42,809 -> 1024
2017-12-09 20:51:53,290 -> 03.Joining datasets,10.48,72372,30.0,28
Done!!! Sat Dec  9 20:51:53 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 20:51:56,293 -> Starting session,1.76,0
2017-12-09 20:51:56,293 -> Setting variables,0.00,0
2017-12-09 20:52:02,412 -> Reading datasets,6.12,0
2017-12-09 20:52:02,422 -> Points partitions: 2
2017-12-09 20:52:02,429 -> Centers partitions: 2
2017-12-09 20:52:11,933 -> 01.Indexing points,9.47,19715,30.0,28
2017-12-09 20:52:18,044 -> 02.Indexing centers,6.11,72374,30.0,28
2017-12-09 20:52:18,056 -> 992
2017-12-09 20:52:18,089 -> 1024
2017-12-09 20:52:29,029 -> 03.Joining datasets,10.94,72372,30.0,28
Done!!! Sat Dec  9 20:52:29 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 20:52:31,898 -> Starting session,1.80,0
2017-12-09 20:52:31,901 -> Setting variables,0.00,0
2017-12-09 20:52:38,681 -> Reading datasets,6.78,0
2017-12-09 20:52:38,692 -> Points partitions: 2
2017-12-09 20:52:38,699 -> Centers partitions: 2
2017-12-09 20:52:47,673 -> 01.Indexing points,8.93,19715,30.0,28
2017-12-09 20:52:53,441 -> 02.Indexing centers,5.77,72374,30.0,28
2017-12-09 20:52:53,451 -> 992
2017-12-09 20:52:53,457 -> 1024
2017-12-09 20:53:04,136 -> 03.Joining datasets,10.68,72372,30.0,28
Done!!! Sat Dec  9 20:53:05 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 20:53:07,672 -> Starting session,1.55,0
2017-12-09 20:53:07,673 -> Setting variables,0.00,0
2017-12-09 20:53:14,647 -> Reading datasets,6.97,0
2017-12-09 20:53:14,657 -> Points partitions: 2
2017-12-09 20:53:14,664 -> Centers partitions: 2
2017-12-09 20:53:20,752 -> 01.Indexing points,6.05,19715,30.0,28
2017-12-09 20:53:25,188 -> 02.Indexing centers,4.44,72374,30.0,28
2017-12-09 20:53:25,196 -> 992
2017-12-09 20:53:25,202 -> 1024
2017-12-09 20:53:35,894 -> 03.Joining datasets,10.69,72372,30.0,28
Done!!! Sat Dec  9 20:53:36 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 20:53:44,156 -> Starting session,1.56,0
2017-12-09 20:53:44,156 -> Setting variables,0.00,0
2017-12-09 20:53:48,248 -> Reading datasets,4.09,0
2017-12-09 20:53:48,259 -> Points partitions: 2
2017-12-09 20:53:48,266 -> Centers partitions: 2
2017-12-09 20:53:55,817 -> 01.Indexing points,7.52,78857,20.0,7
2017-12-09 20:54:01,196 -> 02.Indexing centers,5.38,190656,20.0,7
2017-12-09 20:54:01,204 -> 1024
2017-12-09 20:54:01,209 -> 1024
2017-12-09 20:54:18,319 -> 03.Joining datasets,17.11,190652,20.0,7
Done!!! Sat Dec  9 20:54:18 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 20:54:20,734 -> Starting session,1.43,0
2017-12-09 20:54:20,735 -> Setting variables,0.00,0
2017-12-09 20:54:24,982 -> Reading datasets,4.25,0
2017-12-09 20:54:24,995 -> Points partitions: 2
2017-12-09 20:54:25,005 -> Centers partitions: 2
2017-12-09 20:54:33,164 -> 01.Indexing points,8.12,78857,20.0,7
2017-12-09 20:54:38,945 -> 02.Indexing centers,5.78,190656,20.0,7
2017-12-09 20:54:38,955 -> 1024
2017-12-09 20:54:38,962 -> 1024
2017-12-09 20:54:57,118 -> 03.Joining datasets,18.24,190652,20.0,7
Done!!! Sat Dec  9 20:54:57 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 20:54:59,517 -> Starting session,1.40,0
2017-12-09 20:54:59,518 -> Setting variables,0.00,0
2017-12-09 20:55:03,709 -> Reading datasets,4.29,0
2017-12-09 20:55:03,720 -> Points partitions: 2
2017-12-09 20:55:03,727 -> Centers partitions: 2
2017-12-09 20:55:11,416 -> 01.Indexing points,7.65,78857,20.0,7
2017-12-09 20:55:16,742 -> 02.Indexing centers,5.32,190656,20.0,7
2017-12-09 20:55:16,751 -> 1024
2017-12-09 20:55:16,759 -> 1024
2017-12-09 20:55:33,798 -> 03.Joining datasets,17.04,190652,20.0,7
Done!!! Sat Dec  9 20:55:34 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 20:55:36,363 -> Starting session,1.56,0
2017-12-09 20:55:36,364 -> Setting variables,0.00,0
2017-12-09 20:55:40,780 -> Reading datasets,4.42,0
2017-12-09 20:55:40,791 -> Points partitions: 2
2017-12-09 20:55:40,798 -> Centers partitions: 2
2017-12-09 20:55:48,361 -> 01.Indexing points,7.53,78857,20.0,7
2017-12-09 20:55:53,649 -> 02.Indexing centers,5.29,190656,20.0,7
2017-12-09 20:55:53,660 -> 1024
2017-12-09 20:55:53,665 -> 1024
2017-12-09 20:56:10,566 -> 03.Joining datasets,16.90,190652,20.0,7
Done!!! Sat Dec  9 20:56:10 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 20:56:13,049 -> Starting session,1.52,0
2017-12-09 20:56:13,050 -> Setting variables,0.00,0
2017-12-09 20:56:17,312 -> Reading datasets,4.26,0
2017-12-09 20:56:17,322 -> Points partitions: 2
2017-12-09 20:56:17,330 -> Centers partitions: 2
2017-12-09 20:56:24,928 -> 01.Indexing points,7.56,78857,20.0,7
2017-12-09 20:56:30,758 -> 02.Indexing centers,5.83,190656,20.0,7
2017-12-09 20:56:30,765 -> 1024
2017-12-09 20:56:30,771 -> 1024
2017-12-09 20:56:48,529 -> 03.Joining datasets,17.76,190652,20.0,7
Done!!! Sat Dec  9 20:56:48 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 20:56:56,668 -> Starting session,1.73,0
2017-12-09 20:56:56,669 -> Setting variables,0.00,0
2017-12-09 20:57:01,782 -> Reading datasets,5.11,0
2017-12-09 20:57:01,796 -> Points partitions: 2
2017-12-09 20:57:01,807 -> Centers partitions: 2
2017-12-09 20:57:09,777 -> 01.Indexing points,7.93,78857,20.0,14
2017-12-09 20:57:15,324 -> 02.Indexing centers,5.55,190656,20.0,14
2017-12-09 20:57:15,333 -> 1024
2017-12-09 20:57:15,339 -> 1024
2017-12-09 20:57:30,343 -> 03.Joining datasets,15.00,190652,20.0,14
Done!!! Sat Dec  9 20:57:30 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 20:57:33,233 -> Starting session,1.67,0
2017-12-09 20:57:33,233 -> Setting variables,0.00,0
2017-12-09 20:57:38,270 -> Reading datasets,5.04,0
2017-12-09 20:57:38,283 -> Points partitions: 2
2017-12-09 20:57:38,293 -> Centers partitions: 2
2017-12-09 20:57:46,826 -> 01.Indexing points,8.49,78857,20.0,14
2017-12-09 20:57:52,659 -> 02.Indexing centers,5.83,190656,20.0,14
2017-12-09 20:57:52,670 -> 1024
2017-12-09 20:57:52,678 -> 1024
2017-12-09 20:58:06,754 -> 03.Joining datasets,14.08,190652,20.0,14
Done!!! Sat Dec  9 20:58:07 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 20:58:09,698 -> Starting session,1.69,0
2017-12-09 20:58:09,699 -> Setting variables,0.00,0
2017-12-09 20:58:14,818 -> Reading datasets,5.12,0
2017-12-09 20:58:14,831 -> Points partitions: 2
2017-12-09 20:58:14,841 -> Centers partitions: 2
2017-12-09 20:58:23,103 -> 01.Indexing points,8.22,78857,20.0,14
2017-12-09 20:58:29,005 -> 02.Indexing centers,5.90,190656,20.0,14
2017-12-09 20:58:29,014 -> 1024
2017-12-09 20:58:29,021 -> 1024
2017-12-09 20:58:43,460 -> 03.Joining datasets,14.44,190652,20.0,14
Done!!! Sat Dec  9 20:58:44 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 20:58:46,596 -> Starting session,1.87,0
2017-12-09 20:58:46,596 -> Setting variables,0.00,0
2017-12-09 20:58:51,878 -> Reading datasets,5.28,0
2017-12-09 20:58:51,890 -> Points partitions: 2
2017-12-09 20:58:51,898 -> Centers partitions: 2
2017-12-09 20:59:00,042 -> 01.Indexing points,8.10,78857,20.0,14
2017-12-09 20:59:05,875 -> 02.Indexing centers,5.83,190656,20.0,14
2017-12-09 20:59:05,883 -> 1024
2017-12-09 20:59:05,888 -> 1024
2017-12-09 20:59:20,689 -> 03.Joining datasets,14.80,190652,20.0,14
Done!!! Sat Dec  9 20:59:21 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 20:59:23,600 -> Starting session,1.67,0
2017-12-09 20:59:23,601 -> Setting variables,0.00,0
2017-12-09 20:59:28,512 -> Reading datasets,4.91,0
2017-12-09 20:59:28,523 -> Points partitions: 2
2017-12-09 20:59:28,530 -> Centers partitions: 2
2017-12-09 20:59:36,961 -> 01.Indexing points,8.39,78857,20.0,14
2017-12-09 20:59:43,080 -> 02.Indexing centers,6.12,190656,20.0,14
2017-12-09 20:59:43,087 -> 1024
2017-12-09 20:59:43,093 -> 1024
2017-12-09 20:59:57,730 -> 03.Joining datasets,14.64,190652,20.0,14
Done!!! Sat Dec  9 20:59:58 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:00:05,981 -> Starting session,1.76,0
2017-12-09 21:00:05,982 -> Setting variables,0.00,0
2017-12-09 21:00:12,044 -> Reading datasets,6.06,0
2017-12-09 21:00:12,054 -> Points partitions: 2
2017-12-09 21:00:12,061 -> Centers partitions: 2
2017-12-09 21:00:19,580 -> 01.Indexing points,7.48,78857,20.0,21
2017-12-09 21:00:24,640 -> 02.Indexing centers,5.06,190656,20.0,21
2017-12-09 21:00:24,648 -> 1024
2017-12-09 21:00:24,654 -> 1024
2017-12-09 21:00:35,044 -> 03.Joining datasets,10.39,190652,20.0,21
Done!!! Sat Dec  9 21:00:35 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:00:37,989 -> Starting session,1.82,0
2017-12-09 21:00:37,989 -> Setting variables,0.00,0
2017-12-09 21:00:43,847 -> Reading datasets,5.86,0
2017-12-09 21:00:43,892 -> Points partitions: 2
2017-12-09 21:00:43,904 -> Centers partitions: 2
2017-12-09 21:00:51,046 -> 01.Indexing points,7.09,78857,20.0,21
2017-12-09 21:00:56,201 -> 02.Indexing centers,5.15,190656,20.0,21
2017-12-09 21:00:56,212 -> 1024
2017-12-09 21:00:56,219 -> 1024
2017-12-09 21:01:07,495 -> 03.Joining datasets,11.28,190652,20.0,21
Done!!! Sat Dec  9 21:01:07 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:01:10,166 -> Starting session,1.61,0
2017-12-09 21:01:10,167 -> Setting variables,0.00,0
2017-12-09 21:01:15,185 -> Reading datasets,5.02,0
2017-12-09 21:01:15,197 -> Points partitions: 2
2017-12-09 21:01:15,207 -> Centers partitions: 2
2017-12-09 21:01:23,517 -> 01.Indexing points,8.26,78857,20.0,21
2017-12-09 21:01:28,707 -> 02.Indexing centers,5.19,190656,20.0,21
2017-12-09 21:01:28,718 -> 1024
2017-12-09 21:01:28,729 -> 1024
2017-12-09 21:01:40,894 -> 03.Joining datasets,12.16,190652,20.0,21
Done!!! Sat Dec  9 21:01:41 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:01:44,030 -> Starting session,1.72,0
2017-12-09 21:01:44,031 -> Setting variables,0.00,0
2017-12-09 21:01:49,023 -> Reading datasets,4.99,0
2017-12-09 21:01:49,033 -> Points partitions: 2
2017-12-09 21:01:49,041 -> Centers partitions: 2
2017-12-09 21:01:57,474 -> 01.Indexing points,8.39,78857,20.0,21
2017-12-09 21:02:02,432 -> 02.Indexing centers,4.96,190656,20.0,21
2017-12-09 21:02:02,441 -> 1024
2017-12-09 21:02:02,448 -> 1024
2017-12-09 21:02:13,857 -> 03.Joining datasets,11.41,190652,20.0,21
Done!!! Sat Dec  9 21:02:15 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:02:17,504 -> Starting session,1.65,0
2017-12-09 21:02:17,505 -> Setting variables,0.00,0
2017-12-09 21:02:22,486 -> Reading datasets,4.98,0
2017-12-09 21:02:22,499 -> Points partitions: 2
2017-12-09 21:02:22,509 -> Centers partitions: 2
2017-12-09 21:02:30,707 -> 01.Indexing points,8.15,78857,20.0,21
2017-12-09 21:02:35,412 -> 02.Indexing centers,4.70,190656,20.0,21
2017-12-09 21:02:35,423 -> 1024
2017-12-09 21:02:35,431 -> 1024
2017-12-09 21:02:47,857 -> 03.Joining datasets,12.43,190652,20.0,21
Done!!! Sat Dec  9 21:02:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:02:56,336 -> Starting session,1.86,0
2017-12-09 21:02:56,337 -> Setting variables,0.00,0
2017-12-09 21:03:02,379 -> Reading datasets,6.04,0
2017-12-09 21:03:02,390 -> Points partitions: 2
2017-12-09 21:03:02,397 -> Centers partitions: 2
2017-12-09 21:03:11,960 -> 01.Indexing points,9.53,78857,20.0,28
2017-12-09 21:03:16,509 -> 02.Indexing centers,4.55,190656,20.0,28
2017-12-09 21:03:16,519 -> 1024
2017-12-09 21:03:16,528 -> 1024
2017-12-09 21:03:26,782 -> 03.Joining datasets,10.25,190652,20.0,28
Done!!! Sat Dec  9 21:03:27 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:03:29,658 -> Starting session,1.74,0
2017-12-09 21:03:29,659 -> Setting variables,0.00,0
2017-12-09 21:03:35,736 -> Reading datasets,6.08,0
2017-12-09 21:03:35,751 -> Points partitions: 2
2017-12-09 21:03:35,762 -> Centers partitions: 2
2017-12-09 21:03:45,736 -> 01.Indexing points,9.93,78857,20.0,28
2017-12-09 21:03:50,988 -> 02.Indexing centers,5.25,190656,20.0,28
2017-12-09 21:03:50,997 -> 1024
2017-12-09 21:03:51,005 -> 1024
2017-12-09 21:04:01,642 -> 03.Joining datasets,10.64,190652,20.0,28
Done!!! Sat Dec  9 21:04:02 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:04:04,522 -> Starting session,1.70,0
2017-12-09 21:04:04,522 -> Setting variables,0.00,0
2017-12-09 21:04:11,762 -> Reading datasets,7.24,0
2017-12-09 21:04:11,776 -> Points partitions: 2
2017-12-09 21:04:11,785 -> Centers partitions: 2
2017-12-09 21:04:20,426 -> 01.Indexing points,8.60,78857,20.0,28
2017-12-09 21:04:25,506 -> 02.Indexing centers,4.68,190656,20.0,28
2017-12-09 21:04:25,516 -> 1024
2017-12-09 21:04:25,523 -> 1024
2017-12-09 21:04:36,145 -> 03.Joining datasets,10.62,190652,20.0,28
Done!!! Sat Dec  9 21:04:36 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 21:04:38,835 -> Starting session,1.57,0
2017-12-09 21:04:38,835 -> Setting variables,0.00,0
2017-12-09 21:04:44,769 -> Reading datasets,5.93,0
2017-12-09 21:04:44,779 -> Points partitions: 2
2017-12-09 21:04:44,786 -> Centers partitions: 2
2017-12-09 21:04:55,083 -> 01.Indexing points,10.26,78857,20.0,28
2017-12-09 21:05:00,838 -> 02.Indexing centers,5.75,190656,20.0,28
2017-12-09 21:05:00,847 -> 1024
2017-12-09 21:05:00,854 -> 1024
2017-12-09 21:05:10,755 -> 03.Joining datasets,9.90,190652,20.0,28
Done!!! Sat Dec  9 21:05:11 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 21:05:13,506 -> Starting session,1.51,0
2017-12-09 21:05:13,507 -> Setting variables,0.00,0
2017-12-09 21:05:18,684 -> Reading datasets,5.18,0
2017-12-09 21:05:18,697 -> Points partitions: 2
2017-12-09 21:05:18,707 -> Centers partitions: 2
2017-12-09 21:05:28,440 -> 01.Indexing points,9.69,78857,20.0,28
2017-12-09 21:05:33,762 -> 02.Indexing centers,5.32,190656,20.0,28
2017-12-09 21:05:33,773 -> 1024
2017-12-09 21:05:33,782 -> 1024
2017-12-09 21:05:43,961 -> 03.Joining datasets,10.18,190652,20.0,28
Done!!! Sat Dec  9 21:05:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 21:05:52,209 -> Starting session,1.54,0
2017-12-09 21:05:52,210 -> Setting variables,0.00,0
2017-12-09 21:05:56,306 -> Reading datasets,4.10,0
2017-12-09 21:05:56,317 -> Points partitions: 2
2017-12-09 21:05:56,324 -> Centers partitions: 2
2017-12-09 21:06:04,153 -> 01.Indexing points,7.79,59143,20.0,7
2017-12-09 21:06:09,527 -> 02.Indexing centers,5.37,142992,20.0,7
2017-12-09 21:06:09,534 -> 1024
2017-12-09 21:06:09,540 -> 1024
2017-12-09 21:06:26,156 -> 03.Joining datasets,16.62,142988,20.0,7
Done!!! Sat Dec  9 21:06:26 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 21:06:28,576 -> Starting session,1.40,0
2017-12-09 21:06:28,576 -> Setting variables,0.00,0
2017-12-09 21:06:32,759 -> Reading datasets,4.18,0
2017-12-09 21:06:32,772 -> Points partitions: 2
2017-12-09 21:06:32,782 -> Centers partitions: 2
2017-12-09 21:06:40,114 -> 01.Indexing points,7.29,59143,20.0,7
2017-12-09 21:06:45,239 -> 02.Indexing centers,5.12,142992,20.0,7
2017-12-09 21:06:45,247 -> 1024
2017-12-09 21:06:45,253 -> 1024
2017-12-09 21:07:01,600 -> 03.Joining datasets,16.35,142988,20.0,7
Done!!! Sat Dec  9 21:07:02 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 21:07:04,161 -> Starting session,1.53,0
2017-12-09 21:07:04,161 -> Setting variables,0.00,0
2017-12-09 21:07:08,501 -> Reading datasets,4.34,0
2017-12-09 21:07:08,514 -> Points partitions: 2
2017-12-09 21:07:08,523 -> Centers partitions: 2
2017-12-09 21:07:16,165 -> 01.Indexing points,7.60,59143,20.0,7
2017-12-09 21:07:21,364 -> 02.Indexing centers,5.20,142992,20.0,7
2017-12-09 21:07:21,372 -> 1024
2017-12-09 21:07:21,378 -> 1024
2017-12-09 21:07:37,204 -> 03.Joining datasets,15.83,142988,20.0,7
Done!!! Sat Dec  9 21:07:37 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 21:07:39,882 -> Starting session,1.66,0
2017-12-09 21:07:39,883 -> Setting variables,0.00,0
2017-12-09 21:07:43,996 -> Reading datasets,4.11,0
2017-12-09 21:07:44,008 -> Points partitions: 2
2017-12-09 21:07:44,017 -> Centers partitions: 2
2017-12-09 21:07:51,543 -> 01.Indexing points,7.48,59143,20.0,7
2017-12-09 21:07:56,661 -> 02.Indexing centers,5.12,142992,20.0,7
2017-12-09 21:07:56,668 -> 1024
2017-12-09 21:07:56,674 -> 1024
2017-12-09 21:08:12,780 -> 03.Joining datasets,16.11,142988,20.0,7
Done!!! Sat Dec  9 21:08:13 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 21:08:15,322 -> Starting session,1.54,0
2017-12-09 21:08:15,323 -> Setting variables,0.00,0
2017-12-09 21:08:19,457 -> Reading datasets,4.13,0
2017-12-09 21:08:19,472 -> Points partitions: 2
2017-12-09 21:08:19,483 -> Centers partitions: 2
2017-12-09 21:08:26,901 -> 01.Indexing points,7.37,59143,20.0,7
2017-12-09 21:08:32,023 -> 02.Indexing centers,5.12,142992,20.0,7
2017-12-09 21:08:32,031 -> 1024
2017-12-09 21:08:32,036 -> 1024
2017-12-09 21:08:48,490 -> 03.Joining datasets,16.45,142988,20.0,7
Done!!! Sat Dec  9 21:08:48 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 21:08:56,754 -> Starting session,1.89,0
2017-12-09 21:08:56,755 -> Setting variables,0.00,0
2017-12-09 21:09:01,855 -> Reading datasets,5.10,0
2017-12-09 21:09:01,867 -> Points partitions: 2
2017-12-09 21:09:01,874 -> Centers partitions: 2
2017-12-09 21:09:09,956 -> 01.Indexing points,8.04,59143,20.0,14
2017-12-09 21:09:15,319 -> 02.Indexing centers,5.36,142992,20.0,14
2017-12-09 21:09:15,329 -> 1024
2017-12-09 21:09:15,338 -> 1024
2017-12-09 21:09:29,049 -> 03.Joining datasets,13.71,142988,20.0,14
Done!!! Sat Dec  9 21:09:29 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 21:09:31,815 -> Starting session,1.50,0
2017-12-09 21:09:31,816 -> Setting variables,0.00,0
2017-12-09 21:09:36,706 -> Reading datasets,4.89,0
2017-12-09 21:09:36,717 -> Points partitions: 2
2017-12-09 21:09:36,727 -> Centers partitions: 2
2017-12-09 21:09:44,806 -> 01.Indexing points,8.03,59143,20.0,14
2017-12-09 21:09:50,222 -> 02.Indexing centers,5.41,142992,20.0,14
2017-12-09 21:09:50,234 -> 1024
2017-12-09 21:09:50,244 -> 1024
2017-12-09 21:10:05,132 -> 03.Joining datasets,14.89,142988,20.0,14
Done!!! Sat Dec  9 21:10:05 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 21:10:08,026 -> Starting session,1.76,0
2017-12-09 21:10:08,027 -> Setting variables,0.00,0
2017-12-09 21:10:12,985 -> Reading datasets,4.96,0
2017-12-09 21:10:12,999 -> Points partitions: 2
2017-12-09 21:10:13,009 -> Centers partitions: 2
2017-12-09 21:10:20,968 -> 01.Indexing points,7.92,59143,20.0,14
2017-12-09 21:10:26,310 -> 02.Indexing centers,5.34,142992,20.0,14
2017-12-09 21:10:26,318 -> 1024
2017-12-09 21:10:26,324 -> 1024
2017-12-09 21:10:40,263 -> 03.Joining datasets,13.94,142988,20.0,14
Done!!! Sat Dec  9 21:10:40 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 21:10:43,109 -> Starting session,1.61,0
2017-12-09 21:10:43,109 -> Setting variables,0.00,0
2017-12-09 21:10:48,283 -> Reading datasets,5.17,0
2017-12-09 21:10:48,297 -> Points partitions: 2
2017-12-09 21:10:48,308 -> Centers partitions: 2
2017-12-09 21:10:56,368 -> 01.Indexing points,8.02,59143,20.0,14
2017-12-09 21:11:01,462 -> 02.Indexing centers,5.09,142992,20.0,14
2017-12-09 21:11:01,469 -> 1024
2017-12-09 21:11:01,475 -> 1024
2017-12-09 21:11:15,405 -> 03.Joining datasets,13.93,142988,20.0,14
Done!!! Sat Dec  9 21:11:16 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 21:11:19,027 -> Starting session,1.64,0
2017-12-09 21:11:19,028 -> Setting variables,0.00,0
2017-12-09 21:11:24,163 -> Reading datasets,5.14,0
2017-12-09 21:11:24,177 -> Points partitions: 2
2017-12-09 21:11:24,185 -> Centers partitions: 2
2017-12-09 21:11:32,053 -> 01.Indexing points,7.83,59143,20.0,14
2017-12-09 21:11:37,266 -> 02.Indexing centers,5.21,142992,20.0,14
2017-12-09 21:11:37,274 -> 1024
2017-12-09 21:11:37,280 -> 1024
2017-12-09 21:11:51,342 -> 03.Joining datasets,14.06,142988,20.0,14
Done!!! Sat Dec  9 21:11:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:11:59,580 -> Starting session,1.72,0
2017-12-09 21:11:59,581 -> Setting variables,0.00,0
2017-12-09 21:12:04,456 -> Reading datasets,4.87,0
2017-12-09 21:12:04,470 -> Points partitions: 2
2017-12-09 21:12:04,481 -> Centers partitions: 2
2017-12-09 21:12:12,151 -> 01.Indexing points,7.63,59143,20.0,21
2017-12-09 21:12:16,676 -> 02.Indexing centers,4.52,142992,20.0,21
2017-12-09 21:12:16,684 -> 1024
2017-12-09 21:12:16,690 -> 1024
2017-12-09 21:12:27,585 -> 03.Joining datasets,10.89,142988,20.0,21
Done!!! Sat Dec  9 21:12:28 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:12:30,503 -> Starting session,1.68,0
2017-12-09 21:12:30,503 -> Setting variables,0.00,0
2017-12-09 21:12:36,507 -> Reading datasets,6.00,0
2017-12-09 21:12:36,519 -> Points partitions: 2
2017-12-09 21:12:36,527 -> Centers partitions: 2
2017-12-09 21:12:43,471 -> 01.Indexing points,6.90,59143,20.0,21
2017-12-09 21:12:48,730 -> 02.Indexing centers,5.26,142992,20.0,21
2017-12-09 21:12:48,740 -> 1024
2017-12-09 21:12:48,745 -> 1024
2017-12-09 21:13:00,575 -> 03.Joining datasets,11.83,142988,20.0,21
Done!!! Sat Dec  9 21:13:01 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:13:03,320 -> Starting session,1.65,0
2017-12-09 21:13:03,320 -> Setting variables,0.00,0
2017-12-09 21:13:09,080 -> Reading datasets,5.76,0
2017-12-09 21:13:09,093 -> Points partitions: 2
2017-12-09 21:13:09,102 -> Centers partitions: 2
2017-12-09 21:13:16,329 -> 01.Indexing points,7.18,59143,20.0,21
2017-12-09 21:13:20,962 -> 02.Indexing centers,4.63,142992,20.0,21
2017-12-09 21:13:20,972 -> 1024
2017-12-09 21:13:20,985 -> 1024
2017-12-09 21:13:32,005 -> 03.Joining datasets,11.02,142988,20.0,21
Done!!! Sat Dec  9 21:13:32 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:13:34,871 -> Starting session,1.64,0
2017-12-09 21:13:34,872 -> Setting variables,0.00,0
2017-12-09 21:13:40,287 -> Reading datasets,5.42,0
2017-12-09 21:13:40,301 -> Points partitions: 2
2017-12-09 21:13:40,312 -> Centers partitions: 2
2017-12-09 21:13:48,347 -> 01.Indexing points,7.96,59143,20.0,21
2017-12-09 21:13:52,841 -> 02.Indexing centers,4.49,142992,20.0,21
2017-12-09 21:13:52,848 -> 1024
2017-12-09 21:13:52,854 -> 1024
2017-12-09 21:14:03,230 -> 03.Joining datasets,10.48,142988,20.0,21
Done!!! Sat Dec  9 21:14:03 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:14:06,174 -> Starting session,1.69,0
2017-12-09 21:14:06,175 -> Setting variables,0.00,0
2017-12-09 21:14:11,994 -> Reading datasets,5.82,0
2017-12-09 21:14:12,007 -> Points partitions: 2
2017-12-09 21:14:12,018 -> Centers partitions: 2
2017-12-09 21:14:18,897 -> 01.Indexing points,6.83,59143,20.0,21
2017-12-09 21:14:23,237 -> 02.Indexing centers,4.34,142992,20.0,21
2017-12-09 21:14:23,248 -> 1024
2017-12-09 21:14:23,256 -> 1024
2017-12-09 21:14:33,924 -> 03.Joining datasets,10.67,142988,20.0,21
Done!!! Sat Dec  9 21:14:34 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:14:42,367 -> Starting session,1.86,0
2017-12-09 21:14:42,369 -> Setting variables,0.00,0
2017-12-09 21:14:48,615 -> Reading datasets,6.25,0
2017-12-09 21:14:48,625 -> Points partitions: 2
2017-12-09 21:14:48,633 -> Centers partitions: 2
2017-12-09 21:14:58,114 -> 01.Indexing points,9.44,59143,20.0,28
2017-12-09 21:15:02,956 -> 02.Indexing centers,4.84,142992,20.0,28
2017-12-09 21:15:02,964 -> 1024
2017-12-09 21:15:02,970 -> 1024
2017-12-09 21:15:13,836 -> 03.Joining datasets,10.87,142988,20.0,28
Done!!! Sat Dec  9 21:15:14 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:15:16,613 -> Starting session,1.66,0
2017-12-09 21:15:16,614 -> Setting variables,0.00,0
2017-12-09 21:15:22,497 -> Reading datasets,5.88,0
2017-12-09 21:15:22,510 -> Points partitions: 2
2017-12-09 21:15:22,518 -> Centers partitions: 2
2017-12-09 21:15:30,015 -> 01.Indexing points,7.46,59143,20.0,28
2017-12-09 21:15:34,980 -> 02.Indexing centers,4.96,142992,20.0,28
2017-12-09 21:15:34,990 -> 1024
2017-12-09 21:15:34,998 -> 1024
2017-12-09 21:15:45,493 -> 03.Joining datasets,10.49,142988,20.0,28
Done!!! Sat Dec  9 21:15:46 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:15:49,019 -> Starting session,1.67,0
2017-12-09 21:15:49,020 -> Setting variables,0.00,0
2017-12-09 21:15:53,958 -> Reading datasets,4.94,0
2017-12-09 21:15:53,972 -> Points partitions: 2
2017-12-09 21:15:53,983 -> Centers partitions: 2
2017-12-09 21:16:04,138 -> 01.Indexing points,10.11,59143,20.0,28
2017-12-09 21:16:09,755 -> 02.Indexing centers,5.62,142992,20.0,28
2017-12-09 21:16:09,766 -> 1024
2017-12-09 21:16:09,775 -> 1024
2017-12-09 21:16:19,355 -> 03.Joining datasets,9.58,142988,20.0,28
Done!!! Sat Dec  9 21:16:19 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 21:16:22,191 -> Starting session,1.70,0
2017-12-09 21:16:22,192 -> Setting variables,0.00,0
2017-12-09 21:16:29,270 -> Reading datasets,7.08,0
2017-12-09 21:16:29,284 -> Points partitions: 2
2017-12-09 21:16:29,293 -> Centers partitions: 2
2017-12-09 21:16:37,412 -> 01.Indexing points,8.07,59143,20.0,28
2017-12-09 21:16:43,055 -> 02.Indexing centers,5.64,142992,20.0,28
2017-12-09 21:16:43,066 -> 1024
2017-12-09 21:16:43,076 -> 1024
2017-12-09 21:16:53,498 -> 03.Joining datasets,10.42,142988,20.0,28
Done!!! Sat Dec  9 21:16:53 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 21:16:56,229 -> Starting session,1.63,0
2017-12-09 21:16:56,230 -> Setting variables,0.00,0
2017-12-09 21:17:03,504 -> Reading datasets,7.27,0
2017-12-09 21:17:03,515 -> Points partitions: 2
2017-12-09 21:17:03,522 -> Centers partitions: 2
2017-12-09 21:17:11,929 -> 01.Indexing points,8.37,59143,20.0,28
2017-12-09 21:17:16,601 -> 02.Indexing centers,4.67,142992,20.0,28
2017-12-09 21:17:16,609 -> 1024
2017-12-09 21:17:16,615 -> 1024
2017-12-09 21:17:26,194 -> 03.Joining datasets,9.58,142988,20.0,28
Done!!! Sat Dec  9 21:17:26 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 21:17:34,420 -> Starting session,1.57,0
2017-12-09 21:17:34,420 -> Setting variables,0.00,0
2017-12-09 21:17:38,517 -> Reading datasets,4.10,0
2017-12-09 21:17:38,530 -> Points partitions: 2
2017-12-09 21:17:38,540 -> Centers partitions: 2
2017-12-09 21:17:45,893 -> 01.Indexing points,7.31,39429,20.0,7
2017-12-09 21:17:50,732 -> 02.Indexing centers,4.84,95328,20.0,7
2017-12-09 21:17:50,740 -> 1024
2017-12-09 21:17:50,745 -> 1024
2017-12-09 21:18:05,132 -> 03.Joining datasets,14.39,95325,20.0,7
Done!!! Sat Dec  9 21:18:05 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 21:18:07,549 -> Starting session,1.42,0
2017-12-09 21:18:07,550 -> Setting variables,0.00,0
2017-12-09 21:18:11,673 -> Reading datasets,4.12,0
2017-12-09 21:18:11,686 -> Points partitions: 2
2017-12-09 21:18:11,696 -> Centers partitions: 2
2017-12-09 21:18:19,083 -> 01.Indexing points,7.35,39429,20.0,7
2017-12-09 21:18:24,109 -> 02.Indexing centers,5.03,95328,20.0,7
2017-12-09 21:18:24,118 -> 1024
2017-12-09 21:18:24,124 -> 1024
2017-12-09 21:18:38,880 -> 03.Joining datasets,14.76,95325,20.0,7
Done!!! Sat Dec  9 21:18:39 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 21:18:41,473 -> Starting session,1.56,0
2017-12-09 21:18:41,473 -> Setting variables,0.00,0
2017-12-09 21:18:45,641 -> Reading datasets,4.17,0
2017-12-09 21:18:45,656 -> Points partitions: 2
2017-12-09 21:18:45,668 -> Centers partitions: 2
2017-12-09 21:18:52,840 -> 01.Indexing points,7.13,39429,20.0,7
2017-12-09 21:18:57,741 -> 02.Indexing centers,4.90,95328,20.0,7
2017-12-09 21:18:57,749 -> 1024
2017-12-09 21:18:57,754 -> 1024
2017-12-09 21:19:12,651 -> 03.Joining datasets,14.90,95325,20.0,7
Done!!! Sat Dec  9 21:19:13 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 21:19:15,241 -> Starting session,1.58,0
2017-12-09 21:19:15,241 -> Setting variables,0.00,0
2017-12-09 21:19:19,400 -> Reading datasets,4.16,0
2017-12-09 21:19:19,414 -> Points partitions: 2
2017-12-09 21:19:19,424 -> Centers partitions: 2
2017-12-09 21:19:26,808 -> 01.Indexing points,7.34,39429,20.0,7
2017-12-09 21:19:31,698 -> 02.Indexing centers,5.01,95328,20.0,7
2017-12-09 21:19:31,707 -> 1024
2017-12-09 21:19:31,714 -> 1024
2017-12-09 21:19:46,744 -> 03.Joining datasets,15.03,95325,20.0,7
Done!!! Sat Dec  9 21:19:47 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 21:19:49,254 -> Starting session,1.49,0
2017-12-09 21:19:49,255 -> Setting variables,0.00,0
2017-12-09 21:19:53,502 -> Reading datasets,4.25,0
2017-12-09 21:19:53,513 -> Points partitions: 2
2017-12-09 21:19:53,520 -> Centers partitions: 2
2017-12-09 21:20:00,982 -> 01.Indexing points,7.43,39429,20.0,7
2017-12-09 21:20:05,919 -> 02.Indexing centers,4.94,95328,20.0,7
2017-12-09 21:20:05,927 -> 1024
2017-12-09 21:20:05,933 -> 1024
2017-12-09 21:20:21,225 -> 03.Joining datasets,15.29,95325,20.0,7
Done!!! Sat Dec  9 21:20:21 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 21:20:29,454 -> Starting session,1.81,0
2017-12-09 21:20:29,455 -> Setting variables,0.00,0
2017-12-09 21:20:34,329 -> Reading datasets,4.87,0
2017-12-09 21:20:34,342 -> Points partitions: 2
2017-12-09 21:20:34,352 -> Centers partitions: 2
2017-12-09 21:20:41,921 -> 01.Indexing points,7.53,39429,20.0,14
2017-12-09 21:20:47,033 -> 02.Indexing centers,5.11,95328,20.0,14
2017-12-09 21:20:47,043 -> 1024
2017-12-09 21:20:47,052 -> 1024
2017-12-09 21:20:58,987 -> 03.Joining datasets,11.94,95325,20.0,14
Done!!! Sat Dec  9 21:20:59 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 21:21:01,808 -> Starting session,1.74,0
2017-12-09 21:21:01,809 -> Setting variables,0.00,0
2017-12-09 21:21:06,686 -> Reading datasets,4.88,0
2017-12-09 21:21:06,698 -> Points partitions: 2
2017-12-09 21:21:06,706 -> Centers partitions: 2
2017-12-09 21:21:14,156 -> 01.Indexing points,7.41,39429,20.0,14
2017-12-09 21:21:18,931 -> 02.Indexing centers,4.77,95328,20.0,14
2017-12-09 21:21:18,938 -> 1024
2017-12-09 21:21:18,945 -> 1024
2017-12-09 21:21:32,284 -> 03.Joining datasets,13.34,95325,20.0,14
Done!!! Sat Dec  9 21:21:32 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 21:21:35,087 -> Starting session,1.73,0
2017-12-09 21:21:35,087 -> Setting variables,0.00,0
2017-12-09 21:21:39,890 -> Reading datasets,4.80,0
2017-12-09 21:21:39,904 -> Points partitions: 2
2017-12-09 21:21:39,914 -> Centers partitions: 2
2017-12-09 21:21:47,547 -> 01.Indexing points,7.59,39429,20.0,14
2017-12-09 21:21:52,648 -> 02.Indexing centers,5.10,95328,20.0,14
2017-12-09 21:21:52,659 -> 1024
2017-12-09 21:21:52,668 -> 1024
2017-12-09 21:22:06,545 -> 03.Joining datasets,13.88,95325,20.0,14
Done!!! Sat Dec  9 21:22:06 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 21:22:09,576 -> Starting session,1.85,0
2017-12-09 21:22:09,577 -> Setting variables,0.00,0
2017-12-09 21:22:14,291 -> Reading datasets,4.71,0
2017-12-09 21:22:14,302 -> Points partitions: 2
2017-12-09 21:22:14,309 -> Centers partitions: 2
2017-12-09 21:22:21,788 -> 01.Indexing points,7.44,39429,20.0,14
2017-12-09 21:22:26,798 -> 02.Indexing centers,5.01,95328,20.0,14
2017-12-09 21:22:26,808 -> 1024
2017-12-09 21:22:26,817 -> 1024
2017-12-09 21:22:39,674 -> 03.Joining datasets,12.86,95325,20.0,14
Done!!! Sat Dec  9 21:22:40 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 21:22:42,533 -> Starting session,1.61,0
2017-12-09 21:22:42,534 -> Setting variables,0.00,0
2017-12-09 21:22:47,333 -> Reading datasets,4.80,0
2017-12-09 21:22:47,343 -> Points partitions: 2
2017-12-09 21:22:47,351 -> Centers partitions: 2
2017-12-09 21:22:55,100 -> 01.Indexing points,7.71,39429,20.0,14
2017-12-09 21:23:00,593 -> 02.Indexing centers,5.49,95328,20.0,14
2017-12-09 21:23:00,603 -> 1024
2017-12-09 21:23:00,613 -> 1024
2017-12-09 21:23:14,162 -> 03.Joining datasets,13.55,95325,20.0,14
Done!!! Sat Dec  9 21:23:14 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:23:22,355 -> Starting session,1.57,0
2017-12-09 21:23:22,356 -> Setting variables,0.00,0
2017-12-09 21:23:27,528 -> Reading datasets,5.17,0
2017-12-09 21:23:27,538 -> Points partitions: 2
2017-12-09 21:23:27,545 -> Centers partitions: 2
2017-12-09 21:23:35,591 -> 01.Indexing points,8.01,39429,20.0,21
2017-12-09 21:23:39,689 -> 02.Indexing centers,4.10,95328,20.0,21
2017-12-09 21:23:39,697 -> 1024
2017-12-09 21:23:39,703 -> 1024
2017-12-09 21:23:49,976 -> 03.Joining datasets,10.27,95325,20.0,21
Done!!! Sat Dec  9 21:23:50 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:23:52,912 -> Starting session,1.83,0
2017-12-09 21:23:52,913 -> Setting variables,0.00,0
2017-12-09 21:23:57,699 -> Reading datasets,4.79,0
2017-12-09 21:23:57,712 -> Points partitions: 2
2017-12-09 21:23:57,721 -> Centers partitions: 2
2017-12-09 21:24:05,547 -> 01.Indexing points,7.87,39429,20.0,21
2017-12-09 21:24:09,908 -> 02.Indexing centers,4.36,95328,20.0,21
2017-12-09 21:24:09,916 -> 1024
2017-12-09 21:24:09,924 -> 1024
2017-12-09 21:24:20,563 -> 03.Joining datasets,10.64,95325,20.0,21
Done!!! Sat Dec  9 21:24:21 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:24:24,291 -> Starting session,1.74,0
2017-12-09 21:24:24,292 -> Setting variables,0.00,0
2017-12-09 21:24:30,107 -> Reading datasets,5.82,0
2017-12-09 21:24:30,121 -> Points partitions: 2
2017-12-09 21:24:30,132 -> Centers partitions: 2
2017-12-09 21:24:36,679 -> 01.Indexing points,6.38,39429,20.0,21
2017-12-09 21:24:40,887 -> 02.Indexing centers,4.21,95328,20.0,21
2017-12-09 21:24:40,898 -> 1024
2017-12-09 21:24:40,904 -> 1024
2017-12-09 21:24:51,710 -> 03.Joining datasets,10.80,95325,20.0,21
Done!!! Sat Dec  9 21:24:52 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:24:54,564 -> Starting session,1.64,0
2017-12-09 21:24:54,565 -> Setting variables,0.00,0
2017-12-09 21:24:59,675 -> Reading datasets,5.11,0
2017-12-09 21:24:59,689 -> Points partitions: 2
2017-12-09 21:24:59,699 -> Centers partitions: 2
2017-12-09 21:25:07,705 -> 01.Indexing points,7.96,39429,20.0,21
2017-12-09 21:25:11,918 -> 02.Indexing centers,4.21,95328,20.0,21
2017-12-09 21:25:11,930 -> 1024
2017-12-09 21:25:11,940 -> 1024
2017-12-09 21:25:21,328 -> 03.Joining datasets,9.39,95325,20.0,21
Done!!! Sat Dec  9 21:25:21 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:25:24,169 -> Starting session,1.72,0
2017-12-09 21:25:24,170 -> Setting variables,0.00,0
2017-12-09 21:25:29,095 -> Reading datasets,4.93,0
2017-12-09 21:25:29,110 -> Points partitions: 2
2017-12-09 21:25:29,121 -> Centers partitions: 2
2017-12-09 21:25:36,418 -> 01.Indexing points,7.25,39429,20.0,21
2017-12-09 21:25:40,606 -> 02.Indexing centers,4.19,95328,20.0,21
2017-12-09 21:25:40,618 -> 1024
2017-12-09 21:25:40,627 -> 1024
2017-12-09 21:25:50,496 -> 03.Joining datasets,9.87,95325,20.0,21
Done!!! Sat Dec  9 21:25:51 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:25:59,804 -> Starting session,1.84,0
2017-12-09 21:25:59,804 -> Setting variables,0.00,0
2017-12-09 21:26:06,905 -> Reading datasets,7.10,0
2017-12-09 21:26:06,918 -> Points partitions: 2
2017-12-09 21:26:06,929 -> Centers partitions: 2
2017-12-09 21:26:14,838 -> 01.Indexing points,7.86,39429,20.0,28
2017-12-09 21:26:19,603 -> 02.Indexing centers,4.76,95328,20.0,28
2017-12-09 21:26:19,614 -> 1024
2017-12-09 21:26:19,623 -> 1024
2017-12-09 21:26:28,468 -> 03.Joining datasets,8.84,95325,20.0,28
Done!!! Sat Dec  9 21:26:28 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:26:31,228 -> Starting session,1.53,0
2017-12-09 21:26:31,229 -> Setting variables,0.00,0
2017-12-09 21:26:38,376 -> Reading datasets,7.15,0
2017-12-09 21:26:38,390 -> Points partitions: 2
2017-12-09 21:26:38,399 -> Centers partitions: 2
2017-12-09 21:26:46,800 -> 01.Indexing points,8.36,39429,20.0,28
2017-12-09 21:26:51,168 -> 02.Indexing centers,4.37,95328,20.0,28
2017-12-09 21:26:51,177 -> 1024
2017-12-09 21:26:51,185 -> 1024
2017-12-09 21:27:00,163 -> 03.Joining datasets,8.98,95325,20.0,28
Done!!! Sat Dec  9 21:27:00 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:27:02,872 -> Starting session,1.60,0
2017-12-09 21:27:02,872 -> Setting variables,0.00,0
2017-12-09 21:27:08,928 -> Reading datasets,6.06,0
2017-12-09 21:27:08,940 -> Points partitions: 2
2017-12-09 21:27:08,950 -> Centers partitions: 2
2017-12-09 21:27:18,034 -> 01.Indexing points,9.04,39429,20.0,28
2017-12-09 21:27:22,311 -> 02.Indexing centers,4.28,95328,20.0,28
2017-12-09 21:27:22,320 -> 1024
2017-12-09 21:27:22,326 -> 1024
2017-12-09 21:27:31,671 -> 03.Joining datasets,9.34,95325,20.0,28
Done!!! Sat Dec  9 21:27:32 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 21:27:34,603 -> Starting session,1.65,0
2017-12-09 21:27:34,603 -> Setting variables,0.00,0
2017-12-09 21:27:41,842 -> Reading datasets,7.24,0
2017-12-09 21:27:41,855 -> Points partitions: 2
2017-12-09 21:27:41,866 -> Centers partitions: 2
2017-12-09 21:27:49,070 -> 01.Indexing points,7.16,39429,20.0,28
2017-12-09 21:27:53,635 -> 02.Indexing centers,4.56,95328,20.0,28
2017-12-09 21:27:53,643 -> 1024
2017-12-09 21:27:53,650 -> 1024
2017-12-09 21:28:02,505 -> 03.Joining datasets,8.85,95325,20.0,28
Done!!! Sat Dec  9 21:28:02 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 21:28:05,314 -> Starting session,1.69,0
2017-12-09 21:28:05,314 -> Setting variables,0.00,0
2017-12-09 21:28:11,151 -> Reading datasets,5.84,0
2017-12-09 21:28:11,161 -> Points partitions: 2
2017-12-09 21:28:11,168 -> Centers partitions: 2
2017-12-09 21:28:20,173 -> 01.Indexing points,8.97,39429,20.0,28
2017-12-09 21:28:24,490 -> 02.Indexing centers,4.32,95328,20.0,28
2017-12-09 21:28:24,501 -> 1024
2017-12-09 21:28:24,511 -> 1024
2017-12-09 21:28:33,238 -> 03.Joining datasets,8.73,95325,20.0,28
Done!!! Sat Dec  9 21:28:33 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 21:28:41,500 -> Starting session,1.61,0
2017-12-09 21:28:41,500 -> Setting variables,0.00,0
2017-12-09 21:28:45,607 -> Reading datasets,4.11,0
2017-12-09 21:28:45,621 -> Points partitions: 2
2017-12-09 21:28:45,633 -> Centers partitions: 2
2017-12-09 21:28:52,280 -> 01.Indexing points,6.60,19715,20.0,7
2017-12-09 21:28:56,627 -> 02.Indexing centers,4.35,47664,20.0,7
2017-12-09 21:28:56,635 -> 992
2017-12-09 21:28:56,641 -> 1024
2017-12-09 21:29:09,245 -> 03.Joining datasets,12.66,47663,20.0,7
Done!!! Sat Dec  9 21:29:09 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 21:29:11,909 -> Starting session,1.64,0
2017-12-09 21:29:11,910 -> Setting variables,0.00,0
2017-12-09 21:29:16,013 -> Reading datasets,4.10,0
2017-12-09 21:29:16,027 -> Points partitions: 2
2017-12-09 21:29:16,038 -> Centers partitions: 2
2017-12-09 21:29:22,840 -> 01.Indexing points,6.76,19715,20.0,7
2017-12-09 21:29:27,120 -> 02.Indexing centers,4.28,47664,20.0,7
2017-12-09 21:29:27,127 -> 992
2017-12-09 21:29:27,134 -> 1024
2017-12-09 21:29:40,061 -> 03.Joining datasets,12.93,47663,20.0,7
Done!!! Sat Dec  9 21:29:40 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 21:29:42,673 -> Starting session,1.59,0
2017-12-09 21:29:42,674 -> Setting variables,0.00,0
2017-12-09 21:29:46,811 -> Reading datasets,4.14,0
2017-12-09 21:29:46,826 -> Points partitions: 2
2017-12-09 21:29:46,837 -> Centers partitions: 2
2017-12-09 21:29:53,531 -> 01.Indexing points,6.65,19715,20.0,7
2017-12-09 21:29:57,632 -> 02.Indexing centers,4.10,47664,20.0,7
2017-12-09 21:29:57,641 -> 992
2017-12-09 21:29:57,649 -> 1024
2017-12-09 21:30:10,422 -> 03.Joining datasets,12.77,47663,20.0,7
Done!!! Sat Dec  9 21:30:10 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 21:30:13,091 -> Starting session,1.70,0
2017-12-09 21:30:13,091 -> Setting variables,0.00,0
2017-12-09 21:30:17,230 -> Reading datasets,4.14,0
2017-12-09 21:30:17,240 -> Points partitions: 2
2017-12-09 21:30:17,247 -> Centers partitions: 2
2017-12-09 21:30:24,149 -> 01.Indexing points,6.87,19715,20.0,7
2017-12-09 21:30:28,510 -> 02.Indexing centers,4.36,47664,20.0,7
2017-12-09 21:30:28,517 -> 992
2017-12-09 21:30:28,523 -> 1024
2017-12-09 21:30:41,692 -> 03.Joining datasets,13.17,47663,20.0,7
Done!!! Sat Dec  9 21:30:42 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 21:30:44,170 -> Starting session,1.47,0
2017-12-09 21:30:44,171 -> Setting variables,0.00,0
2017-12-09 21:30:48,338 -> Reading datasets,4.17,0
2017-12-09 21:30:48,352 -> Points partitions: 2
2017-12-09 21:30:48,361 -> Centers partitions: 2
2017-12-09 21:30:55,323 -> 01.Indexing points,6.92,19715,20.0,7
2017-12-09 21:30:59,467 -> 02.Indexing centers,4.14,47664,20.0,7
2017-12-09 21:30:59,474 -> 992
2017-12-09 21:30:59,480 -> 1024
2017-12-09 21:31:12,592 -> 03.Joining datasets,13.11,47663,20.0,7
Done!!! Sat Dec  9 21:31:12 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 21:31:20,806 -> Starting session,1.64,0
2017-12-09 21:31:20,806 -> Setting variables,0.00,0
2017-12-09 21:31:25,934 -> Reading datasets,5.13,0
2017-12-09 21:31:25,948 -> Points partitions: 2
2017-12-09 21:31:25,959 -> Centers partitions: 2
2017-12-09 21:31:33,223 -> 01.Indexing points,7.23,19715,20.0,14
2017-12-09 21:31:37,661 -> 02.Indexing centers,4.44,47664,20.0,14
2017-12-09 21:31:37,671 -> 992
2017-12-09 21:31:37,679 -> 1024
2017-12-09 21:31:48,398 -> 03.Joining datasets,10.72,47663,20.0,14
Done!!! Sat Dec  9 21:31:48 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 21:31:51,126 -> Starting session,1.53,0
2017-12-09 21:31:51,127 -> Setting variables,0.00,0
2017-12-09 21:31:56,084 -> Reading datasets,4.96,0
2017-12-09 21:31:56,094 -> Points partitions: 2
2017-12-09 21:31:56,102 -> Centers partitions: 2
2017-12-09 21:32:03,921 -> 01.Indexing points,7.78,19715,20.0,14
2017-12-09 21:32:08,671 -> 02.Indexing centers,4.75,47664,20.0,14
2017-12-09 21:32:08,682 -> 992
2017-12-09 21:32:08,690 -> 1024
2017-12-09 21:32:20,361 -> 03.Joining datasets,11.67,47663,20.0,14
Done!!! Sat Dec  9 21:32:21 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 21:32:23,884 -> Starting session,1.71,0
2017-12-09 21:32:23,884 -> Setting variables,0.00,0
2017-12-09 21:32:28,881 -> Reading datasets,5.00,0
2017-12-09 21:32:28,898 -> Points partitions: 2
2017-12-09 21:32:28,908 -> Centers partitions: 2
2017-12-09 21:32:36,325 -> 01.Indexing points,7.37,19715,20.0,14
2017-12-09 21:32:40,487 -> 02.Indexing centers,4.16,47664,20.0,14
2017-12-09 21:32:40,499 -> 992
2017-12-09 21:32:40,509 -> 1024
2017-12-09 21:32:51,914 -> 03.Joining datasets,11.40,47663,20.0,14
Done!!! Sat Dec  9 21:32:53 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 21:32:55,499 -> Starting session,1.65,0
2017-12-09 21:32:55,500 -> Setting variables,0.00,0
2017-12-09 21:33:00,298 -> Reading datasets,4.80,0
2017-12-09 21:33:00,309 -> Points partitions: 2
2017-12-09 21:33:00,317 -> Centers partitions: 2
2017-12-09 21:33:07,856 -> 01.Indexing points,7.50,19715,20.0,14
2017-12-09 21:33:12,323 -> 02.Indexing centers,4.47,47664,20.0,14
2017-12-09 21:33:12,334 -> 992
2017-12-09 21:33:12,342 -> 1024
2017-12-09 21:33:23,256 -> 03.Joining datasets,10.91,47663,20.0,14
Done!!! Sat Dec  9 21:33:23 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 21:33:26,051 -> Starting session,1.63,0
2017-12-09 21:33:26,051 -> Setting variables,0.00,0
2017-12-09 21:33:31,015 -> Reading datasets,4.96,0
2017-12-09 21:33:31,030 -> Points partitions: 2
2017-12-09 21:33:31,040 -> Centers partitions: 2
2017-12-09 21:33:38,595 -> 01.Indexing points,7.51,19715,20.0,14
2017-12-09 21:33:43,049 -> 02.Indexing centers,4.45,47664,20.0,14
2017-12-09 21:33:43,057 -> 992
2017-12-09 21:33:43,066 -> 1024
2017-12-09 21:33:54,743 -> 03.Joining datasets,11.68,47663,20.0,14
Done!!! Sat Dec  9 21:33:55 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:34:03,037 -> Starting session,1.63,0
2017-12-09 21:34:03,039 -> Setting variables,0.00,0
2017-12-09 21:34:08,026 -> Reading datasets,4.99,0
2017-12-09 21:34:08,037 -> Points partitions: 2
2017-12-09 21:34:08,044 -> Centers partitions: 2
2017-12-09 21:34:16,785 -> 01.Indexing points,8.70,19715,20.0,21
2017-12-09 21:34:20,979 -> 02.Indexing centers,4.19,47664,20.0,21
2017-12-09 21:34:20,986 -> 992
2017-12-09 21:34:20,992 -> 1024
2017-12-09 21:34:32,412 -> 03.Joining datasets,11.49,47663,20.0,21
Done!!! Sat Dec  9 21:34:32 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:34:35,319 -> Starting session,1.68,0
2017-12-09 21:34:35,320 -> Setting variables,0.00,0
2017-12-09 21:34:41,475 -> Reading datasets,6.15,0
2017-12-09 21:34:41,489 -> Points partitions: 2
2017-12-09 21:34:41,499 -> Centers partitions: 2
2017-12-09 21:34:49,263 -> 01.Indexing points,7.72,19715,20.0,21
2017-12-09 21:34:53,480 -> 02.Indexing centers,4.21,47664,20.0,21
2017-12-09 21:34:53,490 -> 992
2017-12-09 21:34:53,498 -> 1024
2017-12-09 21:35:03,666 -> 03.Joining datasets,10.17,47663,20.0,21
Done!!! Sat Dec  9 21:35:04 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:35:06,575 -> Starting session,1.64,0
2017-12-09 21:35:06,576 -> Setting variables,0.00,0
2017-12-09 21:35:12,689 -> Reading datasets,6.11,0
2017-12-09 21:35:12,706 -> Points partitions: 2
2017-12-09 21:35:12,717 -> Centers partitions: 2
2017-12-09 21:35:17,628 -> 01.Indexing points,4.87,19715,20.0,21
2017-12-09 21:35:22,285 -> 02.Indexing centers,4.66,47664,20.0,21
2017-12-09 21:35:22,294 -> 992
2017-12-09 21:35:22,301 -> 1024
2017-12-09 21:35:30,760 -> 03.Joining datasets,8.46,47663,20.0,21
Done!!! Sat Dec  9 21:35:32 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:35:34,298 -> Starting session,1.58,0
2017-12-09 21:35:34,299 -> Setting variables,0.00,0
2017-12-09 21:35:40,169 -> Reading datasets,5.87,0
2017-12-09 21:35:40,182 -> Points partitions: 2
2017-12-09 21:35:40,191 -> Centers partitions: 2
2017-12-09 21:35:45,181 -> 01.Indexing points,4.95,19715,20.0,21
2017-12-09 21:35:49,620 -> 02.Indexing centers,4.44,47664,20.0,21
2017-12-09 21:35:49,628 -> 992
2017-12-09 21:35:49,634 -> 1024
2017-12-09 21:35:58,986 -> 03.Joining datasets,9.35,47663,20.0,21
Done!!! Sat Dec  9 21:35:59 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:36:01,808 -> Starting session,1.64,0
2017-12-09 21:36:01,809 -> Setting variables,0.00,0
2017-12-09 21:36:07,647 -> Reading datasets,5.84,0
2017-12-09 21:36:07,661 -> Points partitions: 2
2017-12-09 21:36:07,672 -> Centers partitions: 2
2017-12-09 21:36:12,705 -> 01.Indexing points,4.99,19715,20.0,21
2017-12-09 21:36:16,788 -> 02.Indexing centers,4.08,47664,20.0,21
2017-12-09 21:36:16,796 -> 992
2017-12-09 21:36:16,802 -> 1024
2017-12-09 21:36:28,522 -> 03.Joining datasets,11.72,47663,20.0,21
Done!!! Sat Dec  9 21:36:29 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:36:38,055 -> Starting session,2.09,0
2017-12-09 21:36:38,056 -> Setting variables,0.00,0
2017-12-09 21:36:45,010 -> Reading datasets,6.95,0
2017-12-09 21:36:45,020 -> Points partitions: 2
2017-12-09 21:36:45,028 -> Centers partitions: 2
2017-12-09 21:36:53,245 -> 01.Indexing points,8.18,19715,20.0,28
2017-12-09 21:36:57,040 -> 02.Indexing centers,3.79,47664,20.0,28
2017-12-09 21:36:57,050 -> 992
2017-12-09 21:36:57,059 -> 1024
2017-12-09 21:37:07,429 -> 03.Joining datasets,10.37,47663,20.0,28
Done!!! Sat Dec  9 21:37:07 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:37:10,276 -> Starting session,1.63,0
2017-12-09 21:37:10,277 -> Setting variables,0.00,0
2017-12-09 21:37:17,514 -> Reading datasets,7.24,0
2017-12-09 21:37:17,524 -> Points partitions: 2
2017-12-09 21:37:17,531 -> Centers partitions: 2
2017-12-09 21:37:26,371 -> 01.Indexing points,8.80,19715,20.0,28
2017-12-09 21:37:30,526 -> 02.Indexing centers,4.15,47664,20.0,28
2017-12-09 21:37:30,536 -> 992
2017-12-09 21:37:30,543 -> 1024
2017-12-09 21:37:41,315 -> 03.Joining datasets,10.77,47663,20.0,28
Done!!! Sat Dec  9 21:37:41 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:37:44,103 -> Starting session,1.61,0
2017-12-09 21:37:44,104 -> Setting variables,0.00,0
2017-12-09 21:37:51,421 -> Reading datasets,7.32,0
2017-12-09 21:37:51,432 -> Points partitions: 2
2017-12-09 21:37:51,439 -> Centers partitions: 2
2017-12-09 21:37:59,685 -> 01.Indexing points,8.21,19715,20.0,28
2017-12-09 21:38:03,855 -> 02.Indexing centers,4.17,47664,20.0,28
2017-12-09 21:38:03,865 -> 992
2017-12-09 21:38:03,871 -> 1024
2017-12-09 21:38:13,861 -> 03.Joining datasets,9.99,47663,20.0,28
Done!!! Sat Dec  9 21:38:14 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 21:38:16,690 -> Starting session,1.74,0
2017-12-09 21:38:16,691 -> Setting variables,0.00,0
2017-12-09 21:38:23,839 -> Reading datasets,7.15,0
2017-12-09 21:38:23,849 -> Points partitions: 2
2017-12-09 21:38:23,857 -> Centers partitions: 2
2017-12-09 21:38:30,113 -> 01.Indexing points,6.22,19715,20.0,28
2017-12-09 21:38:34,356 -> 02.Indexing centers,4.29,47664,20.0,28
2017-12-09 21:38:34,365 -> 992
2017-12-09 21:38:34,372 -> 1024
2017-12-09 21:38:43,076 -> 03.Joining datasets,8.70,47663,20.0,28
Done!!! Sat Dec  9 21:38:43 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 21:38:45,859 -> Starting session,1.70,0
2017-12-09 21:38:45,860 -> Setting variables,0.00,0
2017-12-09 21:38:51,884 -> Reading datasets,6.02,0
2017-12-09 21:38:51,894 -> Points partitions: 2
2017-12-09 21:38:51,901 -> Centers partitions: 2
2017-12-09 21:39:01,835 -> 01.Indexing points,9.89,19715,20.0,28
2017-12-09 21:39:06,554 -> 02.Indexing centers,4.72,47664,20.0,28
2017-12-09 21:39:06,562 -> 992
2017-12-09 21:39:06,568 -> 1024
2017-12-09 21:39:17,575 -> 03.Joining datasets,11.01,47663,20.0,28
Done!!! Sat Dec  9 21:39:18 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 21:39:25,812 -> Starting session,1.60,0
2017-12-09 21:39:25,813 -> Setting variables,0.00,0
2017-12-09 21:39:29,856 -> Reading datasets,4.04,0
2017-12-09 21:39:29,868 -> Points partitions: 2
2017-12-09 21:39:29,878 -> Centers partitions: 2
2017-12-09 21:39:37,295 -> 01.Indexing points,7.37,78857,10.0,7
2017-12-09 21:39:41,930 -> 02.Indexing centers,4.63,89928,10.0,7
2017-12-09 21:39:41,938 -> 1024
2017-12-09 21:39:41,943 -> 1024
2017-12-09 21:39:57,072 -> 03.Joining datasets,15.13,89927,10.0,7
Done!!! Sat Dec  9 21:39:57 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 21:39:59,605 -> Starting session,1.50,0
2017-12-09 21:39:59,605 -> Setting variables,0.00,0
2017-12-09 21:40:03,892 -> Reading datasets,4.29,0
2017-12-09 21:40:03,905 -> Points partitions: 2
2017-12-09 21:40:03,915 -> Centers partitions: 2
2017-12-09 21:40:11,354 -> 01.Indexing points,7.40,78857,10.0,7
2017-12-09 21:40:15,883 -> 02.Indexing centers,4.53,89928,10.0,7
2017-12-09 21:40:15,891 -> 1024
2017-12-09 21:40:15,897 -> 1024
2017-12-09 21:40:30,652 -> 03.Joining datasets,14.76,89927,10.0,7
Done!!! Sat Dec  9 21:40:31 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 21:40:33,041 -> Starting session,1.40,0
2017-12-09 21:40:33,042 -> Setting variables,0.00,0
2017-12-09 21:40:37,382 -> Reading datasets,4.34,0
2017-12-09 21:40:37,392 -> Points partitions: 2
2017-12-09 21:40:37,399 -> Centers partitions: 2
2017-12-09 21:40:45,055 -> 01.Indexing points,7.62,78857,10.0,7
2017-12-09 21:40:49,643 -> 02.Indexing centers,4.59,89928,10.0,7
2017-12-09 21:40:49,650 -> 1024
2017-12-09 21:40:49,656 -> 1024
2017-12-09 21:41:04,674 -> 03.Joining datasets,15.02,89927,10.0,7
Done!!! Sat Dec  9 21:41:05 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 21:41:07,220 -> Starting session,1.55,0
2017-12-09 21:41:07,221 -> Setting variables,0.00,0
2017-12-09 21:41:11,287 -> Reading datasets,4.07,0
2017-12-09 21:41:11,297 -> Points partitions: 2
2017-12-09 21:41:11,305 -> Centers partitions: 2
2017-12-09 21:41:19,288 -> 01.Indexing points,7.94,78857,10.0,7
2017-12-09 21:41:23,912 -> 02.Indexing centers,4.62,89928,10.0,7
2017-12-09 21:41:23,920 -> 1024
2017-12-09 21:41:23,926 -> 1024
2017-12-09 21:41:38,767 -> 03.Joining datasets,14.84,89927,10.0,7
Done!!! Sat Dec  9 21:41:39 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 21:41:41,324 -> Starting session,1.57,0
2017-12-09 21:41:41,324 -> Setting variables,0.00,0
2017-12-09 21:41:45,518 -> Reading datasets,4.19,0
2017-12-09 21:41:45,529 -> Points partitions: 2
2017-12-09 21:41:45,539 -> Centers partitions: 2
2017-12-09 21:41:53,855 -> 01.Indexing points,8.27,78857,10.0,7
2017-12-09 21:41:58,694 -> 02.Indexing centers,4.84,89928,10.0,7
2017-12-09 21:41:58,702 -> 1024
2017-12-09 21:41:58,707 -> 1024
2017-12-09 21:42:14,450 -> 03.Joining datasets,15.74,89927,10.0,7
Done!!! Sat Dec  9 21:42:14 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 21:42:22,689 -> Starting session,1.80,0
2017-12-09 21:42:22,689 -> Setting variables,0.00,0
2017-12-09 21:42:27,658 -> Reading datasets,4.97,0
2017-12-09 21:42:27,672 -> Points partitions: 2
2017-12-09 21:42:27,684 -> Centers partitions: 2
2017-12-09 21:42:35,989 -> 01.Indexing points,8.26,78857,10.0,14
2017-12-09 21:42:40,516 -> 02.Indexing centers,4.53,89928,10.0,14
2017-12-09 21:42:40,528 -> 1024
2017-12-09 21:42:40,536 -> 1024
2017-12-09 21:42:53,973 -> 03.Joining datasets,13.44,89927,10.0,14
Done!!! Sat Dec  9 21:42:54 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 21:42:56,908 -> Starting session,1.70,0
2017-12-09 21:42:56,908 -> Setting variables,0.00,0
2017-12-09 21:43:01,900 -> Reading datasets,4.99,0
2017-12-09 21:43:01,911 -> Points partitions: 2
2017-12-09 21:43:01,918 -> Centers partitions: 2
2017-12-09 21:43:10,123 -> 01.Indexing points,8.17,78857,10.0,14
2017-12-09 21:43:14,665 -> 02.Indexing centers,4.54,89928,10.0,14
2017-12-09 21:43:14,673 -> 1024
2017-12-09 21:43:14,679 -> 1024
2017-12-09 21:43:27,476 -> 03.Joining datasets,12.80,89927,10.0,14
Done!!! Sat Dec  9 21:43:27 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 21:43:30,362 -> Starting session,1.81,0
2017-12-09 21:43:30,362 -> Setting variables,0.00,0
2017-12-09 21:43:35,301 -> Reading datasets,4.94,0
2017-12-09 21:43:35,315 -> Points partitions: 2
2017-12-09 21:43:35,326 -> Centers partitions: 2
2017-12-09 21:43:43,635 -> 01.Indexing points,8.26,78857,10.0,14
2017-12-09 21:43:48,741 -> 02.Indexing centers,5.10,89928,10.0,14
2017-12-09 21:43:48,752 -> 1024
2017-12-09 21:43:48,760 -> 1024
2017-12-09 21:44:01,113 -> 03.Joining datasets,12.35,89927,10.0,14
Done!!! Sat Dec  9 21:44:02 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 21:44:04,586 -> Starting session,1.62,0
2017-12-09 21:44:04,586 -> Setting variables,0.00,0
2017-12-09 21:44:09,717 -> Reading datasets,5.13,0
2017-12-09 21:44:09,730 -> Points partitions: 2
2017-12-09 21:44:09,740 -> Centers partitions: 2
2017-12-09 21:44:17,813 -> 01.Indexing points,8.03,78857,10.0,14
2017-12-09 21:44:22,306 -> 02.Indexing centers,4.49,89928,10.0,14
2017-12-09 21:44:22,314 -> 1024
2017-12-09 21:44:22,319 -> 1024
2017-12-09 21:44:34,678 -> 03.Joining datasets,12.36,89927,10.0,14
Done!!! Sat Dec  9 21:44:35 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 21:44:37,650 -> Starting session,1.68,0
2017-12-09 21:44:37,650 -> Setting variables,0.00,0
2017-12-09 21:44:42,710 -> Reading datasets,5.06,0
2017-12-09 21:44:42,720 -> Points partitions: 2
2017-12-09 21:44:42,728 -> Centers partitions: 2
2017-12-09 21:44:50,448 -> 01.Indexing points,7.68,78857,10.0,14
2017-12-09 21:44:54,781 -> 02.Indexing centers,4.33,89928,10.0,14
2017-12-09 21:44:54,789 -> 1024
2017-12-09 21:44:54,794 -> 1024
2017-12-09 21:45:07,657 -> 03.Joining datasets,12.86,89927,10.0,14
Done!!! Sat Dec  9 21:45:08 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:45:16,885 -> Starting session,1.81,0
2017-12-09 21:45:16,887 -> Setting variables,0.00,0
2017-12-09 21:45:22,140 -> Reading datasets,5.25,0
2017-12-09 21:45:22,154 -> Points partitions: 2
2017-12-09 21:45:22,165 -> Centers partitions: 2
2017-12-09 21:45:30,507 -> 01.Indexing points,8.30,78857,10.0,21
2017-12-09 21:45:34,597 -> 02.Indexing centers,4.09,89928,10.0,21
2017-12-09 21:45:34,606 -> 1024
2017-12-09 21:45:34,614 -> 1024
2017-12-09 21:45:45,086 -> 03.Joining datasets,10.47,89927,10.0,21
Done!!! Sat Dec  9 21:45:46 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:45:48,853 -> Starting session,1.79,0
2017-12-09 21:45:48,854 -> Setting variables,0.00,0
2017-12-09 21:45:54,892 -> Reading datasets,6.04,0
2017-12-09 21:45:54,906 -> Points partitions: 2
2017-12-09 21:45:54,916 -> Centers partitions: 2
2017-12-09 21:46:02,610 -> 01.Indexing points,7.66,78857,10.0,21
2017-12-09 21:46:06,860 -> 02.Indexing centers,4.25,89928,10.0,21
2017-12-09 21:46:06,871 -> 1024
2017-12-09 21:46:06,881 -> 1024
2017-12-09 21:46:16,717 -> 03.Joining datasets,9.84,89927,10.0,21
Done!!! Sat Dec  9 21:46:17 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:46:19,542 -> Starting session,1.73,0
2017-12-09 21:46:19,542 -> Setting variables,0.00,0
2017-12-09 21:46:24,559 -> Reading datasets,5.02,0
2017-12-09 21:46:24,572 -> Points partitions: 2
2017-12-09 21:46:24,583 -> Centers partitions: 2
2017-12-09 21:46:32,902 -> 01.Indexing points,8.27,78857,10.0,21
2017-12-09 21:46:37,133 -> 02.Indexing centers,4.23,89928,10.0,21
2017-12-09 21:46:37,145 -> 1024
2017-12-09 21:46:37,154 -> 1024
2017-12-09 21:46:47,290 -> 03.Joining datasets,10.14,89927,10.0,21
Done!!! Sat Dec  9 21:46:47 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:46:50,151 -> Starting session,1.74,0
2017-12-09 21:46:50,152 -> Setting variables,0.00,0
2017-12-09 21:46:55,218 -> Reading datasets,5.07,0
2017-12-09 21:46:55,231 -> Points partitions: 2
2017-12-09 21:46:55,241 -> Centers partitions: 2
2017-12-09 21:47:03,871 -> 01.Indexing points,8.58,78857,10.0,21
2017-12-09 21:47:07,870 -> 02.Indexing centers,4.00,89928,10.0,21
2017-12-09 21:47:07,878 -> 1024
2017-12-09 21:47:07,884 -> 1024
2017-12-09 21:47:17,546 -> 03.Joining datasets,9.66,89927,10.0,21
Done!!! Sat Dec  9 21:47:18 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:47:20,472 -> Starting session,1.68,0
2017-12-09 21:47:20,473 -> Setting variables,0.00,0
2017-12-09 21:47:26,345 -> Reading datasets,5.87,0
2017-12-09 21:47:26,356 -> Points partitions: 2
2017-12-09 21:47:26,367 -> Centers partitions: 2
2017-12-09 21:47:33,931 -> 01.Indexing points,7.53,78857,10.0,21
2017-12-09 21:47:38,156 -> 02.Indexing centers,4.22,89928,10.0,21
2017-12-09 21:47:38,167 -> 1024
2017-12-09 21:47:38,176 -> 1024
2017-12-09 21:47:47,906 -> 03.Joining datasets,9.73,89927,10.0,21
Done!!! Sat Dec  9 21:47:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:47:56,418 -> Starting session,1.83,0
2017-12-09 21:47:56,418 -> Setting variables,0.00,0
2017-12-09 21:48:03,658 -> Reading datasets,7.24,0
2017-12-09 21:48:03,669 -> Points partitions: 2
2017-12-09 21:48:03,676 -> Centers partitions: 2
2017-12-09 21:48:11,298 -> 01.Indexing points,7.59,78857,10.0,28
2017-12-09 21:48:15,932 -> 02.Indexing centers,4.63,89928,10.0,28
2017-12-09 21:48:15,940 -> 1024
2017-12-09 21:48:15,946 -> 1024
2017-12-09 21:48:25,094 -> 03.Joining datasets,9.15,89927,10.0,28
Done!!! Sat Dec  9 21:48:25 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:48:27,910 -> Starting session,1.66,0
2017-12-09 21:48:27,911 -> Setting variables,0.00,0
2017-12-09 21:48:35,093 -> Reading datasets,7.18,0
2017-12-09 21:48:35,106 -> Points partitions: 2
2017-12-09 21:48:35,117 -> Centers partitions: 2
2017-12-09 21:48:43,245 -> 01.Indexing points,8.08,78857,10.0,28
2017-12-09 21:48:47,506 -> 02.Indexing centers,4.26,89928,10.0,28
2017-12-09 21:48:47,514 -> 1024
2017-12-09 21:48:47,521 -> 1024
2017-12-09 21:48:56,409 -> 03.Joining datasets,8.89,89927,10.0,28
Done!!! Sat Dec  9 21:48:56 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:48:59,346 -> Starting session,1.71,0
2017-12-09 21:48:59,347 -> Setting variables,0.00,0
2017-12-09 21:49:06,491 -> Reading datasets,7.14,0
2017-12-09 21:49:06,506 -> Points partitions: 2
2017-12-09 21:49:06,517 -> Centers partitions: 2
2017-12-09 21:49:15,921 -> 01.Indexing points,9.36,78857,10.0,28
2017-12-09 21:49:20,112 -> 02.Indexing centers,4.19,89928,10.0,28
2017-12-09 21:49:20,120 -> 1024
2017-12-09 21:49:20,127 -> 1024
2017-12-09 21:49:29,173 -> 03.Joining datasets,9.05,89927,10.0,28
Done!!! Sat Dec  9 21:49:30 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 21:49:32,918 -> Starting session,1.73,0
2017-12-09 21:49:32,918 -> Setting variables,0.00,0
2017-12-09 21:49:40,368 -> Reading datasets,7.45,0
2017-12-09 21:49:40,378 -> Points partitions: 2
2017-12-09 21:49:40,386 -> Centers partitions: 2
2017-12-09 21:49:48,521 -> 01.Indexing points,8.10,78857,10.0,28
2017-12-09 21:49:52,266 -> 02.Indexing centers,3.74,89928,10.0,28
2017-12-09 21:49:52,273 -> 1024
2017-12-09 21:49:52,279 -> 1024
2017-12-09 21:50:01,387 -> 03.Joining datasets,8.71,89927,10.0,28
Done!!! Sat Dec  9 21:50:01 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 21:50:04,291 -> Starting session,1.63,0
2017-12-09 21:50:04,292 -> Setting variables,0.00,0
2017-12-09 21:50:11,471 -> Reading datasets,7.18,0
2017-12-09 21:50:11,482 -> Points partitions: 2
2017-12-09 21:50:11,489 -> Centers partitions: 2
2017-12-09 21:50:19,686 -> 01.Indexing points,8.16,78857,10.0,28
2017-12-09 21:50:24,154 -> 02.Indexing centers,4.47,89928,10.0,28
2017-12-09 21:50:24,166 -> 1024
2017-12-09 21:50:24,176 -> 1024
2017-12-09 21:50:33,177 -> 03.Joining datasets,9.00,89927,10.0,28
Done!!! Sat Dec  9 21:50:33 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 21:50:41,350 -> Starting session,1.46,0
2017-12-09 21:50:41,350 -> Setting variables,0.00,0
2017-12-09 21:50:45,586 -> Reading datasets,4.24,0
2017-12-09 21:50:45,596 -> Points partitions: 2
2017-12-09 21:50:45,604 -> Centers partitions: 2
2017-12-09 21:50:53,247 -> 01.Indexing points,7.60,59143,10.0,7
2017-12-09 21:50:57,592 -> 02.Indexing centers,4.34,67446,10.0,7
2017-12-09 21:50:57,599 -> 1024
2017-12-09 21:50:57,605 -> 1024
2017-12-09 21:51:12,166 -> 03.Joining datasets,14.56,67446,10.0,7
Done!!! Sat Dec  9 21:51:12 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 21:51:14,559 -> Starting session,1.39,0
2017-12-09 21:51:14,560 -> Setting variables,0.00,0
2017-12-09 21:51:18,900 -> Reading datasets,4.34,0
2017-12-09 21:51:18,913 -> Points partitions: 2
2017-12-09 21:51:18,923 -> Centers partitions: 2
2017-12-09 21:51:26,275 -> 01.Indexing points,7.31,59143,10.0,7
2017-12-09 21:51:30,723 -> 02.Indexing centers,4.45,67446,10.0,7
2017-12-09 21:51:30,730 -> 1024
2017-12-09 21:51:30,736 -> 1024
2017-12-09 21:51:45,092 -> 03.Joining datasets,14.47,67446,10.0,7
Done!!! Sat Dec  9 21:51:45 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 21:51:47,605 -> Starting session,1.50,0
2017-12-09 21:51:47,605 -> Setting variables,0.00,0
2017-12-09 21:51:51,930 -> Reading datasets,4.32,0
2017-12-09 21:51:51,940 -> Points partitions: 2
2017-12-09 21:51:51,947 -> Centers partitions: 2
2017-12-09 21:51:59,224 -> 01.Indexing points,7.24,59143,10.0,7
2017-12-09 21:52:03,604 -> 02.Indexing centers,4.38,67446,10.0,7
2017-12-09 21:52:03,611 -> 1024
2017-12-09 21:52:03,617 -> 1024
2017-12-09 21:52:17,678 -> 03.Joining datasets,14.06,67446,10.0,7
Done!!! Sat Dec  9 21:52:18 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 21:52:20,355 -> Starting session,1.70,0
2017-12-09 21:52:20,356 -> Setting variables,0.00,0
2017-12-09 21:52:24,545 -> Reading datasets,4.19,0
2017-12-09 21:52:24,559 -> Points partitions: 2
2017-12-09 21:52:24,570 -> Centers partitions: 2
2017-12-09 21:52:31,927 -> 01.Indexing points,7.31,59143,10.0,7
2017-12-09 21:52:36,329 -> 02.Indexing centers,4.40,67446,10.0,7
2017-12-09 21:52:36,336 -> 1024
2017-12-09 21:52:36,342 -> 1024
2017-12-09 21:52:50,130 -> 03.Joining datasets,13.79,67446,10.0,7
Done!!! Sat Dec  9 21:52:50 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 21:52:52,659 -> Starting session,1.54,0
2017-12-09 21:52:52,660 -> Setting variables,0.00,0
2017-12-09 21:52:56,750 -> Reading datasets,4.09,0
2017-12-09 21:52:56,763 -> Points partitions: 2
2017-12-09 21:52:56,773 -> Centers partitions: 2
2017-12-09 21:53:04,461 -> 01.Indexing points,7.65,59143,10.0,7
2017-12-09 21:53:08,785 -> 02.Indexing centers,4.32,67446,10.0,7
2017-12-09 21:53:08,793 -> 1024
2017-12-09 21:53:08,799 -> 1024
2017-12-09 21:53:23,378 -> 03.Joining datasets,14.58,67446,10.0,7
Done!!! Sat Dec  9 21:53:23 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 21:53:31,417 -> Starting session,1.61,0
2017-12-09 21:53:31,417 -> Setting variables,0.00,0
2017-12-09 21:53:36,463 -> Reading datasets,5.05,0
2017-12-09 21:53:36,473 -> Points partitions: 2
2017-12-09 21:53:36,481 -> Centers partitions: 2
2017-12-09 21:53:44,749 -> 01.Indexing points,8.23,59143,10.0,14
2017-12-09 21:53:49,078 -> 02.Indexing centers,4.33,67446,10.0,14
2017-12-09 21:53:49,086 -> 1024
2017-12-09 21:53:49,092 -> 1024
2017-12-09 21:54:01,276 -> 03.Joining datasets,12.18,67446,10.0,14
Done!!! Sat Dec  9 21:54:01 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 21:54:04,238 -> Starting session,1.72,0
2017-12-09 21:54:04,238 -> Setting variables,0.00,0
2017-12-09 21:54:09,024 -> Reading datasets,4.79,0
2017-12-09 21:54:09,036 -> Points partitions: 2
2017-12-09 21:54:09,043 -> Centers partitions: 2
2017-12-09 21:54:16,986 -> 01.Indexing points,7.91,59143,10.0,14
2017-12-09 21:54:21,409 -> 02.Indexing centers,4.42,67446,10.0,14
2017-12-09 21:54:21,420 -> 1024
2017-12-09 21:54:21,428 -> 1024
2017-12-09 21:54:33,895 -> 03.Joining datasets,12.47,67446,10.0,14
Done!!! Sat Dec  9 21:54:34 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 21:54:36,764 -> Starting session,1.76,0
2017-12-09 21:54:36,765 -> Setting variables,0.00,0
2017-12-09 21:54:41,865 -> Reading datasets,5.10,0
2017-12-09 21:54:41,877 -> Points partitions: 2
2017-12-09 21:54:41,884 -> Centers partitions: 2
2017-12-09 21:54:49,678 -> 01.Indexing points,7.76,59143,10.0,14
2017-12-09 21:54:54,415 -> 02.Indexing centers,4.74,67446,10.0,14
2017-12-09 21:54:54,422 -> 1024
2017-12-09 21:54:54,428 -> 1024
2017-12-09 21:55:06,387 -> 03.Joining datasets,11.96,67446,10.0,14
Done!!! Sat Dec  9 21:55:06 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 21:55:09,080 -> Starting session,1.60,0
2017-12-09 21:55:09,081 -> Setting variables,0.00,0
2017-12-09 21:55:14,043 -> Reading datasets,4.96,0
2017-12-09 21:55:14,057 -> Points partitions: 2
2017-12-09 21:55:14,069 -> Centers partitions: 2
2017-12-09 21:55:21,913 -> 01.Indexing points,7.80,59143,10.0,14
2017-12-09 21:55:26,365 -> 02.Indexing centers,4.45,67446,10.0,14
2017-12-09 21:55:26,375 -> 1024
2017-12-09 21:55:26,383 -> 1024
2017-12-09 21:55:38,599 -> 03.Joining datasets,12.22,67446,10.0,14
Done!!! Sat Dec  9 21:55:39 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 21:55:42,141 -> Starting session,1.52,0
2017-12-09 21:55:42,142 -> Setting variables,0.00,0
2017-12-09 21:55:47,047 -> Reading datasets,4.91,0
2017-12-09 21:55:47,061 -> Points partitions: 2
2017-12-09 21:55:47,071 -> Centers partitions: 2
2017-12-09 21:55:55,175 -> 01.Indexing points,8.06,59143,10.0,14
2017-12-09 21:55:59,381 -> 02.Indexing centers,4.21,67446,10.0,14
2017-12-09 21:55:59,393 -> 1024
2017-12-09 21:55:59,403 -> 1024
2017-12-09 21:56:12,231 -> 03.Joining datasets,12.83,67446,10.0,14
Done!!! Sat Dec  9 21:56:12 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 21:56:20,604 -> Starting session,1.76,0
2017-12-09 21:56:20,604 -> Setting variables,0.00,0
2017-12-09 21:56:26,445 -> Reading datasets,5.84,0
2017-12-09 21:56:26,460 -> Points partitions: 2
2017-12-09 21:56:26,471 -> Centers partitions: 2
2017-12-09 21:56:33,365 -> 01.Indexing points,6.85,59143,10.0,21
2017-12-09 21:56:37,263 -> 02.Indexing centers,3.90,67446,10.0,21
2017-12-09 21:56:37,271 -> 1024
2017-12-09 21:56:37,277 -> 1024
2017-12-09 21:56:47,319 -> 03.Joining datasets,10.04,67446,10.0,21
Done!!! Sat Dec  9 21:56:47 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 21:56:50,359 -> Starting session,1.81,0
2017-12-09 21:56:50,360 -> Setting variables,0.00,0
2017-12-09 21:56:56,092 -> Reading datasets,5.87,0
2017-12-09 21:56:56,106 -> Points partitions: 2
2017-12-09 21:56:56,115 -> Centers partitions: 2
2017-12-09 21:57:02,930 -> 01.Indexing points,6.77,59143,10.0,21
2017-12-09 21:57:06,691 -> 02.Indexing centers,3.76,67446,10.0,21
2017-12-09 21:57:06,703 -> 1024
2017-12-09 21:57:06,712 -> 1024
2017-12-09 21:57:16,167 -> 03.Joining datasets,9.46,67446,10.0,21
Done!!! Sat Dec  9 21:57:16 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 21:57:18,983 -> Starting session,1.58,0
2017-12-09 21:57:18,984 -> Setting variables,0.00,0
2017-12-09 21:57:24,764 -> Reading datasets,5.78,0
2017-12-09 21:57:24,775 -> Points partitions: 2
2017-12-09 21:57:24,786 -> Centers partitions: 2
2017-12-09 21:57:31,904 -> 01.Indexing points,7.07,59143,10.0,21
2017-12-09 21:57:35,733 -> 02.Indexing centers,3.83,67446,10.0,21
2017-12-09 21:57:35,742 -> 1024
2017-12-09 21:57:35,750 -> 1024
2017-12-09 21:57:45,323 -> 03.Joining datasets,9.57,67446,10.0,21
Done!!! Sat Dec  9 21:57:45 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 21:57:48,203 -> Starting session,1.69,0
2017-12-09 21:57:48,204 -> Setting variables,0.00,0
2017-12-09 21:57:54,120 -> Reading datasets,5.92,0
2017-12-09 21:57:54,134 -> Points partitions: 2
2017-12-09 21:57:54,144 -> Centers partitions: 2
2017-12-09 21:58:01,395 -> 01.Indexing points,7.21,59143,10.0,21
2017-12-09 21:58:05,005 -> 02.Indexing centers,3.61,67446,10.0,21
2017-12-09 21:58:05,013 -> 1024
2017-12-09 21:58:05,018 -> 1024
2017-12-09 21:58:14,724 -> 03.Joining datasets,9.71,67446,10.0,21
Done!!! Sat Dec  9 21:58:15 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 21:58:17,428 -> Starting session,1.61,0
2017-12-09 21:58:17,429 -> Setting variables,0.00,0
2017-12-09 21:58:23,399 -> Reading datasets,5.97,0
2017-12-09 21:58:23,417 -> Points partitions: 2
2017-12-09 21:58:23,428 -> Centers partitions: 2
2017-12-09 21:58:30,835 -> 01.Indexing points,7.36,59143,10.0,21
2017-12-09 21:58:34,686 -> 02.Indexing centers,3.85,67446,10.0,21
2017-12-09 21:58:34,697 -> 1024
2017-12-09 21:58:34,705 -> 1024
2017-12-09 21:58:44,299 -> 03.Joining datasets,9.59,67446,10.0,21
Done!!! Sat Dec  9 21:58:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 21:58:52,852 -> Starting session,1.86,0
2017-12-09 21:58:52,852 -> Setting variables,0.00,0
2017-12-09 21:59:00,192 -> Reading datasets,7.34,0
2017-12-09 21:59:00,204 -> Points partitions: 2
2017-12-09 21:59:00,212 -> Centers partitions: 2
2017-12-09 21:59:08,333 -> 01.Indexing points,8.08,59143,10.0,28
2017-12-09 21:59:12,378 -> 02.Indexing centers,4.04,67446,10.0,28
2017-12-09 21:59:12,387 -> 1024
2017-12-09 21:59:12,393 -> 1024
2017-12-09 21:59:21,402 -> 03.Joining datasets,9.01,67446,10.0,28
Done!!! Sat Dec  9 21:59:21 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 21:59:24,210 -> Starting session,1.60,0
2017-12-09 21:59:24,211 -> Setting variables,0.00,0
2017-12-09 21:59:31,440 -> Reading datasets,7.23,0
2017-12-09 21:59:31,452 -> Points partitions: 2
2017-12-09 21:59:31,459 -> Centers partitions: 2
2017-12-09 21:59:39,089 -> 01.Indexing points,7.59,59143,10.0,28
2017-12-09 21:59:42,964 -> 02.Indexing centers,3.87,67446,10.0,28
2017-12-09 21:59:42,971 -> 1024
2017-12-09 21:59:42,978 -> 1024
2017-12-09 21:59:51,927 -> 03.Joining datasets,8.95,67446,10.0,28
Done!!! Sat Dec  9 21:59:52 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 21:59:54,790 -> Starting session,1.63,0
2017-12-09 21:59:54,790 -> Setting variables,0.00,0
2017-12-09 22:00:00,865 -> Reading datasets,6.07,0
2017-12-09 22:00:00,878 -> Points partitions: 2
2017-12-09 22:00:00,886 -> Centers partitions: 2
2017-12-09 22:00:10,451 -> 01.Indexing points,9.52,59143,10.0,28
2017-12-09 22:00:14,223 -> 02.Indexing centers,3.77,67446,10.0,28
2017-12-09 22:00:14,231 -> 1024
2017-12-09 22:00:14,236 -> 1024
2017-12-09 22:00:23,586 -> 03.Joining datasets,9.35,67446,10.0,28
Done!!! Sat Dec  9 22:00:24 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 22:00:27,182 -> Starting session,1.59,0
2017-12-09 22:00:27,183 -> Setting variables,0.00,0
2017-12-09 22:00:32,163 -> Reading datasets,4.98,0
2017-12-09 22:00:32,176 -> Points partitions: 2
2017-12-09 22:00:32,185 -> Centers partitions: 2
2017-12-09 22:00:42,477 -> 01.Indexing points,10.25,59143,10.0,28
2017-12-09 22:00:46,758 -> 02.Indexing centers,4.28,67446,10.0,28
2017-12-09 22:00:46,769 -> 1024
2017-12-09 22:00:46,778 -> 1024
2017-12-09 22:00:56,074 -> 03.Joining datasets,9.30,67446,10.0,28
Done!!! Sat Dec  9 22:00:56 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 22:00:58,915 -> Starting session,1.66,0
2017-12-09 22:00:58,916 -> Setting variables,0.00,0
2017-12-09 22:01:05,922 -> Reading datasets,7.01,0
2017-12-09 22:01:05,935 -> Points partitions: 2
2017-12-09 22:01:05,944 -> Centers partitions: 2
2017-12-09 22:01:14,083 -> 01.Indexing points,8.10,59143,10.0,28
2017-12-09 22:01:17,996 -> 02.Indexing centers,3.91,67446,10.0,28
2017-12-09 22:01:18,004 -> 1024
2017-12-09 22:01:18,010 -> 1024
2017-12-09 22:01:26,222 -> 03.Joining datasets,8.21,67446,10.0,28
Done!!! Sat Dec  9 22:01:26 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 22:01:34,495 -> Starting session,1.57,0
2017-12-09 22:01:34,495 -> Setting variables,0.00,0
2017-12-09 22:01:38,684 -> Reading datasets,4.19,0
2017-12-09 22:01:38,697 -> Points partitions: 2
2017-12-09 22:01:38,706 -> Centers partitions: 2
2017-12-09 22:01:46,000 -> 01.Indexing points,7.25,39429,10.0,7
2017-12-09 22:01:50,169 -> 02.Indexing centers,4.17,44964,10.0,7
2017-12-09 22:01:50,179 -> 1024
2017-12-09 22:01:50,185 -> 1024
2017-12-09 22:02:03,986 -> 03.Joining datasets,13.80,44964,10.0,7
Done!!! Sat Dec  9 22:02:04 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 22:02:06,553 -> Starting session,1.54,0
2017-12-09 22:02:06,554 -> Setting variables,0.00,0
2017-12-09 22:02:10,815 -> Reading datasets,4.26,0
2017-12-09 22:02:10,828 -> Points partitions: 2
2017-12-09 22:02:10,838 -> Centers partitions: 2
2017-12-09 22:02:18,043 -> 01.Indexing points,7.16,39429,10.0,7
2017-12-09 22:02:22,351 -> 02.Indexing centers,4.31,44964,10.0,7
2017-12-09 22:02:22,358 -> 1024
2017-12-09 22:02:22,364 -> 1024
2017-12-09 22:02:35,867 -> 03.Joining datasets,13.50,44964,10.0,7
Done!!! Sat Dec  9 22:02:36 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 22:02:38,419 -> Starting session,1.53,0
2017-12-09 22:02:38,419 -> Setting variables,0.00,0
2017-12-09 22:02:42,512 -> Reading datasets,4.09,0
2017-12-09 22:02:42,523 -> Points partitions: 2
2017-12-09 22:02:42,530 -> Centers partitions: 2
2017-12-09 22:02:49,553 -> 01.Indexing points,6.96,39429,10.0,7
2017-12-09 22:02:53,837 -> 02.Indexing centers,4.28,44964,10.0,7
2017-12-09 22:02:53,845 -> 1024
2017-12-09 22:02:53,851 -> 1024
2017-12-09 22:03:07,343 -> 03.Joining datasets,13.49,44964,10.0,7
Done!!! Sat Dec  9 22:03:07 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 22:03:09,979 -> Starting session,1.63,0
2017-12-09 22:03:09,980 -> Setting variables,0.00,0
2017-12-09 22:03:13,947 -> Reading datasets,3.97,0
2017-12-09 22:03:13,959 -> Points partitions: 2
2017-12-09 22:03:13,969 -> Centers partitions: 2
2017-12-09 22:03:20,837 -> 01.Indexing points,6.83,39429,10.0,7
2017-12-09 22:03:25,049 -> 02.Indexing centers,4.21,44964,10.0,7
2017-12-09 22:03:25,059 -> 1024
2017-12-09 22:03:25,066 -> 1024
2017-12-09 22:03:38,241 -> 03.Joining datasets,13.17,44964,10.0,7
Done!!! Sat Dec  9 22:03:38 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 22:03:40,688 -> Starting session,1.48,0
2017-12-09 22:03:40,688 -> Setting variables,0.00,0
2017-12-09 22:03:44,573 -> Reading datasets,3.88,0
2017-12-09 22:03:44,586 -> Points partitions: 2
2017-12-09 22:03:44,596 -> Centers partitions: 2
2017-12-09 22:03:51,276 -> 01.Indexing points,6.63,39429,10.0,7
2017-12-09 22:03:55,436 -> 02.Indexing centers,4.16,44964,10.0,7
2017-12-09 22:03:55,444 -> 1024
2017-12-09 22:03:55,449 -> 1024
2017-12-09 22:04:08,035 -> 03.Joining datasets,12.59,44964,10.0,7
Done!!! Sat Dec  9 22:04:08 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 22:04:16,197 -> Starting session,1.77,0
2017-12-09 22:04:16,198 -> Setting variables,0.00,0
2017-12-09 22:04:21,214 -> Reading datasets,5.01,0
2017-12-09 22:04:21,227 -> Points partitions: 2
2017-12-09 22:04:21,235 -> Centers partitions: 2
2017-12-09 22:04:29,276 -> 01.Indexing points,7.99,39429,10.0,14
2017-12-09 22:04:33,950 -> 02.Indexing centers,4.67,44964,10.0,14
2017-12-09 22:04:33,961 -> 1024
2017-12-09 22:04:33,969 -> 1024
2017-12-09 22:04:45,316 -> 03.Joining datasets,11.44,44964,10.0,14
Done!!! Sat Dec  9 22:04:45 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 22:04:48,157 -> Starting session,1.59,0
2017-12-09 22:04:48,158 -> Setting variables,0.00,0
2017-12-09 22:04:53,117 -> Reading datasets,4.96,0
2017-12-09 22:04:53,131 -> Points partitions: 2
2017-12-09 22:04:53,141 -> Centers partitions: 2
2017-12-09 22:05:00,988 -> 01.Indexing points,7.80,39429,10.0,14
2017-12-09 22:05:05,437 -> 02.Indexing centers,4.45,44964,10.0,14
2017-12-09 22:05:05,449 -> 1024
2017-12-09 22:05:05,458 -> 1024
2017-12-09 22:05:16,955 -> 03.Joining datasets,11.50,44964,10.0,14
Done!!! Sat Dec  9 22:05:17 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 22:05:19,813 -> Starting session,1.64,0
2017-12-09 22:05:19,814 -> Setting variables,0.00,0
2017-12-09 22:05:24,806 -> Reading datasets,4.99,0
2017-12-09 22:05:24,821 -> Points partitions: 2
2017-12-09 22:05:24,831 -> Centers partitions: 2
2017-12-09 22:05:32,384 -> 01.Indexing points,7.51,39429,10.0,14
2017-12-09 22:05:36,683 -> 02.Indexing centers,4.30,44964,10.0,14
2017-12-09 22:05:36,691 -> 1024
2017-12-09 22:05:36,697 -> 1024
2017-12-09 22:05:47,812 -> 03.Joining datasets,11.11,44964,10.0,14
Done!!! Sat Dec  9 22:05:49 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 22:05:51,477 -> Starting session,1.65,0
2017-12-09 22:05:51,477 -> Setting variables,0.00,0
2017-12-09 22:05:56,676 -> Reading datasets,5.20,0
2017-12-09 22:05:56,688 -> Points partitions: 2
2017-12-09 22:05:56,699 -> Centers partitions: 2
2017-12-09 22:06:04,570 -> 01.Indexing points,7.83,39429,10.0,14
2017-12-09 22:06:08,621 -> 02.Indexing centers,4.05,44964,10.0,14
2017-12-09 22:06:08,629 -> 1024
2017-12-09 22:06:08,636 -> 1024
2017-12-09 22:06:20,346 -> 03.Joining datasets,11.71,44964,10.0,14
Done!!! Sat Dec  9 22:06:20 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 22:06:23,213 -> Starting session,1.69,0
2017-12-09 22:06:23,213 -> Setting variables,0.00,0
2017-12-09 22:06:28,115 -> Reading datasets,4.90,0
2017-12-09 22:06:28,126 -> Points partitions: 2
2017-12-09 22:06:28,133 -> Centers partitions: 2
2017-12-09 22:06:36,107 -> 01.Indexing points,7.94,39429,10.0,14
2017-12-09 22:06:40,470 -> 02.Indexing centers,4.36,44964,10.0,14
2017-12-09 22:06:40,482 -> 1024
2017-12-09 22:06:40,491 -> 1024
2017-12-09 22:06:52,238 -> 03.Joining datasets,11.75,44964,10.0,14
Done!!! Sat Dec  9 22:06:52 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 22:07:00,552 -> Starting session,1.65,0
2017-12-09 22:07:00,553 -> Setting variables,0.00,0
2017-12-09 22:07:06,865 -> Reading datasets,6.31,0
2017-12-09 22:07:06,876 -> Points partitions: 2
2017-12-09 22:07:06,884 -> Centers partitions: 2
2017-12-09 22:07:14,025 -> 01.Indexing points,7.10,39429,10.0,21
2017-12-09 22:07:17,748 -> 02.Indexing centers,3.72,44964,10.0,21
2017-12-09 22:07:17,758 -> 1024
2017-12-09 22:07:17,768 -> 1024
2017-12-09 22:07:27,320 -> 03.Joining datasets,9.55,44964,10.0,21
Done!!! Sat Dec  9 22:07:27 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 22:07:30,176 -> Starting session,1.63,0
2017-12-09 22:07:30,177 -> Setting variables,0.00,0
2017-12-09 22:07:36,195 -> Reading datasets,6.02,0
2017-12-09 22:07:36,205 -> Points partitions: 2
2017-12-09 22:07:36,213 -> Centers partitions: 2
2017-12-09 22:07:42,737 -> 01.Indexing points,6.56,39429,10.0,21
2017-12-09 22:07:46,239 -> 02.Indexing centers,3.50,44964,10.0,21
2017-12-09 22:07:46,248 -> 1024
2017-12-09 22:07:46,259 -> 1024
2017-12-09 22:07:55,575 -> 03.Joining datasets,9.32,44964,10.0,21
Done!!! Sat Dec  9 22:07:56 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 22:07:58,375 -> Starting session,1.58,0
2017-12-09 22:07:58,376 -> Setting variables,0.00,0
2017-12-09 22:08:03,291 -> Reading datasets,4.92,0
2017-12-09 22:08:03,305 -> Points partitions: 2
2017-12-09 22:08:03,316 -> Centers partitions: 2
2017-12-09 22:08:10,722 -> 01.Indexing points,7.37,39429,10.0,21
2017-12-09 22:08:14,521 -> 02.Indexing centers,3.80,44964,10.0,21
2017-12-09 22:08:14,529 -> 1024
2017-12-09 22:08:14,536 -> 1024
2017-12-09 22:08:23,346 -> 03.Joining datasets,8.81,44964,10.0,21
Done!!! Sat Dec  9 22:08:24 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 22:08:27,135 -> Starting session,1.62,0
2017-12-09 22:08:27,136 -> Setting variables,0.00,0
2017-12-09 22:08:31,923 -> Reading datasets,4.79,0
2017-12-09 22:08:31,933 -> Points partitions: 2
2017-12-09 22:08:31,943 -> Centers partitions: 2
2017-12-09 22:08:39,375 -> 01.Indexing points,7.39,39429,10.0,21
2017-12-09 22:08:43,480 -> 02.Indexing centers,4.10,44964,10.0,21
2017-12-09 22:08:43,491 -> 1024
2017-12-09 22:08:43,500 -> 1024
2017-12-09 22:08:52,807 -> 03.Joining datasets,9.31,44964,10.0,21
Done!!! Sat Dec  9 22:08:53 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 22:08:55,609 -> Starting session,1.69,0
2017-12-09 22:08:55,610 -> Setting variables,0.00,0
2017-12-09 22:09:01,449 -> Reading datasets,5.84,0
2017-12-09 22:09:01,461 -> Points partitions: 2
2017-12-09 22:09:01,472 -> Centers partitions: 2
2017-12-09 22:09:08,296 -> 01.Indexing points,6.78,39429,10.0,21
2017-12-09 22:09:11,870 -> 02.Indexing centers,3.57,44964,10.0,21
2017-12-09 22:09:11,880 -> 1024
2017-12-09 22:09:11,888 -> 1024
2017-12-09 22:09:20,835 -> 03.Joining datasets,8.95,44964,10.0,21
Done!!! Sat Dec  9 22:09:21 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 22:09:29,221 -> Starting session,1.87,0
2017-12-09 22:09:29,222 -> Setting variables,0.00,0
2017-12-09 22:09:36,154 -> Reading datasets,6.93,0
2017-12-09 22:09:36,166 -> Points partitions: 2
2017-12-09 22:09:36,177 -> Centers partitions: 2
2017-12-09 22:09:43,428 -> 01.Indexing points,7.07,39429,10.0,28
2017-12-09 22:09:46,500 -> 02.Indexing centers,3.07,44964,10.0,28
2017-12-09 22:09:46,507 -> 1024
2017-12-09 22:09:46,513 -> 1024
2017-12-09 22:09:55,045 -> 03.Joining datasets,8.53,44964,10.0,28
Done!!! Sat Dec  9 22:09:55 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 22:09:57,942 -> Starting session,1.65,0
2017-12-09 22:09:57,943 -> Setting variables,0.00,0
2017-12-09 22:10:05,065 -> Reading datasets,7.12,0
2017-12-09 22:10:05,078 -> Points partitions: 2
2017-12-09 22:10:05,088 -> Centers partitions: 2
2017-12-09 22:10:12,847 -> 01.Indexing points,7.72,39429,10.0,28
2017-12-09 22:10:16,572 -> 02.Indexing centers,3.72,44964,10.0,28
2017-12-09 22:10:16,581 -> 1024
2017-12-09 22:10:16,587 -> 1024
2017-12-09 22:10:24,110 -> 03.Joining datasets,7.52,44964,10.0,28
Done!!! Sat Dec  9 22:10:24 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 22:10:27,177 -> Starting session,1.71,0
2017-12-09 22:10:27,177 -> Setting variables,0.00,0
2017-12-09 22:10:34,153 -> Reading datasets,6.98,0
2017-12-09 22:10:34,164 -> Points partitions: 2
2017-12-09 22:10:34,174 -> Centers partitions: 2
2017-12-09 22:10:42,549 -> 01.Indexing points,8.33,39429,10.0,28
2017-12-09 22:10:45,826 -> 02.Indexing centers,3.27,44964,10.0,28
2017-12-09 22:10:45,834 -> 1024
2017-12-09 22:10:45,841 -> 1024
2017-12-09 22:10:54,681 -> 03.Joining datasets,8.84,44964,10.0,28
Done!!! Sat Dec  9 22:10:55 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 22:10:57,556 -> Starting session,1.62,0
2017-12-09 22:10:57,559 -> Setting variables,0.00,0
2017-12-09 22:11:04,908 -> Reading datasets,7.35,0
2017-12-09 22:11:04,921 -> Points partitions: 2
2017-12-09 22:11:04,929 -> Centers partitions: 2
2017-12-09 22:11:12,914 -> 01.Indexing points,7.94,39429,10.0,28
2017-12-09 22:11:16,640 -> 02.Indexing centers,3.72,44964,10.0,28
2017-12-09 22:11:16,649 -> 1024
2017-12-09 22:11:16,657 -> 1024
2017-12-09 22:11:24,954 -> 03.Joining datasets,8.30,44964,10.0,28
Done!!! Sat Dec  9 22:11:25 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 22:11:27,799 -> Starting session,1.64,0
2017-12-09 22:11:27,799 -> Setting variables,0.00,0
2017-12-09 22:11:34,881 -> Reading datasets,7.08,0
2017-12-09 22:11:34,894 -> Points partitions: 2
2017-12-09 22:11:34,904 -> Centers partitions: 2
2017-12-09 22:11:41,984 -> 01.Indexing points,7.04,39429,10.0,28
2017-12-09 22:11:45,697 -> 02.Indexing centers,3.71,44964,10.0,28
2017-12-09 22:11:45,713 -> 1024
2017-12-09 22:11:45,722 -> 1024
2017-12-09 22:11:53,695 -> 03.Joining datasets,7.97,44964,10.0,28
Done!!! Sat Dec  9 22:11:54 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 22:12:01,752 -> Starting session,1.47,0
2017-12-09 22:12:01,753 -> Setting variables,0.00,0
2017-12-09 22:12:05,891 -> Reading datasets,4.14,0
2017-12-09 22:12:05,904 -> Points partitions: 2
2017-12-09 22:12:05,915 -> Centers partitions: 2
2017-12-09 22:12:12,749 -> 01.Indexing points,6.78,19715,10.0,7
2017-12-09 22:12:16,798 -> 02.Indexing centers,4.05,22482,10.0,7
2017-12-09 22:12:16,806 -> 992
2017-12-09 22:12:16,812 -> 1024
2017-12-09 22:12:28,922 -> 03.Joining datasets,12.11,22482,10.0,7
Done!!! Sat Dec  9 22:12:29 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 22:12:31,428 -> Starting session,1.54,0
2017-12-09 22:12:31,429 -> Setting variables,0.00,0
2017-12-09 22:12:35,525 -> Reading datasets,4.10,0
2017-12-09 22:12:35,538 -> Points partitions: 2
2017-12-09 22:12:35,548 -> Centers partitions: 2
2017-12-09 22:12:42,072 -> 01.Indexing points,6.48,19715,10.0,7
2017-12-09 22:12:46,178 -> 02.Indexing centers,4.11,22482,10.0,7
2017-12-09 22:12:46,186 -> 992
2017-12-09 22:12:46,191 -> 1024
2017-12-09 22:12:57,879 -> 03.Joining datasets,11.69,22482,10.0,7
Done!!! Sat Dec  9 22:12:58 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 22:13:00,439 -> Starting session,1.45,0
2017-12-09 22:13:00,439 -> Setting variables,0.00,0
2017-12-09 22:13:04,576 -> Reading datasets,4.14,0
2017-12-09 22:13:04,591 -> Points partitions: 2
2017-12-09 22:13:04,601 -> Centers partitions: 2
2017-12-09 22:13:11,394 -> 01.Indexing points,6.74,19715,10.0,7
2017-12-09 22:13:15,413 -> 02.Indexing centers,4.02,22482,10.0,7
2017-12-09 22:13:15,429 -> 992
2017-12-09 22:13:15,435 -> 1024
2017-12-09 22:13:27,449 -> 03.Joining datasets,12.01,22482,10.0,7
Done!!! Sat Dec  9 22:13:27 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 22:13:29,971 -> Starting session,1.54,0
2017-12-09 22:13:29,971 -> Setting variables,0.00,0
2017-12-09 22:13:34,262 -> Reading datasets,4.29,0
2017-12-09 22:13:34,275 -> Points partitions: 2
2017-12-09 22:13:34,286 -> Centers partitions: 2
2017-12-09 22:13:41,122 -> 01.Indexing points,6.80,19715,10.0,7
2017-12-09 22:13:45,270 -> 02.Indexing centers,4.15,22482,10.0,7
2017-12-09 22:13:45,278 -> 992
2017-12-09 22:13:45,284 -> 1024
2017-12-09 22:13:57,578 -> 03.Joining datasets,12.29,22482,10.0,7
Done!!! Sat Dec  9 22:13:57 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 22:14:00,092 -> Starting session,1.49,0
2017-12-09 22:14:00,093 -> Setting variables,0.00,0
2017-12-09 22:14:04,202 -> Reading datasets,4.11,0
2017-12-09 22:14:04,213 -> Points partitions: 2
2017-12-09 22:14:04,220 -> Centers partitions: 2
2017-12-09 22:14:10,925 -> 01.Indexing points,6.67,19715,10.0,7
2017-12-09 22:14:15,123 -> 02.Indexing centers,4.20,22482,10.0,7
2017-12-09 22:14:15,131 -> 992
2017-12-09 22:14:15,137 -> 1024
2017-12-09 22:14:27,216 -> 03.Joining datasets,12.08,22482,10.0,7
Done!!! Sat Dec  9 22:14:27 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 22:14:35,435 -> Starting session,1.66,0
2017-12-09 22:14:35,435 -> Setting variables,0.00,0
2017-12-09 22:14:40,399 -> Reading datasets,4.96,0
2017-12-09 22:14:40,413 -> Points partitions: 2
2017-12-09 22:14:40,423 -> Centers partitions: 2
2017-12-09 22:14:47,919 -> 01.Indexing points,7.45,19715,10.0,14
2017-12-09 22:14:51,911 -> 02.Indexing centers,3.99,22482,10.0,14
2017-12-09 22:14:51,919 -> 992
2017-12-09 22:14:51,924 -> 1024
2017-12-09 22:15:02,375 -> 03.Joining datasets,10.45,22482,10.0,14
Done!!! Sat Dec  9 22:15:03 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 22:15:05,901 -> Starting session,1.67,0
2017-12-09 22:15:05,901 -> Setting variables,0.00,0
2017-12-09 22:15:10,813 -> Reading datasets,4.91,0
2017-12-09 22:15:10,827 -> Points partitions: 2
2017-12-09 22:15:10,837 -> Centers partitions: 2
2017-12-09 22:15:17,977 -> 01.Indexing points,7.10,19715,10.0,14
2017-12-09 22:15:22,067 -> 02.Indexing centers,4.09,22482,10.0,14
2017-12-09 22:15:22,079 -> 992
2017-12-09 22:15:22,088 -> 1024
2017-12-09 22:15:32,316 -> 03.Joining datasets,10.23,22482,10.0,14
Done!!! Sat Dec  9 22:15:33 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 22:15:35,912 -> Starting session,1.75,0
2017-12-09 22:15:35,912 -> Setting variables,0.00,0
2017-12-09 22:15:40,784 -> Reading datasets,4.87,0
2017-12-09 22:15:40,798 -> Points partitions: 2
2017-12-09 22:15:40,808 -> Centers partitions: 2
2017-12-09 22:15:48,358 -> 01.Indexing points,7.51,19715,10.0,14
2017-12-09 22:15:52,272 -> 02.Indexing centers,3.91,22482,10.0,14
2017-12-09 22:15:52,280 -> 992
2017-12-09 22:15:52,285 -> 1024
2017-12-09 22:16:02,527 -> 03.Joining datasets,10.24,22482,10.0,14
Done!!! Sat Dec  9 22:16:03 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 22:16:05,427 -> Starting session,1.67,0
2017-12-09 22:16:05,428 -> Setting variables,0.00,0
2017-12-09 22:16:10,229 -> Reading datasets,4.80,0
2017-12-09 22:16:10,242 -> Points partitions: 2
2017-12-09 22:16:10,250 -> Centers partitions: 2
2017-12-09 22:16:17,480 -> 01.Indexing points,7.19,19715,10.0,14
2017-12-09 22:16:21,303 -> 02.Indexing centers,3.82,22482,10.0,14
2017-12-09 22:16:21,311 -> 992
2017-12-09 22:16:21,316 -> 1024
2017-12-09 22:16:30,860 -> 03.Joining datasets,9.54,22482,10.0,14
Done!!! Sat Dec  9 22:16:31 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 22:16:33,578 -> Starting session,1.63,0
2017-12-09 22:16:33,579 -> Setting variables,0.00,0
2017-12-09 22:16:38,424 -> Reading datasets,4.84,0
2017-12-09 22:16:38,436 -> Points partitions: 2
2017-12-09 22:16:38,443 -> Centers partitions: 2
2017-12-09 22:16:45,638 -> 01.Indexing points,7.15,19715,10.0,14
2017-12-09 22:16:49,835 -> 02.Indexing centers,4.20,22482,10.0,14
2017-12-09 22:16:49,846 -> 992
2017-12-09 22:16:49,854 -> 1024
2017-12-09 22:16:59,926 -> 03.Joining datasets,10.07,22482,10.0,14
Done!!! Sat Dec  9 22:17:01 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 22:17:08,934 -> Starting session,1.65,0
2017-12-09 22:17:08,937 -> Setting variables,0.00,0
2017-12-09 22:17:14,642 -> Reading datasets,5.71,0
2017-12-09 22:17:14,655 -> Points partitions: 2
2017-12-09 22:17:14,665 -> Centers partitions: 2
2017-12-09 22:17:22,676 -> 01.Indexing points,7.97,19715,10.0,21
2017-12-09 22:17:26,479 -> 02.Indexing centers,3.80,22482,10.0,21
2017-12-09 22:17:26,487 -> 992
2017-12-09 22:17:26,492 -> 1024
2017-12-09 22:17:35,668 -> 03.Joining datasets,9.18,22482,10.0,21
Done!!! Sat Dec  9 22:17:36 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 22:17:38,524 -> Starting session,1.67,0
2017-12-09 22:17:38,525 -> Setting variables,0.00,0
2017-12-09 22:17:43,407 -> Reading datasets,4.88,0
2017-12-09 22:17:43,417 -> Points partitions: 2
2017-12-09 22:17:43,424 -> Centers partitions: 2
2017-12-09 22:17:51,488 -> 01.Indexing points,8.03,19715,10.0,21
2017-12-09 22:17:55,228 -> 02.Indexing centers,3.74,22482,10.0,21
2017-12-09 22:17:55,239 -> 992
2017-12-09 22:17:55,247 -> 1024
2017-12-09 22:18:05,559 -> 03.Joining datasets,10.31,22482,10.0,21
Done!!! Sat Dec  9 22:18:06 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 22:18:08,340 -> Starting session,1.58,0
2017-12-09 22:18:08,341 -> Setting variables,0.00,0
2017-12-09 22:18:14,345 -> Reading datasets,6.00,0
2017-12-09 22:18:14,360 -> Points partitions: 2
2017-12-09 22:18:14,370 -> Centers partitions: 2
2017-12-09 22:18:22,403 -> 01.Indexing points,7.99,19715,10.0,21
2017-12-09 22:18:26,528 -> 02.Indexing centers,4.12,22482,10.0,21
2017-12-09 22:18:26,539 -> 992
2017-12-09 22:18:26,547 -> 1024
2017-12-09 22:18:35,651 -> 03.Joining datasets,9.10,22482,10.0,21
Done!!! Sat Dec  9 22:18:36 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 22:18:38,472 -> Starting session,1.74,0
2017-12-09 22:18:38,473 -> Setting variables,0.00,0
2017-12-09 22:18:44,230 -> Reading datasets,5.76,0
2017-12-09 22:18:44,241 -> Points partitions: 2
2017-12-09 22:18:44,249 -> Centers partitions: 2
2017-12-09 22:18:51,751 -> 01.Indexing points,7.46,19715,10.0,21
2017-12-09 22:18:55,649 -> 02.Indexing centers,3.90,22482,10.0,21
2017-12-09 22:18:55,657 -> 992
2017-12-09 22:18:55,663 -> 1024
2017-12-09 22:19:05,644 -> 03.Joining datasets,9.98,22482,10.0,21
Done!!! Sat Dec  9 22:19:06 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 22:19:08,611 -> Starting session,1.72,0
2017-12-09 22:19:08,612 -> Setting variables,0.00,0
2017-12-09 22:19:14,290 -> Reading datasets,5.68,0
2017-12-09 22:19:14,306 -> Points partitions: 2
2017-12-09 22:19:14,316 -> Centers partitions: 2
2017-12-09 22:19:22,312 -> 01.Indexing points,7.95,19715,10.0,21
2017-12-09 22:19:26,944 -> 02.Indexing centers,3.67,22482,10.0,21
2017-12-09 22:19:26,955 -> 992
2017-12-09 22:19:26,963 -> 1024
2017-12-09 22:19:37,588 -> 03.Joining datasets,10.62,22482,10.0,21
Done!!! Sat Dec  9 22:19:38 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 22:19:46,149 -> Starting session,1.90,0
2017-12-09 22:19:46,150 -> Setting variables,0.00,0
2017-12-09 22:19:52,088 -> Reading datasets,5.94,0
2017-12-09 22:19:52,103 -> Points partitions: 2
2017-12-09 22:19:52,113 -> Centers partitions: 2
2017-12-09 22:20:00,062 -> 01.Indexing points,7.91,19715,10.0,28
2017-12-09 22:20:04,339 -> 02.Indexing centers,4.28,22482,10.0,28
2017-12-09 22:20:04,350 -> 992
2017-12-09 22:20:04,358 -> 1024
2017-12-09 22:20:13,069 -> 03.Joining datasets,8.71,22482,10.0,28
Done!!! Sat Dec  9 22:20:13 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 22:20:15,889 -> Starting session,1.59,0
2017-12-09 22:20:15,889 -> Setting variables,0.00,0
2017-12-09 22:20:22,953 -> Reading datasets,7.06,0
2017-12-09 22:20:22,967 -> Points partitions: 2
2017-12-09 22:20:22,979 -> Centers partitions: 2
2017-12-09 22:20:31,982 -> 01.Indexing points,8.95,19715,10.0,28
2017-12-09 22:20:36,062 -> 02.Indexing centers,4.08,22482,10.0,28
2017-12-09 22:20:36,071 -> 992
2017-12-09 22:20:36,077 -> 1024
2017-12-09 22:20:45,783 -> 03.Joining datasets,9.71,22482,10.0,28
Done!!! Sat Dec  9 22:20:46 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 22:20:48,586 -> Starting session,1.72,0
2017-12-09 22:20:48,587 -> Setting variables,0.00,0
2017-12-09 22:20:54,750 -> Reading datasets,6.16,0
2017-12-09 22:20:54,764 -> Points partitions: 2
2017-12-09 22:20:54,774 -> Centers partitions: 2
2017-12-09 22:21:03,481 -> 01.Indexing points,8.74,19715,10.0,28
2017-12-09 22:21:08,210 -> 02.Indexing centers,4.73,22482,10.0,28
2017-12-09 22:21:08,217 -> 992
2017-12-09 22:21:08,222 -> 1024
2017-12-09 22:21:18,142 -> 03.Joining datasets,9.92,22482,10.0,28
Done!!! Sat Dec  9 22:21:18 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 22:21:20,898 -> Starting session,1.70,0
2017-12-09 22:21:20,899 -> Setting variables,0.00,0
2017-12-09 22:21:27,972 -> Reading datasets,7.07,0
2017-12-09 22:21:27,984 -> Points partitions: 2
2017-12-09 22:21:27,993 -> Centers partitions: 2
2017-12-09 22:21:34,446 -> 01.Indexing points,6.41,19715,10.0,28
2017-12-09 22:21:38,208 -> 02.Indexing centers,3.76,22482,10.0,28
2017-12-09 22:21:38,220 -> 992
2017-12-09 22:21:38,230 -> 1024
2017-12-09 22:21:45,854 -> 03.Joining datasets,7.62,22482,10.0,28
Done!!! Sat Dec  9 22:21:46 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 22:21:48,691 -> Starting session,1.63,0
2017-12-09 22:21:48,692 -> Setting variables,0.00,0
2017-12-09 22:21:55,762 -> Reading datasets,7.07,0
2017-12-09 22:21:55,799 -> Points partitions: 2
2017-12-09 22:21:55,809 -> Centers partitions: 2
2017-12-09 22:22:04,493 -> 01.Indexing points,8.64,19715,10.0,28
2017-12-09 22:22:08,574 -> 02.Indexing centers,4.08,22482,10.0,28
2017-12-09 22:22:08,582 -> 992
2017-12-09 22:22:08,588 -> 1024
2017-12-09 22:22:17,708 -> 03.Joining datasets,9.12,22482,10.0,28
Done!!! Sat Dec  9 22:22:18 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
