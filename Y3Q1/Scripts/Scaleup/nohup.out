acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:44 PST 2017
Running iteration 1/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:46 PST 2017
Running iteration 1/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:47 PST 2017
Running iteration 1/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:49 PST 2017
Running iteration 1/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:51 PST 2017
Running iteration 2/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:52 PST 2017
Running iteration 2/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:54 PST 2017
Running iteration 2/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:56 PST 2017
Running iteration 2/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:57 PST 2017
Running iteration 2/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:41:59 PST 2017
Running iteration 3/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:01 PST 2017
Running iteration 3/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:02 PST 2017
Running iteration 3/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:04 PST 2017
Running iteration 3/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:06 PST 2017
Running iteration 3/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:07 PST 2017
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:14 PST 2017
Running iteration 1/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:16 PST 2017
Running iteration 1/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:18 PST 2017
Running iteration 1/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:19 PST 2017
Running iteration 1/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:21 PST 2017
Running iteration 2/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:23 PST 2017
Running iteration 2/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:24 PST 2017
Running iteration 2/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:26 PST 2017
Running iteration 2/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:28 PST 2017
Running iteration 2/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:29 PST 2017
Running iteration 3/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:31 PST 2017
Running iteration 3/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:33 PST 2017
Running iteration 3/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:34 PST 2017
Running iteration 3/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:36 PST 2017
Running iteration 3/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:38 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
Running iteration 2/5 for 7 cores...
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:45 PST 2017
Running iteration 1/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:47 PST 2017
Running iteration 1/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:48 PST 2017
Running iteration 1/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:50 PST 2017
Running iteration 1/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:52 PST 2017
Running iteration 2/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:53 PST 2017
Running iteration 2/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:55 PST 2017
Running iteration 2/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:56 PST 2017
Running iteration 2/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:42:58 PST 2017
Running iteration 2/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:00 PST 2017
Running iteration 3/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:01 PST 2017
Running iteration 3/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:03 PST 2017
Running iteration 3/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:05 PST 2017
Running iteration 3/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:06 PST 2017
Running iteration 3/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:08 PST 2017
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:15 PST 2017
Running iteration 1/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:17 PST 2017
Running iteration 1/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:18 PST 2017
Running iteration 1/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:20 PST 2017
Running iteration 1/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:22 PST 2017
Running iteration 2/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:23 PST 2017
Running iteration 2/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:25 PST 2017
Running iteration 2/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:26 PST 2017
Running iteration 2/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:28 PST 2017
Running iteration 2/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:30 PST 2017
Running iteration 3/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:31 PST 2017
Running iteration 3/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:33 PST 2017
Running iteration 3/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:35 PST 2017
Running iteration 3/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:36 PST 2017
Running iteration 3/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:38 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
Running iteration 3/5 for 7 cores...
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:45 PST 2017
Running iteration 1/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:47 PST 2017
Running iteration 1/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:49 PST 2017
Running iteration 1/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:50 PST 2017
Running iteration 1/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:52 PST 2017
Running iteration 2/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:54 PST 2017
Running iteration 2/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:55 PST 2017
Running iteration 2/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:57 PST 2017
Running iteration 2/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:43:59 PST 2017
Running iteration 2/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:00 PST 2017
Running iteration 3/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:02 PST 2017
Running iteration 3/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:03 PST 2017
Running iteration 3/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:05 PST 2017
Running iteration 3/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:07 PST 2017
Running iteration 3/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:08 PST 2017
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:16 PST 2017
Running iteration 1/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:17 PST 2017
Running iteration 1/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:19 PST 2017
Running iteration 1/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:20 PST 2017
Running iteration 1/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:22 PST 2017
Running iteration 2/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:24 PST 2017
Running iteration 2/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:25 PST 2017
Running iteration 2/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:27 PST 2017
Running iteration 2/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:28 PST 2017
Running iteration 2/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:30 PST 2017
Running iteration 3/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:32 PST 2017
Running iteration 3/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:33 PST 2017
Running iteration 3/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:35 PST 2017
Running iteration 3/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:37 PST 2017
Running iteration 3/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:38 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
Running iteration 4/5 for 7 cores...
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:46 PST 2017
Running iteration 1/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:47 PST 2017
Running iteration 1/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:49 PST 2017
Running iteration 1/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:51 PST 2017
Running iteration 1/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:52 PST 2017
Running iteration 2/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:54 PST 2017
Running iteration 2/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:56 PST 2017
Running iteration 2/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:57 PST 2017
Running iteration 2/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:44:59 PST 2017
Running iteration 2/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:00 PST 2017
Running iteration 3/3 for B20K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:02 PST 2017
Running iteration 3/3 for B20K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:04 PST 2017
Running iteration 3/3 for B20K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:05 PST 2017
Running iteration 3/3 for B20K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:07 PST 2017
Running iteration 3/3 for B20K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:09 PST 2017
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:16 PST 2017
Running iteration 1/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:17 PST 2017
Running iteration 1/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:19 PST 2017
Running iteration 1/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:21 PST 2017
Running iteration 1/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:22 PST 2017
Running iteration 2/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:24 PST 2017
Running iteration 2/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:26 PST 2017
Running iteration 2/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:27 PST 2017
Running iteration 2/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:29 PST 2017
Running iteration 2/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:30 PST 2017
Running iteration 3/3 for B40K (epsilon = 10 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:32 PST 2017
Running iteration 3/3 for B40K (epsilon = 20 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:34 PST 2017
Running iteration 3/3 for B40K (epsilon = 30 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:35 PST 2017
Running iteration 3/3 for B40K (epsilon = 40 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:37 PST 2017
Running iteration 3/3 for B40K (epsilon = 50 , mu = 10)...
[scallop] Error: Bad arguments for option 'cores': '' - you should provide exactly one argument
Done!!! Sat Dec  9 06:45:39 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
Running iteration 5/5 for 7 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:39 PST 2017
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:46 PST 2017
Running iteration 2/5 for 14 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:46 PST 2017
Running iteration 3/5 for 14 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:47 PST 2017
Running iteration 4/5 for 14 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:48 PST 2017
Running iteration 5/5 for 14 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:48 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:55 PST 2017
Running iteration 2/5 for 21 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:55 PST 2017
Running iteration 3/5 for 21 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:56 PST 2017
Running iteration 4/5 for 21 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:57 PST 2017
Running iteration 5/5 for 21 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:45:57 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:46:04 PST 2017
Running iteration 2/5 for 28 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:46:05 PST 2017
Running iteration 3/5 for 28 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:46:05 PST 2017
Running iteration 4/5 for 28 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:46:06 PST 2017
Running iteration 5/5 for 28 cores...
Exception in thread "main" java.lang.ArrayIndexOutOfBoundsException: 0
	at BasicSpatialOps$.main(BasicSpatialOps.scala:27)
	at BasicSpatialOps.main(BasicSpatialOps.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!! Sat Dec  9 06:46:07 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 06:46:46,462 -> Starting session,1.51,0
2017-12-09 06:46:46,463 -> Setting variables,0.00,0
2017-12-09 06:46:50,471 -> Reading datasets,4.01,0
2017-12-09 06:46:50,485 -> Points partitions: 2
2017-12-09 06:46:50,496 -> Centers partitions: 2
2017-12-09 06:46:57,271 -> 01.Indexing points,6.73,19715,10,7
2017-12-09 06:47:01,297 -> 02.Indexing centers,4.03,22482,10,7
2017-12-09 06:47:01,306 -> 992
2017-12-09 06:47:01,313 -> 1024
2017-12-09 06:47:13,001 -> 03.Joining datasets,11.69,22482,10,7
Done!!! Sat Dec  9 06:47:13 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 06:47:15,503 -> Starting session,1.45,0
2017-12-09 06:47:15,504 -> Setting variables,0.00,0
2017-12-09 06:47:19,529 -> Reading datasets,4.02,0
2017-12-09 06:47:19,540 -> Points partitions: 2
2017-12-09 06:47:19,549 -> Centers partitions: 2
2017-12-09 06:47:25,971 -> 01.Indexing points,6.28,19715,10,7
2017-12-09 06:47:29,755 -> 02.Indexing centers,3.78,22482,10,7
2017-12-09 06:47:29,767 -> 992
2017-12-09 06:47:29,776 -> 1024
2017-12-09 06:47:41,065 -> 03.Joining datasets,11.29,22482,10,7
Done!!! Sat Dec  9 06:47:41 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 06:47:43,723 -> Starting session,1.49,0
2017-12-09 06:47:43,724 -> Setting variables,0.00,0
2017-12-09 06:47:47,772 -> Reading datasets,4.05,0
2017-12-09 06:47:47,783 -> Points partitions: 2
2017-12-09 06:47:47,790 -> Centers partitions: 2
2017-12-09 06:47:54,650 -> 01.Indexing points,6.82,19715,10,7
2017-12-09 06:47:58,617 -> 02.Indexing centers,3.97,22482,10,7
2017-12-09 06:47:58,628 -> 992
2017-12-09 06:47:58,635 -> 1024
2017-12-09 06:48:10,234 -> 03.Joining datasets,11.60,22482,10,7
Done!!! Sat Dec  9 06:48:10 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 06:48:12,991 -> Starting session,1.60,0
2017-12-09 06:48:12,991 -> Setting variables,0.00,0
2017-12-09 06:48:17,207 -> Reading datasets,4.22,0
2017-12-09 06:48:17,222 -> Points partitions: 2
2017-12-09 06:48:17,235 -> Centers partitions: 2
2017-12-09 06:48:24,038 -> 01.Indexing points,6.76,19715,10,7
2017-12-09 06:48:28,112 -> 02.Indexing centers,4.07,22482,10,7
2017-12-09 06:48:28,123 -> 992
2017-12-09 06:48:28,131 -> 1024
2017-12-09 06:48:39,975 -> 03.Joining datasets,11.84,22482,10,7
Done!!! Sat Dec  9 06:48:40 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 06:48:42,671 -> Starting session,1.53,0
2017-12-09 06:48:42,672 -> Setting variables,0.00,0
2017-12-09 06:48:46,854 -> Reading datasets,4.18,0
2017-12-09 06:48:46,865 -> Points partitions: 2
2017-12-09 06:48:46,873 -> Centers partitions: 2
2017-12-09 06:48:53,607 -> 01.Indexing points,6.70,19715,10,7
2017-12-09 06:48:57,587 -> 02.Indexing centers,3.98,22482,10,7
2017-12-09 06:48:57,602 -> 992
2017-12-09 06:48:57,611 -> 1024
2017-12-09 06:49:09,721 -> 03.Joining datasets,12.11,22482,10,7
Done!!! Sat Dec  9 06:49:10 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 06:49:18,046 -> Starting session,1.66,0
2017-12-09 06:49:18,047 -> Setting variables,0.00,0
2017-12-09 06:49:22,562 -> Reading datasets,4.52,0
2017-12-09 06:49:22,574 -> Points partitions: 2
2017-12-09 06:49:22,582 -> Centers partitions: 2
2017-12-09 06:49:29,454 -> 01.Indexing points,6.83,19715,10,14
2017-12-09 06:49:33,325 -> 02.Indexing centers,3.87,22482,10,14
2017-12-09 06:49:33,336 -> 992
2017-12-09 06:49:33,345 -> 1024
2017-12-09 06:49:42,749 -> 03.Joining datasets,9.40,22482,10,14
Done!!! Sat Dec  9 06:49:43 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 06:49:45,567 -> Starting session,1.64,0
2017-12-09 06:49:45,568 -> Setting variables,0.00,0
2017-12-09 06:49:50,090 -> Reading datasets,4.52,0
2017-12-09 06:49:50,103 -> Points partitions: 2
2017-12-09 06:49:50,113 -> Centers partitions: 2
2017-12-09 06:49:56,730 -> 01.Indexing points,6.57,19715,10,14
2017-12-09 06:50:00,245 -> 02.Indexing centers,3.51,22482,10,14
2017-12-09 06:50:00,254 -> 992
2017-12-09 06:50:00,261 -> 1024
2017-12-09 06:50:09,917 -> 03.Joining datasets,9.65,22482,10,14
Done!!! Sat Dec  9 06:50:10 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 06:50:12,658 -> Starting session,1.58,0
2017-12-09 06:50:12,658 -> Setting variables,0.00,0
2017-12-09 06:50:17,206 -> Reading datasets,4.55,0
2017-12-09 06:50:17,217 -> Points partitions: 2
2017-12-09 06:50:17,225 -> Centers partitions: 2
2017-12-09 06:50:24,007 -> 01.Indexing points,6.74,19715,10,14
2017-12-09 06:50:27,867 -> 02.Indexing centers,3.86,22482,10,14
2017-12-09 06:50:27,875 -> 992
2017-12-09 06:50:27,881 -> 1024
2017-12-09 06:50:37,626 -> 03.Joining datasets,9.75,22482,10,14
Done!!! Sat Dec  9 06:50:38 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 06:50:40,489 -> Starting session,1.55,0
2017-12-09 06:50:40,490 -> Setting variables,0.00,0
2017-12-09 06:50:45,026 -> Reading datasets,4.54,0
2017-12-09 06:50:45,040 -> Points partitions: 2
2017-12-09 06:50:45,050 -> Centers partitions: 2
2017-12-09 06:50:52,498 -> 01.Indexing points,7.41,19715,10,14
2017-12-09 06:50:56,292 -> 02.Indexing centers,3.79,22482,10,14
2017-12-09 06:50:56,300 -> 992
2017-12-09 06:50:56,306 -> 1024
2017-12-09 06:51:06,434 -> 03.Joining datasets,10.13,22482,10,14
Done!!! Sat Dec  9 06:51:06 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 06:51:09,254 -> Starting session,1.72,0
2017-12-09 06:51:09,255 -> Setting variables,0.00,0
2017-12-09 06:51:13,963 -> Reading datasets,4.71,0
2017-12-09 06:51:13,974 -> Points partitions: 2
2017-12-09 06:51:13,981 -> Centers partitions: 2
2017-12-09 06:51:20,976 -> 01.Indexing points,6.95,19715,10,14
2017-12-09 06:51:24,700 -> 02.Indexing centers,3.85,22482,10,14
2017-12-09 06:51:24,708 -> 992
2017-12-09 06:51:24,714 -> 1024
2017-12-09 06:51:34,128 -> 03.Joining datasets,9.41,22482,10,14
Done!!! Sat Dec  9 06:51:34 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 06:51:42,699 -> Starting session,1.70,0
2017-12-09 06:51:42,699 -> Setting variables,0.00,0
2017-12-09 06:51:47,547 -> Reading datasets,4.85,0
2017-12-09 06:51:47,561 -> Points partitions: 2
2017-12-09 06:51:47,569 -> Centers partitions: 2
2017-12-09 06:51:55,374 -> 01.Indexing points,7.77,19715,10,21
2017-12-09 06:51:58,854 -> 02.Indexing centers,3.48,22482,10,21
2017-12-09 06:51:58,862 -> 992
2017-12-09 06:51:58,868 -> 1024
2017-12-09 06:52:06,959 -> 03.Joining datasets,8.09,22482,10,21
Done!!! Sat Dec  9 06:52:07 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 06:52:09,735 -> Starting session,1.57,0
2017-12-09 06:52:09,735 -> Setting variables,0.00,0
2017-12-09 06:52:15,393 -> Reading datasets,5.66,0
2017-12-09 06:52:15,406 -> Points partitions: 2
2017-12-09 06:52:15,414 -> Centers partitions: 2
2017-12-09 06:52:22,653 -> 01.Indexing points,7.20,19715,10,21
2017-12-09 06:52:25,794 -> 02.Indexing centers,3.14,22482,10,21
2017-12-09 06:52:25,802 -> 992
2017-12-09 06:52:25,808 -> 1024
2017-12-09 06:52:34,249 -> 03.Joining datasets,8.44,22482,10,21
Done!!! Sat Dec  9 06:52:34 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 06:52:37,106 -> Starting session,1.61,0
2017-12-09 06:52:37,106 -> Setting variables,0.00,0
2017-12-09 06:52:42,675 -> Reading datasets,5.57,0
2017-12-09 06:52:42,689 -> Points partitions: 2
2017-12-09 06:52:42,697 -> Centers partitions: 2
2017-12-09 06:52:49,953 -> 01.Indexing points,7.22,19715,10,21
2017-12-09 06:52:53,596 -> 02.Indexing centers,3.64,22482,10,21
2017-12-09 06:52:53,606 -> 992
2017-12-09 06:52:53,614 -> 1024
2017-12-09 06:53:02,155 -> 03.Joining datasets,8.54,22482,10,21
Done!!! Sat Dec  9 06:53:02 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 06:53:05,042 -> Starting session,1.70,0
2017-12-09 06:53:05,042 -> Setting variables,0.00,0
2017-12-09 06:53:10,532 -> Reading datasets,5.49,0
2017-12-09 06:53:10,543 -> Points partitions: 2
2017-12-09 06:53:10,551 -> Centers partitions: 2
2017-12-09 06:53:15,653 -> 01.Indexing points,5.06,19715,10,21
2017-12-09 06:53:19,278 -> 02.Indexing centers,3.62,22482,10,21
2017-12-09 06:53:19,286 -> 992
2017-12-09 06:53:19,292 -> 1024
2017-12-09 06:53:29,043 -> 03.Joining datasets,9.75,22482,10,21
Done!!! Sat Dec  9 06:53:29 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 06:53:31,813 -> Starting session,1.56,0
2017-12-09 06:53:31,813 -> Setting variables,0.00,0
2017-12-09 06:53:37,248 -> Reading datasets,5.43,0
2017-12-09 06:53:37,267 -> Points partitions: 2
2017-12-09 06:53:37,279 -> Centers partitions: 2
2017-12-09 06:53:44,288 -> 01.Indexing points,6.95,19715,10,21
2017-12-09 06:53:48,146 -> 02.Indexing centers,3.86,22482,10,21
2017-12-09 06:53:48,158 -> 992
2017-12-09 06:53:48,167 -> 1024
2017-12-09 06:53:56,817 -> 03.Joining datasets,8.65,22482,10,21
Done!!! Sat Dec  9 06:53:57 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 06:54:05,645 -> Starting session,1.95,0
2017-12-09 06:54:05,646 -> Setting variables,0.00,0
2017-12-09 06:54:12,550 -> Reading datasets,6.90,0
2017-12-09 06:54:12,563 -> Points partitions: 2
2017-12-09 06:54:12,570 -> Centers partitions: 2
2017-12-09 06:54:20,045 -> 01.Indexing points,7.43,19715,10,28
2017-12-09 06:54:23,551 -> 02.Indexing centers,3.50,22482,10,28
2017-12-09 06:54:23,560 -> 992
2017-12-09 06:54:23,569 -> 1024
2017-12-09 06:54:33,890 -> 03.Joining datasets,10.32,22482,10,28
Done!!! Sat Dec  9 06:54:34 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 06:54:36,774 -> Starting session,1.66,0
2017-12-09 06:54:36,774 -> Setting variables,0.00,0
2017-12-09 06:54:43,290 -> Reading datasets,6.52,0
2017-12-09 06:54:43,308 -> Points partitions: 2
2017-12-09 06:54:43,319 -> Centers partitions: 2
2017-12-09 06:54:49,663 -> 01.Indexing points,6.29,19715,10,28
2017-12-09 06:54:53,180 -> 02.Indexing centers,3.52,22482,10,28
2017-12-09 06:54:53,195 -> 992
2017-12-09 06:54:53,204 -> 1024
2017-12-09 06:55:02,690 -> 03.Joining datasets,9.49,22482,10,28
Done!!! Sat Dec  9 06:55:03 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 06:55:05,457 -> Starting session,1.58,0
2017-12-09 06:55:05,458 -> Setting variables,0.00,0
2017-12-09 06:55:12,062 -> Reading datasets,6.60,0
2017-12-09 06:55:12,086 -> Points partitions: 2
2017-12-09 06:55:12,097 -> Centers partitions: 2
2017-12-09 06:55:20,104 -> 01.Indexing points,7.95,19715,10,28
2017-12-09 06:55:24,372 -> 02.Indexing centers,4.27,22482,10,28
2017-12-09 06:55:24,382 -> 992
2017-12-09 06:55:24,390 -> 1024
2017-12-09 06:55:34,017 -> 03.Joining datasets,9.63,22482,10,28
Done!!! Sat Dec  9 06:55:34 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 06:55:36,792 -> Starting session,1.57,0
2017-12-09 06:55:36,792 -> Setting variables,0.00,0
2017-12-09 06:55:41,671 -> Reading datasets,4.88,0
2017-12-09 06:55:41,681 -> Points partitions: 2
2017-12-09 06:55:41,688 -> Centers partitions: 2
2017-12-09 06:55:49,677 -> 01.Indexing points,7.95,19715,10,28
2017-12-09 06:55:54,241 -> 02.Indexing centers,4.56,22482,10,28
2017-12-09 06:55:54,255 -> 992
2017-12-09 06:55:54,265 -> 1024
2017-12-09 06:56:03,181 -> 03.Joining datasets,9.04,22482,10,28
Done!!! Sat Dec  9 06:56:03 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 06:56:06,062 -> Starting session,1.79,0
2017-12-09 06:56:06,063 -> Setting variables,0.00,0
2017-12-09 06:56:13,076 -> Reading datasets,7.01,0
2017-12-09 06:56:13,087 -> Points partitions: 2
2017-12-09 06:56:13,095 -> Centers partitions: 2
2017-12-09 06:56:20,538 -> 01.Indexing points,7.40,19715,10,28
2017-12-09 06:56:24,361 -> 02.Indexing centers,3.82,22482,10,28
2017-12-09 06:56:24,369 -> 992
2017-12-09 06:56:24,375 -> 1024
2017-12-09 06:56:32,809 -> 03.Joining datasets,8.43,22482,10,28
Done!!! Sat Dec  9 06:56:33 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 07:18:11,874 -> Starting session,1.56,0
2017-12-09 07:18:11,875 -> Setting variables,0.00,0
2017-12-09 07:18:16,200 -> Reading datasets,4.33,0
2017-12-09 07:18:16,214 -> Points partitions: 2
2017-12-09 07:18:16,224 -> Centers partitions: 2
2017-12-09 07:18:23,150 -> 01.Indexing points,6.89,19715,20,7
2017-12-09 07:18:27,466 -> 02.Indexing centers,4.31,47664,20,7
2017-12-09 07:18:27,479 -> 992
2017-12-09 07:18:27,493 -> 1024
2017-12-09 07:18:40,831 -> 03.Joining datasets,13.34,47664,20,7
Done!!! Sat Dec  9 07:18:41 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 07:18:43,405 -> Starting session,1.47,0
2017-12-09 07:18:43,406 -> Setting variables,0.00,0
2017-12-09 07:18:47,383 -> Reading datasets,3.98,0
2017-12-09 07:18:47,392 -> Points partitions: 2
2017-12-09 07:18:47,400 -> Centers partitions: 2
2017-12-09 07:18:54,352 -> 01.Indexing points,6.91,19715,20,7
2017-12-09 07:18:58,756 -> 02.Indexing centers,4.40,47664,20,7
2017-12-09 07:18:58,764 -> 992
2017-12-09 07:18:58,770 -> 1024
2017-12-09 07:19:11,317 -> 03.Joining datasets,12.55,47664,20,7
Done!!! Sat Dec  9 07:19:11 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 07:19:14,056 -> Starting session,1.67,0
2017-12-09 07:19:14,056 -> Setting variables,0.00,0
2017-12-09 07:19:18,094 -> Reading datasets,4.04,0
2017-12-09 07:19:18,111 -> Points partitions: 2
2017-12-09 07:19:18,125 -> Centers partitions: 2
2017-12-09 07:19:24,581 -> 01.Indexing points,6.41,19715,20,7
2017-12-09 07:19:29,061 -> 02.Indexing centers,4.48,47664,20,7
2017-12-09 07:19:29,072 -> 992
2017-12-09 07:19:29,080 -> 1024
2017-12-09 07:19:42,092 -> 03.Joining datasets,13.01,47664,20,7
Done!!! Sat Dec  9 07:19:42 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 07:19:44,812 -> Starting session,1.53,0
2017-12-09 07:19:44,812 -> Setting variables,0.00,0
2017-12-09 07:19:49,063 -> Reading datasets,4.25,0
2017-12-09 07:19:49,073 -> Points partitions: 2
2017-12-09 07:19:49,081 -> Centers partitions: 2
2017-12-09 07:19:56,006 -> 01.Indexing points,6.89,19715,20,7
2017-12-09 07:20:00,128 -> 02.Indexing centers,4.12,47664,20,7
2017-12-09 07:20:00,138 -> 992
2017-12-09 07:20:00,145 -> 1024
2017-12-09 07:20:13,176 -> 03.Joining datasets,13.03,47664,20,7
Done!!! Sat Dec  9 07:20:13 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 07:20:15,873 -> Starting session,1.48,0
2017-12-09 07:20:15,873 -> Setting variables,0.00,0
2017-12-09 07:20:20,020 -> Reading datasets,4.15,0
2017-12-09 07:20:20,031 -> Points partitions: 2
2017-12-09 07:20:20,039 -> Centers partitions: 2
2017-12-09 07:20:26,780 -> 01.Indexing points,6.70,19715,20,7
2017-12-09 07:20:31,110 -> 02.Indexing centers,4.33,47664,20,7
2017-12-09 07:20:31,123 -> 992
2017-12-09 07:20:31,132 -> 1024
2017-12-09 07:20:43,750 -> 03.Joining datasets,12.62,47664,20,7
Done!!! Sat Dec  9 07:20:44 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 07:20:52,107 -> Starting session,1.78,0
2017-12-09 07:20:52,107 -> Setting variables,0.00,0
2017-12-09 07:20:56,609 -> Reading datasets,4.50,0
2017-12-09 07:20:56,620 -> Points partitions: 2
2017-12-09 07:20:56,628 -> Centers partitions: 2
2017-12-09 07:21:03,900 -> 01.Indexing points,7.23,19715,20,14
2017-12-09 07:21:07,984 -> 02.Indexing centers,4.08,47664,20,14
2017-12-09 07:21:07,993 -> 992
2017-12-09 07:21:08,000 -> 1024
2017-12-09 07:21:18,144 -> 03.Joining datasets,10.14,47664,20,14
Done!!! Sat Dec  9 07:21:18 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 07:21:20,947 -> Starting session,1.57,0
2017-12-09 07:21:20,948 -> Setting variables,0.00,0
2017-12-09 07:21:25,637 -> Reading datasets,4.69,0
2017-12-09 07:21:25,648 -> Points partitions: 2
2017-12-09 07:21:25,656 -> Centers partitions: 2
2017-12-09 07:21:32,439 -> 01.Indexing points,6.74,19715,20,14
2017-12-09 07:21:36,376 -> 02.Indexing centers,3.94,47664,20,14
2017-12-09 07:21:36,385 -> 992
2017-12-09 07:21:36,391 -> 1024
2017-12-09 07:21:47,310 -> 03.Joining datasets,10.92,47664,20,14
Done!!! Sat Dec  9 07:21:47 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 07:21:50,192 -> Starting session,1.67,0
2017-12-09 07:21:50,193 -> Setting variables,0.00,0
2017-12-09 07:21:54,683 -> Reading datasets,4.49,0
2017-12-09 07:21:54,694 -> Points partitions: 2
2017-12-09 07:21:54,701 -> Centers partitions: 2
2017-12-09 07:22:01,357 -> 01.Indexing points,6.62,19715,20,14
2017-12-09 07:22:05,526 -> 02.Indexing centers,4.17,47664,20,14
2017-12-09 07:22:05,538 -> 992
2017-12-09 07:22:05,547 -> 1024
2017-12-09 07:22:15,813 -> 03.Joining datasets,10.27,47664,20,14
Done!!! Sat Dec  9 07:22:16 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 07:22:18,434 -> Starting session,1.52,0
2017-12-09 07:22:18,435 -> Setting variables,0.00,0
2017-12-09 07:22:22,969 -> Reading datasets,4.53,0
2017-12-09 07:22:22,981 -> Points partitions: 2
2017-12-09 07:22:22,989 -> Centers partitions: 2
2017-12-09 07:22:29,753 -> 01.Indexing points,6.72,19715,20,14
2017-12-09 07:22:33,567 -> 02.Indexing centers,3.81,47664,20,14
2017-12-09 07:22:33,575 -> 992
2017-12-09 07:22:33,581 -> 1024
2017-12-09 07:22:44,220 -> 03.Joining datasets,10.64,47664,20,14
Done!!! Sat Dec  9 07:22:44 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 07:22:46,870 -> Starting session,1.48,0
2017-12-09 07:22:46,870 -> Setting variables,0.00,0
2017-12-09 07:22:51,678 -> Reading datasets,4.81,0
2017-12-09 07:22:51,689 -> Points partitions: 2
2017-12-09 07:22:51,696 -> Centers partitions: 2
2017-12-09 07:22:58,533 -> 01.Indexing points,6.80,19715,20,14
2017-12-09 07:23:02,567 -> 02.Indexing centers,4.03,47664,20,14
2017-12-09 07:23:02,578 -> 992
2017-12-09 07:23:02,585 -> 1024
2017-12-09 07:23:12,961 -> 03.Joining datasets,10.38,47664,20,14
Done!!! Sat Dec  9 07:23:13 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 07:23:21,128 -> Starting session,1.69,0
2017-12-09 07:23:21,129 -> Setting variables,0.00,0
2017-12-09 07:23:26,940 -> Reading datasets,5.81,0
2017-12-09 07:23:26,951 -> Points partitions: 2
2017-12-09 07:23:26,959 -> Centers partitions: 2
2017-12-09 07:23:31,893 -> 01.Indexing points,4.90,19715,20,21
2017-12-09 07:23:36,081 -> 02.Indexing centers,4.19,47664,20,21
2017-12-09 07:23:36,088 -> 992
2017-12-09 07:23:36,094 -> 1024
2017-12-09 07:23:46,765 -> 03.Joining datasets,10.67,47664,20,21
Done!!! Sat Dec  9 07:23:47 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 07:23:49,605 -> Starting session,1.61,0
2017-12-09 07:23:49,606 -> Setting variables,0.00,0
2017-12-09 07:23:54,070 -> Reading datasets,4.46,0
2017-12-09 07:23:54,081 -> Points partitions: 2
2017-12-09 07:23:54,089 -> Centers partitions: 2
2017-12-09 07:24:02,486 -> 01.Indexing points,8.36,19715,20,21
2017-12-09 07:24:06,086 -> 02.Indexing centers,3.60,47664,20,21
2017-12-09 07:24:06,096 -> 992
2017-12-09 07:24:06,105 -> 1024
2017-12-09 07:24:15,163 -> 03.Joining datasets,9.06,47664,20,21
Done!!! Sat Dec  9 07:24:16 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 07:24:18,810 -> Starting session,1.62,0
2017-12-09 07:24:18,811 -> Setting variables,0.00,0
2017-12-09 07:24:24,395 -> Reading datasets,5.58,0
2017-12-09 07:24:24,406 -> Points partitions: 2
2017-12-09 07:24:24,414 -> Centers partitions: 2
2017-12-09 07:24:31,309 -> 01.Indexing points,6.85,19715,20,21
2017-12-09 07:24:34,827 -> 02.Indexing centers,3.52,47664,20,21
2017-12-09 07:24:34,839 -> 992
2017-12-09 07:24:34,848 -> 1024
2017-12-09 07:24:44,496 -> 03.Joining datasets,9.65,47664,20,21
Done!!! Sat Dec  9 07:24:45 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 07:24:47,316 -> Starting session,1.61,0
2017-12-09 07:24:47,317 -> Setting variables,0.00,0
2017-12-09 07:24:53,044 -> Reading datasets,5.73,0
2017-12-09 07:24:53,059 -> Points partitions: 2
2017-12-09 07:24:53,068 -> Centers partitions: 2
2017-12-09 07:25:00,198 -> 01.Indexing points,7.09,19715,20,21
2017-12-09 07:25:04,085 -> 02.Indexing centers,3.89,47664,20,21
2017-12-09 07:25:04,093 -> 992
2017-12-09 07:25:04,099 -> 1024
2017-12-09 07:25:13,307 -> 03.Joining datasets,9.21,47664,20,21
Done!!! Sat Dec  9 07:25:13 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 07:25:16,021 -> Starting session,1.56,0
2017-12-09 07:25:16,022 -> Setting variables,0.00,0
2017-12-09 07:25:20,776 -> Reading datasets,4.75,0
2017-12-09 07:25:20,788 -> Points partitions: 2
2017-12-09 07:25:20,796 -> Centers partitions: 2
2017-12-09 07:25:28,629 -> 01.Indexing points,7.79,19715,20,21
2017-12-09 07:25:32,505 -> 02.Indexing centers,3.87,47664,20,21
2017-12-09 07:25:32,515 -> 992
2017-12-09 07:25:32,522 -> 1024
2017-12-09 07:25:43,962 -> 03.Joining datasets,11.44,47664,20,21
Done!!! Sat Dec  9 07:25:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 07:25:52,454 -> Starting session,1.71,0
2017-12-09 07:25:52,456 -> Setting variables,0.00,0
2017-12-09 07:25:59,402 -> Reading datasets,6.95,0
2017-12-09 07:25:59,421 -> Points partitions: 2
2017-12-09 07:25:59,434 -> Centers partitions: 2
2017-12-09 07:26:06,853 -> 01.Indexing points,7.27,19715,20,28
2017-12-09 07:26:10,904 -> 02.Indexing centers,4.05,47664,20,28
2017-12-09 07:26:10,913 -> 992
2017-12-09 07:26:10,920 -> 1024
2017-12-09 07:26:19,158 -> 03.Joining datasets,8.24,47664,20,28
Done!!! Sat Dec  9 07:26:19 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 07:26:22,066 -> Starting session,1.62,0
2017-12-09 07:26:22,067 -> Setting variables,0.00,0
2017-12-09 07:26:28,977 -> Reading datasets,6.91,0
2017-12-09 07:26:28,997 -> Points partitions: 2
2017-12-09 07:26:29,010 -> Centers partitions: 2
2017-12-09 07:26:35,980 -> 01.Indexing points,6.92,19715,20,28
2017-12-09 07:26:40,034 -> 02.Indexing centers,4.05,47664,20,28
2017-12-09 07:26:40,048 -> 992
2017-12-09 07:26:40,058 -> 1024
2017-12-09 07:26:48,546 -> 03.Joining datasets,8.49,47664,20,28
Done!!! Sat Dec  9 07:26:49 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 07:26:51,348 -> Starting session,1.60,0
2017-12-09 07:26:51,348 -> Setting variables,0.00,0
2017-12-09 07:26:58,193 -> Reading datasets,6.84,0
2017-12-09 07:26:58,206 -> Points partitions: 2
2017-12-09 07:26:58,216 -> Centers partitions: 2
2017-12-09 07:27:04,545 -> 01.Indexing points,6.29,19715,20,28
2017-12-09 07:27:08,542 -> 02.Indexing centers,4.00,47664,20,28
2017-12-09 07:27:08,549 -> 992
2017-12-09 07:27:08,555 -> 1024
2017-12-09 07:27:18,487 -> 03.Joining datasets,9.98,47664,20,28
Done!!! Sat Dec  9 07:27:19 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 07:27:21,215 -> Starting session,1.49,0
2017-12-09 07:27:21,216 -> Setting variables,0.00,0
2017-12-09 07:27:27,911 -> Reading datasets,6.70,0
2017-12-09 07:27:27,931 -> Points partitions: 2
2017-12-09 07:27:27,944 -> Centers partitions: 2
2017-12-09 07:27:34,051 -> 01.Indexing points,6.05,19715,20,28
2017-12-09 07:27:38,032 -> 02.Indexing centers,3.98,47664,20,28
2017-12-09 07:27:38,041 -> 992
2017-12-09 07:27:38,048 -> 1024
2017-12-09 07:27:47,818 -> 03.Joining datasets,9.77,47664,20,28
Done!!! Sat Dec  9 07:27:49 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 07:27:51,404 -> Starting session,1.60,0
2017-12-09 07:27:51,404 -> Setting variables,0.00,0
2017-12-09 07:27:58,235 -> Reading datasets,6.83,0
2017-12-09 07:27:58,246 -> Points partitions: 2
2017-12-09 07:27:58,254 -> Centers partitions: 2
2017-12-09 07:28:06,354 -> 01.Indexing points,8.06,19715,20,28
2017-12-09 07:28:10,413 -> 02.Indexing centers,4.06,47664,20,28
2017-12-09 07:28:10,426 -> 992
2017-12-09 07:28:10,433 -> 1024
2017-12-09 07:28:21,485 -> 03.Joining datasets,11.05,47664,20,28
Done!!! Sat Dec  9 07:28:21 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 07:30:35,929 -> Starting session,1.58,0
2017-12-09 07:30:35,929 -> Setting variables,0.00,0
2017-12-09 07:30:40,335 -> Reading datasets,4.41,0
2017-12-09 07:30:40,347 -> Points partitions: 2
2017-12-09 07:30:40,355 -> Centers partitions: 2
2017-12-09 07:30:47,167 -> 01.Indexing points,6.77,19715,30,7
2017-12-09 07:30:52,185 -> 02.Indexing centers,5.02,72374,30,7
2017-12-09 07:30:52,195 -> 992
2017-12-09 07:30:52,203 -> 1024
2017-12-09 07:31:06,676 -> 03.Joining datasets,14.47,72374,30,7
Done!!! Sat Dec  9 07:31:07 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 07:31:09,281 -> Starting session,1.51,0
2017-12-09 07:31:09,282 -> Setting variables,0.00,0
2017-12-09 07:31:13,459 -> Reading datasets,4.18,0
2017-12-09 07:31:13,473 -> Points partitions: 2
2017-12-09 07:31:13,482 -> Centers partitions: 2
2017-12-09 07:31:20,092 -> 01.Indexing points,6.57,19715,30,7
2017-12-09 07:31:24,953 -> 02.Indexing centers,4.86,72374,30,7
2017-12-09 07:31:24,964 -> 992
2017-12-09 07:31:24,972 -> 1024
2017-12-09 07:31:38,697 -> 03.Joining datasets,13.72,72374,30,7
Done!!! Sat Dec  9 07:31:39 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 07:31:41,400 -> Starting session,1.54,0
2017-12-09 07:31:41,400 -> Setting variables,0.00,0
2017-12-09 07:31:45,497 -> Reading datasets,4.10,0
2017-12-09 07:31:45,514 -> Points partitions: 2
2017-12-09 07:31:45,526 -> Centers partitions: 2
2017-12-09 07:31:52,530 -> 01.Indexing points,6.96,19715,30,7
2017-12-09 07:31:57,147 -> 02.Indexing centers,4.62,72374,30,7
2017-12-09 07:31:57,160 -> 992
2017-12-09 07:31:57,168 -> 1024
2017-12-09 07:32:11,001 -> 03.Joining datasets,13.83,72374,30,7
Done!!! Sat Dec  9 07:32:11 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 07:32:13,538 -> Starting session,1.43,0
2017-12-09 07:32:13,539 -> Setting variables,0.00,0
2017-12-09 07:32:17,830 -> Reading datasets,4.29,0
2017-12-09 07:32:17,842 -> Points partitions: 2
2017-12-09 07:32:17,850 -> Centers partitions: 2
2017-12-09 07:32:24,642 -> 01.Indexing points,6.75,19715,30,7
2017-12-09 07:32:29,478 -> 02.Indexing centers,4.83,72374,30,7
2017-12-09 07:32:29,489 -> 992
2017-12-09 07:32:29,495 -> 1024
2017-12-09 07:32:43,088 -> 03.Joining datasets,13.67,72374,30,7
Done!!! Sat Dec  9 07:32:43 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 07:32:45,668 -> Starting session,1.41,0
2017-12-09 07:32:45,669 -> Setting variables,0.00,0
2017-12-09 07:32:49,927 -> Reading datasets,4.26,0
2017-12-09 07:32:49,943 -> Points partitions: 2
2017-12-09 07:32:49,953 -> Centers partitions: 2
2017-12-09 07:32:56,317 -> 01.Indexing points,6.33,19715,30,7
2017-12-09 07:33:00,918 -> 02.Indexing centers,4.60,72374,30,7
2017-12-09 07:33:00,931 -> 992
2017-12-09 07:33:00,939 -> 1024
2017-12-09 07:33:14,837 -> 03.Joining datasets,13.90,72374,30,7
Done!!! Sat Dec  9 07:33:15 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 07:33:23,197 -> Starting session,1.60,0
2017-12-09 07:33:23,197 -> Setting variables,0.00,0
2017-12-09 07:33:27,982 -> Reading datasets,4.78,0
2017-12-09 07:33:27,992 -> Points partitions: 2
2017-12-09 07:33:28,000 -> Centers partitions: 2
2017-12-09 07:33:34,923 -> 01.Indexing points,6.89,19715,30,14
2017-12-09 07:33:39,347 -> 02.Indexing centers,4.42,72374,30,14
2017-12-09 07:33:39,355 -> 992
2017-12-09 07:33:39,361 -> 1024
2017-12-09 07:33:50,411 -> 03.Joining datasets,11.05,72374,30,14
Done!!! Sat Dec  9 07:33:50 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 07:33:53,229 -> Starting session,1.58,0
2017-12-09 07:33:53,229 -> Setting variables,0.00,0
2017-12-09 07:33:57,950 -> Reading datasets,4.72,0
2017-12-09 07:33:57,961 -> Points partitions: 2
2017-12-09 07:33:57,969 -> Centers partitions: 2
2017-12-09 07:34:05,207 -> 01.Indexing points,7.20,19715,30,14
2017-12-09 07:34:09,699 -> 02.Indexing centers,4.49,72374,30,14
2017-12-09 07:34:09,707 -> 992
2017-12-09 07:34:09,713 -> 1024
2017-12-09 07:34:21,102 -> 03.Joining datasets,11.39,72374,30,14
Done!!! Sat Dec  9 07:34:21 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 07:34:24,010 -> Starting session,1.74,0
2017-12-09 07:34:24,011 -> Setting variables,0.00,0
2017-12-09 07:34:28,723 -> Reading datasets,4.71,0
2017-12-09 07:34:28,737 -> Points partitions: 2
2017-12-09 07:34:28,748 -> Centers partitions: 2
2017-12-09 07:34:35,830 -> 01.Indexing points,7.04,19715,30,14
2017-12-09 07:34:40,222 -> 02.Indexing centers,4.39,72374,30,14
2017-12-09 07:34:40,231 -> 992
2017-12-09 07:34:40,239 -> 1024
2017-12-09 07:34:51,233 -> 03.Joining datasets,10.99,72374,30,14
Done!!! Sat Dec  9 07:34:51 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 07:34:54,002 -> Starting session,1.56,0
2017-12-09 07:34:54,003 -> Setting variables,0.00,0
2017-12-09 07:34:58,502 -> Reading datasets,4.50,0
2017-12-09 07:34:58,512 -> Points partitions: 2
2017-12-09 07:34:58,520 -> Centers partitions: 2
2017-12-09 07:35:05,247 -> 01.Indexing points,6.69,19715,30,14
2017-12-09 07:35:09,838 -> 02.Indexing centers,4.59,72374,30,14
2017-12-09 07:35:09,845 -> 992
2017-12-09 07:35:09,851 -> 1024
2017-12-09 07:35:20,858 -> 03.Joining datasets,11.01,72374,30,14
Done!!! Sat Dec  9 07:35:21 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 07:35:23,648 -> Starting session,1.65,0
2017-12-09 07:35:23,649 -> Setting variables,0.00,0
2017-12-09 07:35:28,240 -> Reading datasets,4.59,0
2017-12-09 07:35:28,254 -> Points partitions: 2
2017-12-09 07:35:28,265 -> Centers partitions: 2
2017-12-09 07:35:34,947 -> 01.Indexing points,6.64,19715,30,14
2017-12-09 07:35:39,780 -> 02.Indexing centers,4.83,72374,30,14
2017-12-09 07:35:39,791 -> 992
2017-12-09 07:35:39,800 -> 1024
2017-12-09 07:35:50,366 -> 03.Joining datasets,10.57,72374,30,14
Done!!! Sat Dec  9 07:35:50 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 07:35:58,723 -> Starting session,1.65,0
2017-12-09 07:35:58,724 -> Setting variables,0.00,0
2017-12-09 07:36:04,286 -> Reading datasets,5.56,0
2017-12-09 07:36:04,304 -> Points partitions: 2
2017-12-09 07:36:04,316 -> Centers partitions: 2
2017-12-09 07:36:09,165 -> 01.Indexing points,4.80,19715,30,21
2017-12-09 07:36:13,348 -> 02.Indexing centers,4.18,72374,30,21
2017-12-09 07:36:13,356 -> 992
2017-12-09 07:36:13,362 -> 1024
2017-12-09 07:36:25,748 -> 03.Joining datasets,12.39,72374,30,21
Done!!! Sat Dec  9 07:36:26 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 07:36:28,603 -> Starting session,1.61,0
2017-12-09 07:36:28,604 -> Setting variables,0.00,0
2017-12-09 07:36:33,434 -> Reading datasets,4.83,0
2017-12-09 07:36:33,448 -> Points partitions: 2
2017-12-09 07:36:33,459 -> Centers partitions: 2
2017-12-09 07:36:41,156 -> 01.Indexing points,7.66,19715,30,21
2017-12-09 07:36:45,218 -> 02.Indexing centers,4.06,72374,30,21
2017-12-09 07:36:45,232 -> 992
2017-12-09 07:36:45,243 -> 1024
2017-12-09 07:36:54,818 -> 03.Joining datasets,9.57,72374,30,21
Done!!! Sat Dec  9 07:36:55 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 07:36:57,724 -> Starting session,1.65,0
2017-12-09 07:36:57,725 -> Setting variables,0.00,0
2017-12-09 07:37:03,488 -> Reading datasets,5.76,0
2017-12-09 07:37:03,508 -> Points partitions: 2
2017-12-09 07:37:03,521 -> Centers partitions: 2
2017-12-09 07:37:08,641 -> 01.Indexing points,5.06,19715,30,21
2017-12-09 07:37:13,754 -> 02.Indexing centers,5.11,72374,30,21
2017-12-09 07:37:13,761 -> 992
2017-12-09 07:37:13,767 -> 1024
2017-12-09 07:37:25,027 -> 03.Joining datasets,11.26,72374,30,21
Done!!! Sat Dec  9 07:37:25 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 07:37:27,836 -> Starting session,1.63,0
2017-12-09 07:37:27,836 -> Setting variables,0.00,0
2017-12-09 07:37:33,693 -> Reading datasets,5.86,0
2017-12-09 07:37:33,704 -> Points partitions: 2
2017-12-09 07:37:33,715 -> Centers partitions: 2
2017-12-09 07:37:38,990 -> 01.Indexing points,5.23,19715,30,21
2017-12-09 07:37:43,817 -> 02.Indexing centers,4.83,72374,30,21
2017-12-09 07:37:43,825 -> 992
2017-12-09 07:37:43,834 -> 1024
2017-12-09 07:37:53,389 -> 03.Joining datasets,9.56,72374,30,21
Done!!! Sat Dec  9 07:37:53 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 07:37:56,111 -> Starting session,1.50,0
2017-12-09 07:37:56,111 -> Setting variables,0.00,0
2017-12-09 07:38:01,827 -> Reading datasets,5.71,0
2017-12-09 07:38:01,849 -> Points partitions: 2
2017-12-09 07:38:01,864 -> Centers partitions: 2
2017-12-09 07:38:06,949 -> 01.Indexing points,4.92,19715,30,21
2017-12-09 07:38:11,419 -> 02.Indexing centers,4.47,72374,30,21
2017-12-09 07:38:11,428 -> 992
2017-12-09 07:38:11,436 -> 1024
2017-12-09 07:38:20,709 -> 03.Joining datasets,9.27,72374,30,21
Done!!! Sat Dec  9 07:38:21 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 07:38:29,335 -> Starting session,1.65,0
2017-12-09 07:38:29,336 -> Setting variables,0.00,0
2017-12-09 07:38:36,096 -> Reading datasets,6.76,0
2017-12-09 07:38:36,108 -> Points partitions: 2
2017-12-09 07:38:36,116 -> Centers partitions: 2
2017-12-09 07:38:44,706 -> 01.Indexing points,8.55,19715,30,28
2017-12-09 07:38:49,564 -> 02.Indexing centers,4.46,72374,30,28
2017-12-09 07:38:49,575 -> 992
2017-12-09 07:38:49,585 -> 1024
2017-12-09 07:39:00,099 -> 03.Joining datasets,10.51,72374,30,28
Done!!! Sat Dec  9 07:39:00 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 07:39:02,938 -> Starting session,1.64,0
2017-12-09 07:39:02,938 -> Setting variables,0.00,0
2017-12-09 07:39:10,064 -> Reading datasets,7.13,0
2017-12-09 07:39:10,081 -> Points partitions: 2
2017-12-09 07:39:10,091 -> Centers partitions: 2
2017-12-09 07:39:18,534 -> 01.Indexing points,8.40,19715,30,28
2017-12-09 07:39:22,885 -> 02.Indexing centers,4.35,72374,30,28
2017-12-09 07:39:22,894 -> 992
2017-12-09 07:39:22,900 -> 1024
2017-12-09 07:39:32,365 -> 03.Joining datasets,9.46,72374,30,28
Done!!! Sat Dec  9 07:39:33 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 07:39:35,872 -> Starting session,1.47,0
2017-12-09 07:39:35,873 -> Setting variables,0.00,0
2017-12-09 07:39:41,782 -> Reading datasets,5.91,0
2017-12-09 07:39:41,798 -> Points partitions: 2
2017-12-09 07:39:41,808 -> Centers partitions: 2
2017-12-09 07:39:48,656 -> 01.Indexing points,6.80,19715,30,28
2017-12-09 07:39:52,852 -> 02.Indexing centers,4.19,72374,30,28
2017-12-09 07:39:52,861 -> 992
2017-12-09 07:39:52,868 -> 1024
2017-12-09 07:40:02,990 -> 03.Joining datasets,10.12,72374,30,28
Done!!! Sat Dec  9 07:40:03 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 07:40:05,863 -> Starting session,1.66,0
2017-12-09 07:40:05,864 -> Setting variables,0.00,0
2017-12-09 07:40:12,603 -> Reading datasets,6.74,0
2017-12-09 07:40:12,626 -> Points partitions: 2
2017-12-09 07:40:12,641 -> Centers partitions: 2
2017-12-09 07:40:21,240 -> 01.Indexing points,8.55,19715,30,28
2017-12-09 07:40:25,520 -> 02.Indexing centers,4.28,72374,30,28
2017-12-09 07:40:25,530 -> 992
2017-12-09 07:40:25,537 -> 1024
2017-12-09 07:40:36,328 -> 03.Joining datasets,10.79,72374,30,28
Done!!! Sat Dec  9 07:40:37 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 07:40:39,939 -> Starting session,1.57,0
2017-12-09 07:40:39,940 -> Setting variables,0.00,0
2017-12-09 07:40:46,762 -> Reading datasets,6.82,0
2017-12-09 07:40:46,781 -> Points partitions: 2
2017-12-09 07:40:46,793 -> Centers partitions: 2
2017-12-09 07:40:53,850 -> 01.Indexing points,7.00,19715,30,28
2017-12-09 07:40:58,355 -> 02.Indexing centers,4.50,72374,30,28
2017-12-09 07:40:58,363 -> 992
2017-12-09 07:40:58,370 -> 1024
2017-12-09 07:41:08,604 -> 03.Joining datasets,10.23,72374,30,28
Done!!! Sat Dec  9 07:41:09 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 08:06:41,464 -> Starting session,1.59,0
2017-12-09 08:06:41,465 -> Setting variables,0.00,0
2017-12-09 08:06:45,646 -> Reading datasets,4.18,0
2017-12-09 08:06:45,664 -> Points partitions: 2
2017-12-09 08:06:45,678 -> Centers partitions: 2
2017-12-09 08:06:52,246 -> 01.Indexing points,6.52,19715,40,7
2017-12-09 08:06:57,416 -> 02.Indexing centers,5.17,98838,40,7
2017-12-09 08:06:57,428 -> 992
2017-12-09 08:06:57,441 -> 1024
2017-12-09 08:07:11,808 -> 03.Joining datasets,14.37,98838,40,7
Done!!! Sat Dec  9 08:07:12 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 08:07:14,385 -> Starting session,1.41,0
2017-12-09 08:07:14,385 -> Setting variables,0.00,0
2017-12-09 08:07:18,835 -> Reading datasets,4.45,0
2017-12-09 08:07:18,849 -> Points partitions: 2
2017-12-09 08:07:18,860 -> Centers partitions: 2
2017-12-09 08:07:25,756 -> 01.Indexing points,6.86,19715,40,7
2017-12-09 08:07:30,911 -> 02.Indexing centers,5.15,98838,40,7
2017-12-09 08:07:30,923 -> 992
2017-12-09 08:07:30,933 -> 1024
2017-12-09 08:07:46,262 -> 03.Joining datasets,15.33,98838,40,7
Done!!! Sat Dec  9 08:07:46 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 08:07:48,902 -> Starting session,1.54,0
2017-12-09 08:07:48,902 -> Setting variables,0.00,0
2017-12-09 08:07:53,212 -> Reading datasets,4.31,0
2017-12-09 08:07:53,225 -> Points partitions: 2
2017-12-09 08:07:53,234 -> Centers partitions: 2
2017-12-09 08:08:00,422 -> 01.Indexing points,7.15,19715,40,7
2017-12-09 08:08:05,344 -> 02.Indexing centers,4.92,98838,40,7
2017-12-09 08:08:05,357 -> 992
2017-12-09 08:08:05,366 -> 1024
2017-12-09 08:08:20,270 -> 03.Joining datasets,14.95,98838,40,7
Done!!! Sat Dec  9 08:08:20 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 08:08:22,987 -> Starting session,1.63,0
2017-12-09 08:08:22,987 -> Setting variables,0.00,0
2017-12-09 08:08:26,997 -> Reading datasets,4.01,0
2017-12-09 08:08:27,010 -> Points partitions: 2
2017-12-09 08:08:27,018 -> Centers partitions: 2
2017-12-09 08:08:33,951 -> 01.Indexing points,6.89,19715,40,7
2017-12-09 08:08:39,130 -> 02.Indexing centers,5.18,98838,40,7
2017-12-09 08:08:39,146 -> 992
2017-12-09 08:08:39,158 -> 1024
2017-12-09 08:08:54,355 -> 03.Joining datasets,15.20,98838,40,7
Done!!! Sat Dec  9 08:08:54 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 08:08:57,067 -> Starting session,1.54,0
2017-12-09 08:08:57,067 -> Setting variables,0.00,0
2017-12-09 08:09:01,290 -> Reading datasets,4.22,0
2017-12-09 08:09:01,302 -> Points partitions: 2
2017-12-09 08:09:01,309 -> Centers partitions: 2
2017-12-09 08:09:08,389 -> 01.Indexing points,7.04,19715,40,7
2017-12-09 08:09:13,354 -> 02.Indexing centers,4.96,98838,40,7
2017-12-09 08:09:13,367 -> 992
2017-12-09 08:09:13,376 -> 1024
2017-12-09 08:09:28,418 -> 03.Joining datasets,15.04,98838,40,7
Done!!! Sat Dec  9 08:09:28 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 08:09:36,755 -> Starting session,1.62,0
2017-12-09 08:09:36,756 -> Setting variables,0.00,0
2017-12-09 08:09:41,430 -> Reading datasets,4.67,0
2017-12-09 08:09:41,444 -> Points partitions: 2
2017-12-09 08:09:41,454 -> Centers partitions: 2
2017-12-09 08:09:48,445 -> 01.Indexing points,6.95,19715,40,14
2017-12-09 08:09:53,459 -> 02.Indexing centers,5.01,98838,40,14
2017-12-09 08:09:53,472 -> 992
2017-12-09 08:09:53,481 -> 1024
2017-12-09 08:10:05,750 -> 03.Joining datasets,12.27,98838,40,14
Done!!! Sat Dec  9 08:10:06 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 08:10:08,612 -> Starting session,1.65,0
2017-12-09 08:10:08,613 -> Setting variables,0.00,0
2017-12-09 08:10:13,375 -> Reading datasets,4.76,0
2017-12-09 08:10:13,390 -> Points partitions: 2
2017-12-09 08:10:13,401 -> Centers partitions: 2
2017-12-09 08:10:20,180 -> 01.Indexing points,6.74,19715,40,14
2017-12-09 08:10:24,792 -> 02.Indexing centers,4.61,98838,40,14
2017-12-09 08:10:24,803 -> 992
2017-12-09 08:10:24,812 -> 1024
2017-12-09 08:10:37,117 -> 03.Joining datasets,12.30,98838,40,14
Done!!! Sat Dec  9 08:10:37 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 08:10:40,051 -> Starting session,1.72,0
2017-12-09 08:10:40,051 -> Setting variables,0.00,0
2017-12-09 08:10:44,718 -> Reading datasets,4.67,0
2017-12-09 08:10:44,731 -> Points partitions: 2
2017-12-09 08:10:44,742 -> Centers partitions: 2
2017-12-09 08:10:51,383 -> 01.Indexing points,6.60,19715,40,14
2017-12-09 08:10:56,228 -> 02.Indexing centers,4.84,98838,40,14
2017-12-09 08:10:56,236 -> 992
2017-12-09 08:10:56,241 -> 1024
2017-12-09 08:11:08,589 -> 03.Joining datasets,12.35,98838,40,14
Done!!! Sat Dec  9 08:11:09 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 08:11:11,322 -> Starting session,1.52,0
2017-12-09 08:11:11,323 -> Setting variables,0.00,0
2017-12-09 08:11:15,818 -> Reading datasets,4.49,0
2017-12-09 08:11:15,829 -> Points partitions: 2
2017-12-09 08:11:15,836 -> Centers partitions: 2
2017-12-09 08:11:22,873 -> 01.Indexing points,7.00,19715,40,14
2017-12-09 08:11:27,899 -> 02.Indexing centers,5.03,98838,40,14
2017-12-09 08:11:27,907 -> 992
2017-12-09 08:11:27,914 -> 1024
2017-12-09 08:11:40,603 -> 03.Joining datasets,12.69,98838,40,14
Done!!! Sat Dec  9 08:11:41 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 08:11:43,729 -> Starting session,1.56,0
2017-12-09 08:11:43,730 -> Setting variables,0.00,0
2017-12-09 08:11:48,125 -> Reading datasets,4.52,0
2017-12-09 08:11:48,136 -> Points partitions: 2
2017-12-09 08:11:48,144 -> Centers partitions: 2
2017-12-09 08:11:54,823 -> 01.Indexing points,6.64,19715,40,14
2017-12-09 08:11:59,526 -> 02.Indexing centers,4.70,98838,40,14
2017-12-09 08:11:59,535 -> 992
2017-12-09 08:11:59,543 -> 1024
2017-12-09 08:12:11,973 -> 03.Joining datasets,12.43,98838,40,14
Done!!! Sat Dec  9 08:12:12 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 08:12:20,529 -> Starting session,1.71,0
2017-12-09 08:12:20,531 -> Setting variables,0.00,0
2017-12-09 08:12:25,373 -> Reading datasets,4.84,0
2017-12-09 08:12:25,389 -> Points partitions: 2
2017-12-09 08:12:25,400 -> Centers partitions: 2
2017-12-09 08:12:33,373 -> 01.Indexing points,7.93,19715,40,21
2017-12-09 08:12:38,078 -> 02.Indexing centers,4.70,98838,40,21
2017-12-09 08:12:38,088 -> 992
2017-12-09 08:12:38,096 -> 1024
2017-12-09 08:12:48,911 -> 03.Joining datasets,10.81,98838,40,21
Done!!! Sat Dec  9 08:12:49 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 08:12:51,672 -> Starting session,1.51,0
2017-12-09 08:12:51,672 -> Setting variables,0.00,0
2017-12-09 08:12:56,464 -> Reading datasets,4.79,0
2017-12-09 08:12:56,475 -> Points partitions: 2
2017-12-09 08:12:56,486 -> Centers partitions: 2
2017-12-09 08:13:03,475 -> 01.Indexing points,6.95,19715,40,21
2017-12-09 08:13:08,976 -> 02.Indexing centers,5.50,98838,40,21
2017-12-09 08:13:08,984 -> 992
2017-12-09 08:13:08,990 -> 1024
2017-12-09 08:13:20,065 -> 03.Joining datasets,11.07,98838,40,21
Done!!! Sat Dec  9 08:13:20 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 08:13:22,692 -> Starting session,1.44,0
2017-12-09 08:13:22,692 -> Setting variables,0.00,0
2017-12-09 08:13:28,362 -> Reading datasets,5.67,0
2017-12-09 08:13:28,385 -> Points partitions: 2
2017-12-09 08:13:28,400 -> Centers partitions: 2
2017-12-09 08:13:35,642 -> 01.Indexing points,7.19,19715,40,21
2017-12-09 08:13:39,980 -> 02.Indexing centers,4.34,98838,40,21
2017-12-09 08:13:39,988 -> 992
2017-12-09 08:13:39,994 -> 1024
2017-12-09 08:13:49,956 -> 03.Joining datasets,9.96,98838,40,21
Done!!! Sat Dec  9 08:13:50 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 08:13:52,656 -> Starting session,1.48,0
2017-12-09 08:13:52,656 -> Setting variables,0.00,0
2017-12-09 08:13:57,307 -> Reading datasets,4.65,0
2017-12-09 08:13:57,321 -> Points partitions: 2
2017-12-09 08:13:57,331 -> Centers partitions: 2
2017-12-09 08:14:03,375 -> 01.Indexing points,6.00,19715,40,21
2017-12-09 08:14:08,495 -> 02.Indexing centers,5.12,98838,40,21
2017-12-09 08:14:08,506 -> 992
2017-12-09 08:14:08,515 -> 1024
2017-12-09 08:14:18,473 -> 03.Joining datasets,9.96,98838,40,21
Done!!! Sat Dec  9 08:14:18 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 08:14:21,247 -> Starting session,1.56,0
2017-12-09 08:14:21,247 -> Setting variables,0.00,0
2017-12-09 08:14:25,894 -> Reading datasets,4.65,0
2017-12-09 08:14:25,908 -> Points partitions: 2
2017-12-09 08:14:25,915 -> Centers partitions: 2
2017-12-09 08:14:31,949 -> 01.Indexing points,6.00,19715,40,21
2017-12-09 08:14:36,348 -> 02.Indexing centers,4.40,98838,40,21
2017-12-09 08:14:36,357 -> 992
2017-12-09 08:14:36,364 -> 1024
2017-12-09 08:14:48,570 -> 03.Joining datasets,12.21,98838,40,21
Done!!! Sat Dec  9 08:14:49 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 08:14:56,867 -> Starting session,1.68,0
2017-12-09 08:14:56,867 -> Setting variables,0.00,0
2017-12-09 08:15:01,579 -> Reading datasets,4.71,0
2017-12-09 08:15:01,592 -> Points partitions: 2
2017-12-09 08:15:01,603 -> Centers partitions: 2
2017-12-09 08:15:10,733 -> 01.Indexing points,9.08,19715,40,28
2017-12-09 08:15:15,443 -> 02.Indexing centers,4.71,98838,40,28
2017-12-09 08:15:15,451 -> 992
2017-12-09 08:15:15,458 -> 1024
2017-12-09 08:15:24,853 -> 03.Joining datasets,9.40,98838,40,28
Done!!! Sat Dec  9 08:15:26 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 08:15:28,570 -> Starting session,1.68,0
2017-12-09 08:15:28,571 -> Setting variables,0.00,0
2017-12-09 08:15:33,362 -> Reading datasets,4.79,0
2017-12-09 08:15:33,376 -> Points partitions: 2
2017-12-09 08:15:33,386 -> Centers partitions: 2
2017-12-09 08:15:41,386 -> 01.Indexing points,7.95,19715,40,28
2017-12-09 08:15:47,968 -> 02.Indexing centers,6.58,98838,40,28
2017-12-09 08:15:47,979 -> 992
2017-12-09 08:15:47,988 -> 1024
2017-12-09 08:15:58,986 -> 03.Joining datasets,11.00,98838,40,28
Done!!! Sat Dec  9 08:15:59 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 08:16:01,781 -> Starting session,1.60,0
2017-12-09 08:16:01,782 -> Setting variables,0.00,0
2017-12-09 08:16:08,634 -> Reading datasets,6.85,0
2017-12-09 08:16:08,652 -> Points partitions: 2
2017-12-09 08:16:08,664 -> Centers partitions: 2
2017-12-09 08:16:17,294 -> 01.Indexing points,8.58,19715,40,28
2017-12-09 08:16:22,308 -> 02.Indexing centers,5.01,98838,40,28
2017-12-09 08:16:22,321 -> 992
2017-12-09 08:16:22,336 -> 1024
2017-12-09 08:16:33,318 -> 03.Joining datasets,10.98,98838,40,28
Done!!! Sat Dec  9 08:16:33 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 08:16:36,133 -> Starting session,1.60,0
2017-12-09 08:16:36,134 -> Setting variables,0.00,0
2017-12-09 08:16:42,972 -> Reading datasets,6.84,0
2017-12-09 08:16:42,982 -> Points partitions: 2
2017-12-09 08:16:42,989 -> Centers partitions: 2
2017-12-09 08:16:52,291 -> 01.Indexing points,9.26,19715,40,28
2017-12-09 08:16:57,057 -> 02.Indexing centers,4.76,98838,40,28
2017-12-09 08:16:57,065 -> 992
2017-12-09 08:16:57,074 -> 1024
2017-12-09 08:17:07,589 -> 03.Joining datasets,10.51,98838,40,28
Done!!! Sat Dec  9 08:17:08 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 08:17:10,484 -> Starting session,1.62,0
2017-12-09 08:17:10,484 -> Setting variables,0.00,0
2017-12-09 08:17:17,229 -> Reading datasets,6.74,0
2017-12-09 08:17:17,249 -> Points partitions: 2
2017-12-09 08:17:17,261 -> Centers partitions: 2
2017-12-09 08:17:22,889 -> 01.Indexing points,5.57,19715,40,28
2017-12-09 08:17:29,235 -> 02.Indexing centers,6.34,98838,40,28
2017-12-09 08:17:29,249 -> 992
2017-12-09 08:17:29,260 -> 1024
2017-12-09 08:17:41,432 -> 03.Joining datasets,12.17,98838,40,28
Done!!! Sat Dec  9 08:17:41 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
acald013@dblab-rack12: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack14: no org.apache.spark.deploy.worker.Worker to stop
acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 7 cores...
2017-12-09 08:55:01,479 -> Starting session,1.59,0
2017-12-09 08:55:01,479 -> Setting variables,0.00,0
2017-12-09 08:55:05,663 -> Reading datasets,4.18,0
2017-12-09 08:55:05,677 -> Points partitions: 2
2017-12-09 08:55:05,688 -> Centers partitions: 2
2017-12-09 08:55:12,790 -> 01.Indexing points,7.06,19715,50,7
2017-12-09 08:55:18,103 -> 02.Indexing centers,5.31,125542,50,7
2017-12-09 08:55:18,115 -> 992
2017-12-09 08:55:18,123 -> 1024
2017-12-09 08:55:33,224 -> 03.Joining datasets,15.10,125542,50,7
Done!!! Sat Dec  9 08:55:33 PST 2017
Running iteration 2/5 for 7 cores...
2017-12-09 08:55:35,964 -> Starting session,1.54,0
2017-12-09 08:55:35,964 -> Setting variables,0.00,0
2017-12-09 08:55:40,152 -> Reading datasets,4.19,0
2017-12-09 08:55:40,163 -> Points partitions: 2
2017-12-09 08:55:40,170 -> Centers partitions: 2
2017-12-09 08:55:46,959 -> 01.Indexing points,6.75,19715,50,7
2017-12-09 08:55:52,299 -> 02.Indexing centers,5.34,125542,50,7
2017-12-09 08:55:52,308 -> 992
2017-12-09 08:55:52,314 -> 1024
2017-12-09 08:56:07,631 -> 03.Joining datasets,15.32,125542,50,7
Done!!! Sat Dec  9 08:56:08 PST 2017
Running iteration 3/5 for 7 cores...
2017-12-09 08:56:10,441 -> Starting session,1.57,0
2017-12-09 08:56:10,441 -> Setting variables,0.00,0
2017-12-09 08:56:14,645 -> Reading datasets,4.20,0
2017-12-09 08:56:14,660 -> Points partitions: 2
2017-12-09 08:56:14,671 -> Centers partitions: 2
2017-12-09 08:56:21,359 -> 01.Indexing points,6.65,19715,50,7
2017-12-09 08:56:26,735 -> 02.Indexing centers,5.37,125542,50,7
2017-12-09 08:56:26,748 -> 992
2017-12-09 08:56:26,756 -> 1024
2017-12-09 08:56:42,439 -> 03.Joining datasets,15.68,125542,50,7
Done!!! Sat Dec  9 08:56:42 PST 2017
Running iteration 4/5 for 7 cores...
2017-12-09 08:56:45,128 -> Starting session,1.48,0
2017-12-09 08:56:45,129 -> Setting variables,0.00,0
2017-12-09 08:56:49,312 -> Reading datasets,4.18,0
2017-12-09 08:56:49,322 -> Points partitions: 2
2017-12-09 08:56:49,330 -> Centers partitions: 2
2017-12-09 08:56:56,050 -> 01.Indexing points,6.68,19715,50,7
2017-12-09 08:57:01,456 -> 02.Indexing centers,5.40,125542,50,7
2017-12-09 08:57:01,465 -> 992
2017-12-09 08:57:01,472 -> 1024
2017-12-09 08:57:17,363 -> 03.Joining datasets,15.89,125542,50,7
Done!!! Sat Dec  9 08:57:17 PST 2017
Running iteration 5/5 for 7 cores...
2017-12-09 08:57:20,104 -> Starting session,1.54,0
2017-12-09 08:57:20,105 -> Setting variables,0.00,0
2017-12-09 08:57:24,282 -> Reading datasets,4.18,0
2017-12-09 08:57:24,295 -> Points partitions: 2
2017-12-09 08:57:24,303 -> Centers partitions: 2
2017-12-09 08:57:31,194 -> 01.Indexing points,6.85,19715,50,7
2017-12-09 08:57:36,705 -> 02.Indexing centers,5.51,125542,50,7
2017-12-09 08:57:36,718 -> 992
2017-12-09 08:57:36,727 -> 1024
2017-12-09 08:57:52,876 -> 03.Joining datasets,16.15,125542,50,7
Done!!! Sat Dec  9 08:57:53 PST 2017
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 14 cores...
2017-12-09 08:58:01,350 -> Starting session,1.71,0
2017-12-09 08:58:01,350 -> Setting variables,0.00,0
2017-12-09 08:58:06,169 -> Reading datasets,4.82,0
2017-12-09 08:58:06,184 -> Points partitions: 2
2017-12-09 08:58:06,193 -> Centers partitions: 2
2017-12-09 08:58:13,115 -> 01.Indexing points,6.88,19715,50,14
2017-12-09 08:58:18,333 -> 02.Indexing centers,5.22,125542,50,14
2017-12-09 08:58:18,341 -> 992
2017-12-09 08:58:18,347 -> 1024
2017-12-09 08:58:31,182 -> 03.Joining datasets,12.83,125542,50,14
Done!!! Sat Dec  9 08:58:31 PST 2017
Running iteration 2/5 for 14 cores...
2017-12-09 08:58:34,089 -> Starting session,1.72,0
2017-12-09 08:58:34,090 -> Setting variables,0.00,0
2017-12-09 08:58:38,661 -> Reading datasets,4.57,0
2017-12-09 08:58:38,673 -> Points partitions: 2
2017-12-09 08:58:38,680 -> Centers partitions: 2
2017-12-09 08:58:45,749 -> 01.Indexing points,7.03,19715,50,14
2017-12-09 08:58:50,494 -> 02.Indexing centers,4.74,125542,50,14
2017-12-09 08:58:50,506 -> 992
2017-12-09 08:58:50,515 -> 1024
2017-12-09 08:59:03,829 -> 03.Joining datasets,13.31,125542,50,14
Done!!! Sat Dec  9 08:59:05 PST 2017
Running iteration 3/5 for 14 cores...
2017-12-09 08:59:07,372 -> Starting session,1.55,0
2017-12-09 08:59:07,373 -> Setting variables,0.00,0
2017-12-09 08:59:11,976 -> Reading datasets,4.60,0
2017-12-09 08:59:11,987 -> Points partitions: 2
2017-12-09 08:59:11,994 -> Centers partitions: 2
2017-12-09 08:59:18,745 -> 01.Indexing points,6.71,19715,50,14
2017-12-09 08:59:23,687 -> 02.Indexing centers,4.94,125542,50,14
2017-12-09 08:59:23,699 -> 992
2017-12-09 08:59:23,705 -> 1024
2017-12-09 08:59:36,616 -> 03.Joining datasets,12.91,125542,50,14
Done!!! Sat Dec  9 08:59:37 PST 2017
Running iteration 4/5 for 14 cores...
2017-12-09 08:59:39,395 -> Starting session,1.61,0
2017-12-09 08:59:39,396 -> Setting variables,0.00,0
2017-12-09 08:59:44,005 -> Reading datasets,4.61,0
2017-12-09 08:59:44,015 -> Points partitions: 2
2017-12-09 08:59:44,023 -> Centers partitions: 2
2017-12-09 08:59:50,980 -> 01.Indexing points,6.92,19715,50,14
2017-12-09 08:59:56,352 -> 02.Indexing centers,5.37,125542,50,14
2017-12-09 08:59:56,363 -> 992
2017-12-09 08:59:56,372 -> 1024
2017-12-09 09:00:09,948 -> 03.Joining datasets,13.58,125542,50,14
Done!!! Sat Dec  9 09:00:11 PST 2017
Running iteration 5/5 for 14 cores...
2017-12-09 09:00:13,724 -> Starting session,1.62,0
2017-12-09 09:00:13,724 -> Setting variables,0.00,0
2017-12-09 09:00:18,317 -> Reading datasets,4.59,0
2017-12-09 09:00:18,329 -> Points partitions: 2
2017-12-09 09:00:18,336 -> Centers partitions: 2
2017-12-09 09:00:25,409 -> 01.Indexing points,7.03,19715,50,14
2017-12-09 09:00:30,540 -> 02.Indexing centers,5.13,125542,50,14
2017-12-09 09:00:30,552 -> 992
2017-12-09 09:00:30,562 -> 1024
2017-12-09 09:00:43,612 -> 03.Joining datasets,13.05,125542,50,14
Done!!! Sat Dec  9 09:00:44 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running iteration 1/5 for 21 cores...
2017-12-09 09:00:52,186 -> Starting session,1.73,0
2017-12-09 09:00:52,188 -> Setting variables,0.00,0
2017-12-09 09:00:57,907 -> Reading datasets,5.72,0
2017-12-09 09:00:57,922 -> Points partitions: 2
2017-12-09 09:00:57,931 -> Centers partitions: 2
2017-12-09 09:01:03,102 -> 01.Indexing points,5.13,19715,50,21
2017-12-09 09:01:08,407 -> 02.Indexing centers,5.30,125542,50,21
2017-12-09 09:01:08,415 -> 992
2017-12-09 09:01:08,423 -> 1024
2017-12-09 09:01:21,219 -> 03.Joining datasets,12.80,125542,50,21
Done!!! Sat Dec  9 09:01:21 PST 2017
Running iteration 2/5 for 21 cores...
2017-12-09 09:01:24,298 -> Starting session,1.89,0
2017-12-09 09:01:24,299 -> Setting variables,0.00,0
2017-12-09 09:01:29,832 -> Reading datasets,5.53,0
2017-12-09 09:01:29,851 -> Points partitions: 2
2017-12-09 09:01:29,863 -> Centers partitions: 2
2017-12-09 09:01:36,889 -> 01.Indexing points,6.97,19715,50,21
2017-12-09 09:01:41,926 -> 02.Indexing centers,5.04,125542,50,21
2017-12-09 09:01:41,935 -> 992
2017-12-09 09:01:41,942 -> 1024
2017-12-09 09:01:55,228 -> 03.Joining datasets,12.37,125542,50,21
Done!!! Sat Dec  9 09:01:55 PST 2017
Running iteration 3/5 for 21 cores...
2017-12-09 09:01:58,010 -> Starting session,1.53,0
2017-12-09 09:01:58,011 -> Setting variables,0.00,0
2017-12-09 09:02:03,739 -> Reading datasets,5.73,0
2017-12-09 09:02:03,762 -> Points partitions: 2
2017-12-09 09:02:03,775 -> Centers partitions: 2
2017-12-09 09:02:10,976 -> 01.Indexing points,7.14,19715,50,21
2017-12-09 09:02:16,097 -> 02.Indexing centers,5.12,125542,50,21
2017-12-09 09:02:16,109 -> 992
2017-12-09 09:02:16,121 -> 1024
2017-12-09 09:02:26,978 -> 03.Joining datasets,10.86,125542,50,21
Done!!! Sat Dec  9 09:02:27 PST 2017
Running iteration 4/5 for 21 cores...
2017-12-09 09:02:29,777 -> Starting session,1.59,0
2017-12-09 09:02:29,777 -> Setting variables,0.00,0
2017-12-09 09:02:34,730 -> Reading datasets,4.95,0
2017-12-09 09:02:34,741 -> Points partitions: 2
2017-12-09 09:02:34,749 -> Centers partitions: 2
2017-12-09 09:02:42,584 -> 01.Indexing points,7.80,19715,50,21
2017-12-09 09:02:47,240 -> 02.Indexing centers,4.65,125542,50,21
2017-12-09 09:02:47,251 -> 992
2017-12-09 09:02:47,260 -> 1024
2017-12-09 09:02:58,048 -> 03.Joining datasets,10.79,125542,50,21
Done!!! Sat Dec  9 09:02:58 PST 2017
Running iteration 5/5 for 21 cores...
2017-12-09 09:03:01,055 -> Starting session,1.74,0
2017-12-09 09:03:01,055 -> Setting variables,0.00,0
2017-12-09 09:03:06,674 -> Reading datasets,5.62,0
2017-12-09 09:03:06,688 -> Points partitions: 2
2017-12-09 09:03:06,699 -> Centers partitions: 2
2017-12-09 09:03:13,653 -> 01.Indexing points,6.92,19715,50,21
2017-12-09 09:03:18,413 -> 02.Indexing centers,4.76,125542,50,21
2017-12-09 09:03:18,425 -> 992
2017-12-09 09:03:18,434 -> 1024
2017-12-09 09:03:29,914 -> 03.Joining datasets,11.48,125542,50,21
Done!!! Sat Dec  9 09:03:30 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack11.cs.ucr.edu.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running iteration 1/5 for 28 cores...
2017-12-09 09:03:38,178 -> Starting session,1.59,0
2017-12-09 09:03:38,179 -> Setting variables,0.00,0
2017-12-09 09:03:45,082 -> Reading datasets,6.90,0
2017-12-09 09:03:45,101 -> Points partitions: 2
2017-12-09 09:03:45,114 -> Centers partitions: 2
2017-12-09 09:03:54,204 -> 01.Indexing points,9.03,19715,50,28
2017-12-09 09:03:59,451 -> 02.Indexing centers,5.24,125542,50,28
2017-12-09 09:03:59,465 -> 992
2017-12-09 09:03:59,473 -> 1024
2017-12-09 09:04:11,012 -> 03.Joining datasets,11.54,125542,50,28
Done!!! Sat Dec  9 09:04:11 PST 2017
Running iteration 2/5 for 28 cores...
2017-12-09 09:04:13,880 -> Starting session,1.65,0
2017-12-09 09:04:13,881 -> Setting variables,0.00,0
2017-12-09 09:04:20,924 -> Reading datasets,7.04,0
2017-12-09 09:04:20,949 -> Points partitions: 2
2017-12-09 09:04:20,968 -> Centers partitions: 2
2017-12-09 09:04:29,083 -> 01.Indexing points,8.05,19715,50,28
2017-12-09 09:04:34,543 -> 02.Indexing centers,5.46,125542,50,28
2017-12-09 09:04:34,553 -> 992
2017-12-09 09:04:34,562 -> 1024
2017-12-09 09:04:47,243 -> 03.Joining datasets,12.68,125542,50,28
Done!!! Sat Dec  9 09:04:48 PST 2017
Running iteration 3/5 for 28 cores...
2017-12-09 09:04:50,746 -> Starting session,1.51,0
2017-12-09 09:04:50,746 -> Setting variables,0.00,0
2017-12-09 09:04:57,813 -> Reading datasets,7.07,0
2017-12-09 09:04:57,835 -> Points partitions: 2
2017-12-09 09:04:57,852 -> Centers partitions: 2
2017-12-09 09:05:05,515 -> 01.Indexing points,7.61,19715,50,28
2017-12-09 09:05:11,178 -> 02.Indexing centers,5.66,125542,50,28
2017-12-09 09:05:11,188 -> 992
2017-12-09 09:05:11,197 -> 1024
2017-12-09 09:05:23,640 -> 03.Joining datasets,12.44,125542,50,28
Done!!! Sat Dec  9 09:05:24 PST 2017
Running iteration 4/5 for 28 cores...
2017-12-09 09:05:26,364 -> Starting session,1.51,0
2017-12-09 09:05:26,364 -> Setting variables,0.00,0
2017-12-09 09:05:31,919 -> Reading datasets,5.55,0
2017-12-09 09:05:31,938 -> Points partitions: 2
2017-12-09 09:05:31,950 -> Centers partitions: 2
2017-12-09 09:05:41,039 -> 01.Indexing points,9.04,19715,50,28
2017-12-09 09:05:47,498 -> 02.Indexing centers,6.46,125542,50,28
2017-12-09 09:05:47,509 -> 992
2017-12-09 09:05:47,516 -> 1024
2017-12-09 09:05:59,436 -> 03.Joining datasets,11.92,125542,50,28
Done!!! Sat Dec  9 09:05:59 PST 2017
Running iteration 5/5 for 28 cores...
2017-12-09 09:06:02,212 -> Starting session,1.65,0
2017-12-09 09:06:02,212 -> Setting variables,0.00,0
2017-12-09 09:06:09,159 -> Reading datasets,6.95,0
2017-12-09 09:06:09,292 -> Points partitions: 2
2017-12-09 09:06:09,304 -> Centers partitions: 2
2017-12-09 09:06:17,682 -> 01.Indexing points,8.33,19715,50,28
2017-12-09 09:06:23,786 -> 02.Indexing centers,6.10,125542,50,28
2017-12-09 09:06:23,798 -> 992
2017-12-09 09:06:23,805 -> 1024
2017-12-09 09:06:34,150 -> 03.Joining datasets,10.34,125542,50,28
Done!!! Sat Dec  9 09:06:35 PST 2017
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
