Running in 6 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 08:20:56 INFO SparkContext: Running Spark version 2.1.0
17/09/15 08:20:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 08:20:57 INFO SecurityManager: Changing view acls to: acald013
17/09/15 08:20:57 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 08:20:57 INFO SecurityManager: Changing view acls groups to: 
17/09/15 08:20:57 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 08:20:57 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 08:20:58 INFO Utils: Successfully started service 'sparkDriver' on port 42145.
17/09/15 08:20:58 INFO SparkEnv: Registering MapOutputTracker
17/09/15 08:20:58 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 08:20:58 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 08:20:58 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 08:20:58 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-5f9876a6-a5fd-4a20-b660-eb622bc68340
17/09/15 08:20:58 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 08:20:58 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 08:20:58 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 08:20:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 08:20:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 08:20:58 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 08:20:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 08:20:58 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:42145/jars/pflock_2.11-1.0.jar with timestamp 1505488858780
17/09/15 08:20:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 08:20:59 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 48 ms (0 ms spent in bootstraps)
17/09/15 08:20:59 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915082059-0000
17/09/15 08:20:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37245.
17/09/15 08:20:59 INFO NettyBlockTransferService: Server created on 169.235.27.138:37245
17/09/15 08:20:59 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 08:20:59 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 37245, None)
17/09/15 08:20:59 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:37245 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 37245, None)
17/09/15 08:20:59 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 37245, None)
17/09/15 08:20:59 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 37245, None)
17/09/15 08:20:59 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915082059-0000/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 6 cores
17/09/15 08:20:59 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915082059-0000/0 on hostPort 169.235.27.138:36465 with 6 cores, 12.0 GB RAM
17/09/15 08:20:59 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170915082059-0000/0 is now RUNNING
17/09/15 08:21:00 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915082059-0000
17/09/15 08:21:00 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 08:21:00 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915082059-0000 on 6 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       50.0        80K    160.303     89.491    249.794     575930        663          6       1024    08:25:11.075
    PFlock       60.0        80K    152.016     99.054    251.070     700950        935          6       1024    08:29:22.307
    PFlock       70.0        80K    160.234    108.555    268.789     833016       1097          6       1024    08:33:51.193
    PFlock       80.0        80K    169.832    120.341    290.173     974432       1289          6       1024    08:38:41.461
    PFlock       90.0        80K    181.153    132.382    313.535    1130136       1567          6       1024    08:43:55.089
    PFlock      100.0        80K    191.332    146.010    337.342    1302882       1853          6       1024    08:49:32.544
Done!!!
Running in 6 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 08:49:37 INFO SparkContext: Running Spark version 2.1.0
17/09/15 08:49:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 08:49:38 INFO SecurityManager: Changing view acls to: acald013
17/09/15 08:49:38 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 08:49:38 INFO SecurityManager: Changing view acls groups to: 
17/09/15 08:49:38 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 08:49:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 08:49:39 INFO Utils: Successfully started service 'sparkDriver' on port 41151.
17/09/15 08:49:39 INFO SparkEnv: Registering MapOutputTracker
17/09/15 08:49:39 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 08:49:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 08:49:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 08:49:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-fd79f2a3-136b-4d90-821c-0f7d02cf8809
17/09/15 08:49:39 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 08:49:39 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 08:49:40 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 08:49:40 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 08:49:40 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 08:49:40 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 08:49:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 08:49:40 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:41151/jars/pflock_2.11-1.0.jar with timestamp 1505490580111
17/09/15 08:49:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 08:49:40 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 50 ms (0 ms spent in bootstraps)
17/09/15 08:49:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915084940-0001
17/09/15 08:49:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915084940-0001/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 6 cores
17/09/15 08:49:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915084940-0001/0 on hostPort 169.235.27.138:36465 with 6 cores, 12.0 GB RAM
17/09/15 08:49:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37594.
17/09/15 08:49:40 INFO NettyBlockTransferService: Server created on 169.235.27.138:37594
17/09/15 08:49:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 08:49:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 37594, None)
17/09/15 08:49:40 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:37594 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 37594, None)
17/09/15 08:49:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 37594, None)
17/09/15 08:49:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 37594, None)
17/09/15 08:49:41 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915084940-0001
17/09/15 08:49:41 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 08:49:41 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915084940-0001 on 6 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       50.0        80K    158.815     88.156    246.971     575930        663          6       1024    08:53:49.706
    PFlock       60.0        80K    149.903     96.718    246.621     700950        935          6       1024    08:57:56.484
    PFlock       70.0        80K    159.893    105.983    265.876     833016       1097          6       1024    09:02:22.459
    PFlock       80.0        80K    171.809    117.698    289.507     974432       1289          6       1024    09:07:12.060
    PFlock       90.0        80K    179.282    129.120    308.402    1130136       1567          6       1024    09:12:20.556
    PFlock      100.0        80K    191.744    141.932    333.676    1302882       1853          6       1024    09:17:54.325
Done!!!
Running in 5 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 09:18:00 INFO SparkContext: Running Spark version 2.1.0
17/09/15 09:18:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 09:18:00 INFO SecurityManager: Changing view acls to: acald013
17/09/15 09:18:00 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 09:18:00 INFO SecurityManager: Changing view acls groups to: 
17/09/15 09:18:00 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 09:18:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 09:18:01 INFO Utils: Successfully started service 'sparkDriver' on port 38918.
17/09/15 09:18:01 INFO SparkEnv: Registering MapOutputTracker
17/09/15 09:18:01 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 09:18:01 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 09:18:01 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 09:18:01 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-07870b08-1e20-482e-bf6e-220dd949d313
17/09/15 09:18:01 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 09:18:01 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 09:18:01 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 09:18:01 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 09:18:01 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 09:18:02 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 09:18:02 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 09:18:02 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:38918/jars/pflock_2.11-1.0.jar with timestamp 1505492282056
17/09/15 09:18:02 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 09:18:02 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 46 ms (0 ms spent in bootstraps)
17/09/15 09:18:02 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915091802-0002
17/09/15 09:18:02 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915091802-0002/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 5 cores
17/09/15 09:18:02 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915091802-0002/0 on hostPort 169.235.27.138:36465 with 5 cores, 12.0 GB RAM
17/09/15 09:18:02 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44371.
17/09/15 09:18:02 INFO NettyBlockTransferService: Server created on 169.235.27.138:44371
17/09/15 09:18:02 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 09:18:02 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 44371, None)
17/09/15 09:18:02 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170915091802-0002/0 is now RUNNING
17/09/15 09:18:02 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:44371 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 44371, None)
17/09/15 09:18:02 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 44371, None)
17/09/15 09:18:02 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 44371, None)
17/09/15 09:18:03 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915091802-0002
17/09/15 09:18:03 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 09:18:03 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915091802-0002 on 5 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       50.0        80K    180.933    102.482    283.415     575930        663          5       1024    09:22:47.878
    PFlock       60.0        80K    174.015    112.488    286.503     700950        935          5       1024    09:27:34.544
    PFlock       70.0        80K    186.531    124.126    310.657     833016       1097          5       1024    09:32:45.297
    PFlock       80.0        80K    194.451    138.394    332.845     974432       1289          5       1024    09:38:18.235
    PFlock       90.0        80K    210.409    152.909    363.318    1130136       1567          5       1024    09:44:21.645
    PFlock      100.0        80K    220.536    168.317    388.853    1302882       1853          5       1024    09:50:50.587
Done!!!
Running in 5 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 09:50:56 INFO SparkContext: Running Spark version 2.1.0
17/09/15 09:50:56 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 09:50:56 INFO SecurityManager: Changing view acls to: acald013
17/09/15 09:50:56 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 09:50:56 INFO SecurityManager: Changing view acls groups to: 
17/09/15 09:50:56 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 09:50:56 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 09:50:57 INFO Utils: Successfully started service 'sparkDriver' on port 41661.
17/09/15 09:50:57 INFO SparkEnv: Registering MapOutputTracker
17/09/15 09:50:57 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 09:50:57 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 09:50:57 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 09:50:57 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-3f00b2ae-8da5-4509-90b4-08056207bad6
17/09/15 09:50:57 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 09:50:57 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 09:50:57 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 09:50:58 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 09:50:58 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 09:50:58 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 09:50:58 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 09:50:58 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:41661/jars/pflock_2.11-1.0.jar with timestamp 1505494258088
17/09/15 09:50:58 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 09:50:58 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 44 ms (0 ms spent in bootstraps)
17/09/15 09:50:58 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915095058-0003
17/09/15 09:50:58 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915095058-0003/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 5 cores
17/09/15 09:50:58 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915095058-0003/0 on hostPort 169.235.27.138:36465 with 5 cores, 12.0 GB RAM
17/09/15 09:50:58 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43138.
17/09/15 09:50:58 INFO NettyBlockTransferService: Server created on 169.235.27.138:43138
17/09/15 09:50:58 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 09:50:58 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 43138, None)
17/09/15 09:50:58 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170915095058-0003/0 is now RUNNING
17/09/15 09:50:58 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:43138 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 43138, None)
17/09/15 09:50:58 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 43138, None)
17/09/15 09:50:58 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 43138, None)
17/09/15 09:50:59 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915095058-0003
17/09/15 09:50:59 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 09:50:59 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915095058-0003 on 5 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       50.0        80K    178.051    100.749    278.800     575930        663          5       1024    09:55:39.297
    PFlock       60.0        80K    170.889    110.440    281.329     700950        935          5       1024    10:00:20.790
    PFlock       70.0        80K    182.523    122.787    305.310     833016       1097          5       1024    10:05:26.196
    PFlock       80.0        80K    195.302    134.849    330.151     974432       1289          5       1024    10:10:56.436
    PFlock       90.0        80K    207.619    149.694    357.313    1130136       1567          5       1024    10:16:53.839
    PFlock      100.0        80K    219.612    165.572    385.184    1302882       1853          5       1024    10:23:19.112
Done!!!
Running in 4 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 10:23:24 INFO SparkContext: Running Spark version 2.1.0
17/09/15 10:23:24 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 10:23:25 INFO SecurityManager: Changing view acls to: acald013
17/09/15 10:23:25 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 10:23:25 INFO SecurityManager: Changing view acls groups to: 
17/09/15 10:23:25 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 10:23:25 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 10:23:25 INFO Utils: Successfully started service 'sparkDriver' on port 32996.
17/09/15 10:23:25 INFO SparkEnv: Registering MapOutputTracker
17/09/15 10:23:25 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 10:23:25 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 10:23:25 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 10:23:25 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-46dcee4d-06ca-4ef4-98fc-f1fd5dbf466b
17/09/15 10:23:25 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 10:23:26 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 10:23:26 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 10:23:26 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 10:23:26 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 10:23:26 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 10:23:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 10:23:26 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:32996/jars/pflock_2.11-1.0.jar with timestamp 1505496206632
17/09/15 10:23:26 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 10:23:26 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 43 ms (0 ms spent in bootstraps)
17/09/15 10:23:27 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915102327-0004
17/09/15 10:23:27 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915102327-0004/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 4 cores
17/09/15 10:23:27 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915102327-0004/0 on hostPort 169.235.27.138:36465 with 4 cores, 12.0 GB RAM
17/09/15 10:23:27 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38669.
17/09/15 10:23:27 INFO NettyBlockTransferService: Server created on 169.235.27.138:38669
17/09/15 10:23:27 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 10:23:27 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 38669, None)
17/09/15 10:23:27 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170915102327-0004/0 is now RUNNING
17/09/15 10:23:27 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:38669 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 38669, None)
17/09/15 10:23:27 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 38669, None)
17/09/15 10:23:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 38669, None)
17/09/15 10:23:27 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915102327-0004
17/09/15 10:23:27 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 10:23:27 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915102327-0004 on 4 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
    PFlock       50.0        80K    210.759    122.092    332.851     575930        663          4       1024    10:29:01.946
    PFlock       60.0        80K    204.944    135.077    340.021     700950        935          4       1024    10:34:42.127
    PFlock       70.0        80K    218.749    148.979    367.728     833016       1097          4       1024    10:40:49.951
    PFlock       80.0        80K    230.744    164.986    395.730     974432       1289          4       1024    10:47:25.771
    PFlock       90.0        80K    248.164    183.037    431.201    1130136       1567          4       1024    10:54:37.062
    PFlock      100.0        80K    266.427    200.894    467.321    1302882       1853          4       1024    11:02:24.471
Done!!!
Running in 4 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/15 11:02:29 INFO SparkContext: Running Spark version 2.1.0
17/09/15 11:02:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/15 11:02:30 INFO SecurityManager: Changing view acls to: acald013
17/09/15 11:02:30 INFO SecurityManager: Changing modify acls to: acald013
17/09/15 11:02:30 INFO SecurityManager: Changing view acls groups to: 
17/09/15 11:02:30 INFO SecurityManager: Changing modify acls groups to: 
17/09/15 11:02:30 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/15 11:02:31 INFO Utils: Successfully started service 'sparkDriver' on port 34051.
17/09/15 11:02:31 INFO SparkEnv: Registering MapOutputTracker
17/09/15 11:02:31 INFO SparkEnv: Registering BlockManagerMaster
17/09/15 11:02:31 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/15 11:02:31 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/15 11:02:31 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-56c13d97-81ae-410a-a445-2c23540399ed
17/09/15 11:02:31 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/15 11:02:31 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/15 11:02:31 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.
17/09/15 11:02:31 WARN Utils: Service 'SparkUI' could not bind on port 4041. Attempting port 4042.
17/09/15 11:02:31 WARN Utils: Service 'SparkUI' could not bind on port 4042. Attempting port 4043.
17/09/15 11:02:31 INFO Utils: Successfully started service 'SparkUI' on port 4043.
17/09/15 11:02:31 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4043
17/09/15 11:02:32 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:34051/jars/pflock_2.11-1.0.jar with timestamp 1505498552030
17/09/15 11:02:32 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/15 11:02:32 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 45 ms (0 ms spent in bootstraps)
17/09/15 11:02:32 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170915110232-0005
17/09/15 11:02:32 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170915110232-0005/0 on worker-20170915082044-169.235.27.138-36465 (169.235.27.138:36465) with 4 cores
17/09/15 11:02:32 INFO StandaloneSchedulerBackend: Granted executor ID app-20170915110232-0005/0 on hostPort 169.235.27.138:36465 with 4 cores, 12.0 GB RAM
17/09/15 11:02:32 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41131.
17/09/15 11:02:32 INFO NettyBlockTransferService: Server created on 169.235.27.138:41131
17/09/15 11:02:32 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/15 11:02:32 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 41131, None)
17/09/15 11:02:32 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170915110232-0005/0 is now RUNNING
17/09/15 11:02:32 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:41131 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 41131, None)
17/09/15 11:02:32 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 41131, None)
17/09/15 11:02:32 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 41131, None)
17/09/15 11:02:33 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170915110232-0005
17/09/15 11:02:33 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/15 11:02:33 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170915110232-0005 on 4 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
