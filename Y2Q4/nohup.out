Running in 2 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 12:00:25 INFO SparkContext: Running Spark version 2.1.0
17/09/08 12:00:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 12:00:26 INFO SecurityManager: Changing view acls to: acald013
17/09/08 12:00:26 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 12:00:26 INFO SecurityManager: Changing view acls groups to: 
17/09/08 12:00:26 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 12:00:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 12:00:26 INFO Utils: Successfully started service 'sparkDriver' on port 43705.
17/09/08 12:00:26 INFO SparkEnv: Registering MapOutputTracker
17/09/08 12:00:26 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 12:00:26 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 12:00:26 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 12:00:26 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-ea15c209-6b0a-42ba-a6a7-7025dd490926
17/09/08 12:00:26 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 12:00:26 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 12:00:26 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 12:00:26 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 12:00:26 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:43705/jars/pflock_2.11-1.0.jar with timestamp 1504897226665
17/09/08 12:00:26 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:43705/files/metrics.properties with timestamp 1504897226805
17/09/08 12:00:26 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-b5b480fc-59ff-4f2e-8eab-2fb8eaf150f4/userFiles-c8d8c133-280e-4a25-aae0-27723ed7014e/metrics.properties
17/09/08 12:00:26 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 12:00:26 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 20 ms (0 ms spent in bootstraps)
17/09/08 12:00:26 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908120026-0006
17/09/08 12:00:26 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908120026-0006/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 2 cores
17/09/08 12:00:26 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908120026-0006/0 on hostPort 169.235.27.135:41135 with 2 cores, 12.0 GB RAM
17/09/08 12:00:26 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45282.
17/09/08 12:00:26 INFO NettyBlockTransferService: Server created on 169.235.27.134:45282
17/09/08 12:00:26 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 12:00:26 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 45282, None)
17/09/08 12:00:26 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170908120026-0006/0 is now RUNNING
17/09/08 12:00:26 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:45282 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 45282, None)
17/09/08 12:00:26 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 45282, None)
17/09/08 12:00:27 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 45282, None)
17/09/08 12:00:27 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908120026-0006
17/09/08 12:00:27 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 12:00:27 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908120026-0006 on 2 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	59.826	25.088	2	12:01:52.808
PFlock	20.0	160K	370689	62.543	37.681	2	12:03:33.118
PFlock	30.0	160K	703992	72.932	54.716	2	12:05:40.816
PFlock	40.0	160K	1106670	84.535	74.071	2	12:08:19.462
PFlock	50.0	160K	1554128	96.836	93.087	2	12:11:29.424
PFlock	60.0	160K	2045910	108.539	116.541	2	12:15:14.540
PFlock	70.0	160K	2582295	122.842	142.985	2	12:19:40.401
PFlock	80.0	160K	3165396	135.561	171.949	2	12:24:47.947
PFlock	90.0	160K	3800068	152.589	204.981	2	12:30:45.550
PFlock	100.0	160K	4498819	171.437	242.661	2	12:37:39.687
Done!!!
Running in 2 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 12:37:42 INFO SparkContext: Running Spark version 2.1.0
17/09/08 12:37:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 12:37:42 INFO SecurityManager: Changing view acls to: acald013
17/09/08 12:37:42 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 12:37:42 INFO SecurityManager: Changing view acls groups to: 
17/09/08 12:37:42 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 12:37:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 12:37:42 INFO Utils: Successfully started service 'sparkDriver' on port 39524.
17/09/08 12:37:42 INFO SparkEnv: Registering MapOutputTracker
17/09/08 12:37:42 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 12:37:42 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 12:37:42 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 12:37:42 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b23c0e12-e526-40ac-a126-fde885fca3db
17/09/08 12:37:42 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 12:37:42 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 12:37:42 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 12:37:42 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 12:37:43 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:39524/jars/pflock_2.11-1.0.jar with timestamp 1504899463002
17/09/08 12:37:43 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:39524/files/metrics.properties with timestamp 1504899463151
17/09/08 12:37:43 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-7b9ff69b-2050-444f-a95e-0174f904e404/userFiles-23e5afe5-2acc-451c-8d3c-bf21afa6f790/metrics.properties
17/09/08 12:37:43 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 12:37:43 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 20 ms (0 ms spent in bootstraps)
17/09/08 12:37:43 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908123743-0007
17/09/08 12:37:43 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908123743-0007/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 2 cores
17/09/08 12:37:43 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908123743-0007/0 on hostPort 169.235.27.135:41135 with 2 cores, 12.0 GB RAM
17/09/08 12:37:43 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44190.
17/09/08 12:37:43 INFO NettyBlockTransferService: Server created on 169.235.27.134:44190
17/09/08 12:37:43 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 12:37:43 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 44190, None)
17/09/08 12:37:43 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:44190 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 44190, None)
17/09/08 12:37:43 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 44190, None)
17/09/08 12:37:43 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 44190, None)
17/09/08 12:37:43 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908123743-0007
17/09/08 12:37:43 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 12:37:43 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908123743-0007 on 2 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	58.803	24.517	2	12:39:07.369
PFlock	20.0	160K	370689	60.926	37.222	2	12:40:45.589
PFlock	30.0	160K	703992	71.312	53.524	2	12:42:50.471
PFlock	40.0	160K	1106670	83.141	72.991	2	12:45:26.649
PFlock	50.0	160K	1554128	93.703	92.596	2	12:48:32.987
PFlock	60.0	160K	2045910	107.149	115.836	2	12:52:16.015
PFlock	70.0	160K	2582295	121.3	141.743	2	12:56:39.098
PFlock	80.0	160K	3165396	134.856	171.059	2	13:01:45.046
PFlock	90.0	160K	3800068	150.809	206.864	2	13:07:42.753
PFlock	100.0	160K	4498819	169.963	244.541	2	13:14:37.288
Done!!!
Running in 2 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 13:14:39 INFO SparkContext: Running Spark version 2.1.0
17/09/08 13:14:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 13:14:39 INFO SecurityManager: Changing view acls to: acald013
17/09/08 13:14:39 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 13:14:39 INFO SecurityManager: Changing view acls groups to: 
17/09/08 13:14:39 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 13:14:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 13:14:40 INFO Utils: Successfully started service 'sparkDriver' on port 44520.
17/09/08 13:14:40 INFO SparkEnv: Registering MapOutputTracker
17/09/08 13:14:40 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 13:14:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 13:14:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 13:14:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d98b16bb-1372-417f-8bc5-db40b9d84ae4
17/09/08 13:14:40 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 13:14:40 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 13:14:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 13:14:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 13:14:40 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:44520/jars/pflock_2.11-1.0.jar with timestamp 1504901680476
17/09/08 13:14:40 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:44520/files/metrics.properties with timestamp 1504901680620
17/09/08 13:14:40 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-67a9361c-0bf8-4d4d-8088-bf75186672d0/userFiles-e1d5672a-c74f-4820-8256-9cd68d7af245/metrics.properties
17/09/08 13:14:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 13:14:40 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 21 ms (0 ms spent in bootstraps)
17/09/08 13:14:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908131440-0008
17/09/08 13:14:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908131440-0008/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 2 cores
17/09/08 13:14:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908131440-0008/0 on hostPort 169.235.27.135:41135 with 2 cores, 12.0 GB RAM
17/09/08 13:14:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35627.
17/09/08 13:14:40 INFO NettyBlockTransferService: Server created on 169.235.27.134:35627
17/09/08 13:14:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 13:14:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 35627, None)
17/09/08 13:14:40 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:35627 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 35627, None)
17/09/08 13:14:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 35627, None)
17/09/08 13:14:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 35627, None)
17/09/08 13:14:40 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908131440-0008
17/09/08 13:14:41 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 13:14:41 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908131440-0008 on 2 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	59.892	24.869	2	13:16:06.284
PFlock	20.0	160K	370689	62.917	38.314	2	13:17:47.590
PFlock	30.0	160K	703992	73.21	54.117	2	13:19:54.969
PFlock	40.0	160K	1106670	86.144	74.784	2	13:22:35.938
PFlock	50.0	160K	1554128	96.827	94.606	2	13:25:47.409
PFlock	60.0	160K	2045910	109.995	117.014	2	13:29:34.455
PFlock	70.0	160K	2582295	124.005	145.145	2	13:34:03.639
PFlock	80.0	160K	3165396	140.885	173.763	2	13:39:18.323
PFlock	90.0	160K	3800068	156.037	209.482	2	13:45:23.867
PFlock	100.0	160K	4498819	177.38	250.253	2	13:52:31.535
Done!!!
Running in 3 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 13:52:33 INFO SparkContext: Running Spark version 2.1.0
17/09/08 13:52:34 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 13:52:34 INFO SecurityManager: Changing view acls to: acald013
17/09/08 13:52:34 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 13:52:34 INFO SecurityManager: Changing view acls groups to: 
17/09/08 13:52:34 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 13:52:34 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 13:52:34 INFO Utils: Successfully started service 'sparkDriver' on port 35575.
17/09/08 13:52:34 INFO SparkEnv: Registering MapOutputTracker
17/09/08 13:52:34 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 13:52:34 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 13:52:34 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 13:52:34 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-751dc832-5b34-406a-8d28-18c3f1f49e75
17/09/08 13:52:34 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 13:52:34 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 13:52:34 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 13:52:34 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 13:52:34 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:35575/jars/pflock_2.11-1.0.jar with timestamp 1504903954744
17/09/08 13:52:34 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:35575/files/metrics.properties with timestamp 1504903954886
17/09/08 13:52:34 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-1cdf58f9-b746-4a11-9265-6a1e28bc99e9/userFiles-efc64a4f-b115-4ccd-9139-b4487710ae52/metrics.properties
17/09/08 13:52:34 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 13:52:35 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 27 ms (0 ms spent in bootstraps)
17/09/08 13:52:35 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908135235-0009
17/09/08 13:52:35 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908135235-0009/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 3 cores
17/09/08 13:52:35 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908135235-0009/0 on hostPort 169.235.27.135:41135 with 3 cores, 12.0 GB RAM
17/09/08 13:52:35 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45604.
17/09/08 13:52:35 INFO NettyBlockTransferService: Server created on 169.235.27.134:45604
17/09/08 13:52:35 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 13:52:35 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 45604, None)
17/09/08 13:52:35 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:45604 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 45604, None)
17/09/08 13:52:35 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 45604, None)
17/09/08 13:52:35 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 45604, None)
17/09/08 13:52:35 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908135235-0009
17/09/08 13:52:35 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 13:52:35 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908135235-0009 on 3 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	48.34	18.306	3	13:53:42.566
PFlock	20.0	160K	370689	46.304	25.844	3	13:54:54.785
PFlock	30.0	160K	703992	54.391	36.783	3	13:56:26.006
PFlock	40.0	160K	1106670	62.754	49.786	3	13:58:18.586
PFlock	50.0	160K	1554128	70.563	63.9	3	14:00:33.096
PFlock	60.0	160K	2045910	81.503	79.402	3	14:03:14.036
PFlock	70.0	160K	2582295	90.756	96.405	3	14:06:21.233
PFlock	80.0	160K	3165396	102.01	117.8	3	14:10:01.087
PFlock	90.0	160K	3800068	115.585	141.085	3	14:14:17.792
PFlock	100.0	160K	4498819	130.757	170.269	3	14:19:18.850
Done!!!
Running in 3 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 14:19:21 INFO SparkContext: Running Spark version 2.1.0
17/09/08 14:19:21 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 14:19:21 INFO SecurityManager: Changing view acls to: acald013
17/09/08 14:19:21 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 14:19:21 INFO SecurityManager: Changing view acls groups to: 
17/09/08 14:19:21 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 14:19:21 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 14:19:21 INFO Utils: Successfully started service 'sparkDriver' on port 38147.
17/09/08 14:19:21 INFO SparkEnv: Registering MapOutputTracker
17/09/08 14:19:21 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 14:19:21 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 14:19:21 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 14:19:21 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a6d73ad1-9df6-408b-8a90-3d661c08b266
17/09/08 14:19:21 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 14:19:21 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 14:19:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 14:19:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 14:19:22 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:38147/jars/pflock_2.11-1.0.jar with timestamp 1504905562096
17/09/08 14:19:22 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:38147/files/metrics.properties with timestamp 1504905562240
17/09/08 14:19:22 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-3d910d34-14c1-4b79-9e2f-5dac78e6f62f/userFiles-af5372b1-9d1e-4709-b632-a8d3569c07fb/metrics.properties
17/09/08 14:19:22 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 14:19:22 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 26 ms (0 ms spent in bootstraps)
17/09/08 14:19:22 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908141922-0010
17/09/08 14:19:22 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908141922-0010/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 3 cores
17/09/08 14:19:22 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908141922-0010/0 on hostPort 169.235.27.135:41135 with 3 cores, 12.0 GB RAM
17/09/08 14:19:22 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39147.
17/09/08 14:19:22 INFO NettyBlockTransferService: Server created on 169.235.27.134:39147
17/09/08 14:19:22 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 14:19:22 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 39147, None)
17/09/08 14:19:22 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:39147 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 39147, None)
17/09/08 14:19:22 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 39147, None)
17/09/08 14:19:22 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 39147, None)
17/09/08 14:19:22 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908141922-0010
17/09/08 14:19:22 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 14:19:22 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908141922-0010 on 3 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	51.045	17.937	3	14:20:32.156
PFlock	20.0	160K	370689	46.532	26.292	3	14:21:45.070
PFlock	30.0	160K	703992	53.593	37.357	3	14:23:16.067
PFlock	40.0	160K	1106670	62.28	50.082	3	14:25:08.471
PFlock	50.0	160K	1554128	70.315	65.024	3	14:27:23.849
PFlock	60.0	160K	2045910	80.887	80.486	3	14:30:05.266
PFlock	70.0	160K	2582295	91.271	99.146	3	14:33:15.717
PFlock	80.0	160K	3165396	102.061	118.529	3	14:36:56.346
PFlock	90.0	160K	3800068	114.048	144.421	3	14:41:14.849
PFlock	100.0	160K	4498819	129.464	175.333	3	14:46:19.676
Done!!!
Running in 3 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 14:46:22 INFO SparkContext: Running Spark version 2.1.0
17/09/08 14:46:22 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 14:46:22 INFO SecurityManager: Changing view acls to: acald013
17/09/08 14:46:22 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 14:46:22 INFO SecurityManager: Changing view acls groups to: 
17/09/08 14:46:22 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 14:46:22 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 14:46:22 INFO Utils: Successfully started service 'sparkDriver' on port 43214.
17/09/08 14:46:22 INFO SparkEnv: Registering MapOutputTracker
17/09/08 14:46:22 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 14:46:22 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 14:46:22 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 14:46:22 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-faf39983-9962-40bf-8b2f-4ef1a5d7f659
17/09/08 14:46:22 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 14:46:22 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 14:46:22 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 14:46:22 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 14:46:22 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:43214/jars/pflock_2.11-1.0.jar with timestamp 1504907182941
17/09/08 14:46:23 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:43214/files/metrics.properties with timestamp 1504907183085
17/09/08 14:46:23 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-3d475f96-e0ea-48bf-85ed-fdf90d7681a6/userFiles-cdd541a1-708b-44ef-8bb3-b3e51593da78/metrics.properties
17/09/08 14:46:23 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 14:46:23 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 19 ms (0 ms spent in bootstraps)
17/09/08 14:46:23 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908144623-0011
17/09/08 14:46:23 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908144623-0011/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 3 cores
17/09/08 14:46:23 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908144623-0011/0 on hostPort 169.235.27.135:41135 with 3 cores, 12.0 GB RAM
17/09/08 14:46:23 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43784.
17/09/08 14:46:23 INFO NettyBlockTransferService: Server created on 169.235.27.134:43784
17/09/08 14:46:23 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 14:46:23 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 43784, None)
17/09/08 14:46:23 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:43784 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 43784, None)
17/09/08 14:46:23 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 43784, None)
17/09/08 14:46:23 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 43784, None)
17/09/08 14:46:23 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908144623-0011
17/09/08 14:46:23 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 14:46:23 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908144623-0011 on 3 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	48.547	17.491	3	14:47:30.014
PFlock	20.0	160K	370689	45.797	25.733	3	14:48:41.617
PFlock	30.0	160K	703992	53.996	37.015	3	14:50:12.678
PFlock	40.0	160K	1106670	61.102	49.384	3	14:52:03.207
PFlock	50.0	160K	1554128	69.693	63.139	3	14:54:16.081
PFlock	60.0	160K	2045910	78.823	78.042	3	14:56:52.981
PFlock	70.0	160K	2582295	90.632	95.822	3	14:59:59.469
PFlock	80.0	160K	3165396	100.244	116.313	3	15:03:36.063
PFlock	90.0	160K	3800068	110.76	138.776	3	15:07:45.630
PFlock	100.0	160K	4498819	128.375	167.466	3	15:12:41.501
Done!!!
Running in 4 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 15:12:43 INFO SparkContext: Running Spark version 2.1.0
17/09/08 15:12:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 15:12:44 INFO SecurityManager: Changing view acls to: acald013
17/09/08 15:12:44 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 15:12:44 INFO SecurityManager: Changing view acls groups to: 
17/09/08 15:12:44 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 15:12:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 15:12:44 INFO Utils: Successfully started service 'sparkDriver' on port 46636.
17/09/08 15:12:44 INFO SparkEnv: Registering MapOutputTracker
17/09/08 15:12:44 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 15:12:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 15:12:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 15:12:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4b5d4f79-b6ce-43d0-b9ab-5c4f0a38ccad
17/09/08 15:12:44 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 15:12:44 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 15:12:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 15:12:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 15:12:44 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:46636/jars/pflock_2.11-1.0.jar with timestamp 1504908764772
17/09/08 15:12:44 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:46636/files/metrics.properties with timestamp 1504908764920
17/09/08 15:12:44 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-f668ad83-5539-4c50-8637-93bc0e4b9154/userFiles-e9b444f1-92e7-4703-a183-75446e7f191b/metrics.properties
17/09/08 15:12:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 15:12:45 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 24 ms (0 ms spent in bootstraps)
17/09/08 15:12:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908151245-0012
17/09/08 15:12:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908151245-0012/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 4 cores
17/09/08 15:12:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908151245-0012/0 on hostPort 169.235.27.135:41135 with 4 cores, 12.0 GB RAM
17/09/08 15:12:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46268.
17/09/08 15:12:45 INFO NettyBlockTransferService: Server created on 169.235.27.134:46268
17/09/08 15:12:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 15:12:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 46268, None)
17/09/08 15:12:45 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:46268 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 46268, None)
17/09/08 15:12:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 46268, None)
17/09/08 15:12:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 46268, None)
17/09/08 15:12:45 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908151245-0012
17/09/08 15:12:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 15:12:45 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908151245-0012 on 4 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	42.478	14.878	4	15:13:43.441
PFlock	20.0	160K	370689	38.281	20.875	4	15:14:42.667
PFlock	30.0	160K	703992	43.55	29.317	4	15:15:55.580
PFlock	40.0	160K	1106670	50.546	38.473	4	15:17:24.637
PFlock	50.0	160K	1554128	56.671	48.083	4	15:19:09.429
PFlock	60.0	160K	2045910	64.222	61.975	4	15:21:15.666
PFlock	70.0	160K	2582295	73.445	75.009	4	15:23:44.154
PFlock	80.0	160K	3165396	82.798	92.143	4	15:26:39.127
PFlock	90.0	160K	3800068	92.079	110.189	4	15:30:01.428
PFlock	100.0	160K	4498819	103.724	132.296	4	15:33:57.474
Done!!!
Running in 4 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 15:33:59 INFO SparkContext: Running Spark version 2.1.0
17/09/08 15:34:00 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 15:34:00 INFO SecurityManager: Changing view acls to: acald013
17/09/08 15:34:00 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 15:34:00 INFO SecurityManager: Changing view acls groups to: 
17/09/08 15:34:00 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 15:34:00 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 15:34:00 INFO Utils: Successfully started service 'sparkDriver' on port 43920.
17/09/08 15:34:00 INFO SparkEnv: Registering MapOutputTracker
17/09/08 15:34:00 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 15:34:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 15:34:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 15:34:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8e138b36-9794-4bcd-b913-e1a866f987a9
17/09/08 15:34:00 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 15:34:00 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 15:34:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 15:34:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 15:34:00 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:43920/jars/pflock_2.11-1.0.jar with timestamp 1504910040683
17/09/08 15:34:00 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:43920/files/metrics.properties with timestamp 1504910040823
17/09/08 15:34:00 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-173603dd-553a-46f4-bd44-dccd35f28cca/userFiles-f17a18d6-5b73-482f-b850-c8dca2c87e36/metrics.properties
17/09/08 15:34:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 15:34:00 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 33 ms (0 ms spent in bootstraps)
17/09/08 15:34:01 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908153400-0013
17/09/08 15:34:01 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35091.
17/09/08 15:34:01 INFO NettyBlockTransferService: Server created on 169.235.27.134:35091
17/09/08 15:34:01 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908153400-0013/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 4 cores
17/09/08 15:34:01 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908153400-0013/0 on hostPort 169.235.27.135:41135 with 4 cores, 12.0 GB RAM
17/09/08 15:34:01 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 15:34:01 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 35091, None)
17/09/08 15:34:01 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:35091 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 35091, None)
17/09/08 15:34:01 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 35091, None)
17/09/08 15:34:01 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 35091, None)
17/09/08 15:34:01 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908153400-0013
17/09/08 15:34:01 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 15:34:01 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908153400-0013 on 4 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	42.504	14.704	4	15:34:59.133
PFlock	20.0	160K	370689	37.48	20.529	4	15:35:57.212
PFlock	30.0	160K	703992	42.708	29.097	4	15:37:09.068
PFlock	40.0	160K	1106670	49.347	37.608	4	15:38:36.062
PFlock	50.0	160K	1554128	55.966	48.404	4	15:40:20.469
PFlock	60.0	160K	2045910	63.613	60.808	4	15:42:24.926
PFlock	70.0	160K	2582295	71.569	74.931	4	15:44:51.472
PFlock	80.0	160K	3165396	79.356	89.525	4	15:47:40.386
PFlock	90.0	160K	3800068	88.844	111.071	4	15:51:00.336
PFlock	100.0	160K	4498819	100.533	131.357	4	15:54:52.257
Done!!!
Running in 4 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 15:54:54 INFO SparkContext: Running Spark version 2.1.0
17/09/08 15:54:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 15:54:59 INFO SecurityManager: Changing view acls to: acald013
17/09/08 15:54:59 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 15:54:59 INFO SecurityManager: Changing view acls groups to: 
17/09/08 15:54:59 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 15:54:59 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 15:55:00 INFO Utils: Successfully started service 'sparkDriver' on port 40688.
17/09/08 15:55:00 INFO SparkEnv: Registering MapOutputTracker
17/09/08 15:55:00 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 15:55:00 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 15:55:00 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 15:55:00 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-cf420467-f6de-4190-aee1-cd9a3ffc80ef
17/09/08 15:55:00 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 15:55:00 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 15:55:00 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 15:55:00 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 15:55:00 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:40688/jars/pflock_2.11-1.0.jar with timestamp 1504911300546
17/09/08 15:55:00 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:40688/files/metrics.properties with timestamp 1504911300699
17/09/08 15:55:00 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-5431f4de-0033-4491-9812-09ce88884613/userFiles-0a3f4f1b-e603-4eee-90c2-86a498a1e955/metrics.properties
17/09/08 15:55:00 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 15:55:00 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 29 ms (0 ms spent in bootstraps)
17/09/08 15:55:00 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908155500-0014
17/09/08 15:55:00 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908155500-0014/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 4 cores
17/09/08 15:55:00 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908155500-0014/0 on hostPort 169.235.27.135:41135 with 4 cores, 12.0 GB RAM
17/09/08 15:55:00 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46553.
17/09/08 15:55:00 INFO NettyBlockTransferService: Server created on 169.235.27.134:46553
17/09/08 15:55:00 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 15:55:00 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 46553, None)
17/09/08 15:55:00 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:46553 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 46553, None)
17/09/08 15:55:00 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 46553, None)
17/09/08 15:55:00 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 46553, None)
17/09/08 15:55:00 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170908155500-0014/0 is now RUNNING
17/09/08 15:55:01 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908155500-0014
17/09/08 15:55:01 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 15:55:01 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908155500-0014 on 4 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	47.971	15.089	4	15:56:04.834
PFlock	20.0	160K	370689	37.851	20.202	4	15:57:02.956
PFlock	30.0	160K	703992	44.036	28.157	4	15:58:15.198
PFlock	40.0	160K	1106670	49.691	36.681	4	15:59:41.613
PFlock	50.0	160K	1554128	56.135	46.073	4	16:01:23.868
PFlock	60.0	160K	2045910	64.531	58.417	4	16:03:26.859
PFlock	70.0	160K	2582295	72.373	71.005	4	16:05:50.281
PFlock	80.0	160K	3165396	81.812	87.56	4	16:08:39.691
PFlock	90.0	160K	3800068	92.17	103.242	4	16:11:55.144
PFlock	100.0	160K	4498819	103.192	126.534	4	16:15:44.900
Done!!!
Running in 5 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 16:15:47 INFO SparkContext: Running Spark version 2.1.0
17/09/08 16:15:47 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 16:15:47 INFO SecurityManager: Changing view acls to: acald013
17/09/08 16:15:47 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 16:15:47 INFO SecurityManager: Changing view acls groups to: 
17/09/08 16:15:47 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 16:15:47 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 16:15:47 INFO Utils: Successfully started service 'sparkDriver' on port 33830.
17/09/08 16:15:47 INFO SparkEnv: Registering MapOutputTracker
17/09/08 16:15:48 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 16:15:48 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 16:15:48 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 16:15:48 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-65a7e7e8-5e01-4f24-a915-7b44f9793279
17/09/08 16:15:48 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 16:15:48 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 16:15:48 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 16:15:48 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 16:15:48 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:33830/jars/pflock_2.11-1.0.jar with timestamp 1504912548277
17/09/08 16:15:48 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:33830/files/metrics.properties with timestamp 1504912548425
17/09/08 16:15:48 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-7266d474-58cc-4e7a-b4a9-ff5fe52490d1/userFiles-3beaa72d-5544-4b52-a1b0-cf2c7f9a964d/metrics.properties
17/09/08 16:15:48 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 16:15:48 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 28 ms (0 ms spent in bootstraps)
17/09/08 16:15:48 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908161548-0015
17/09/08 16:15:48 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908161548-0015/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 5 cores
17/09/08 16:15:48 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40862.
17/09/08 16:15:48 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908161548-0015/0 on hostPort 169.235.27.135:41135 with 5 cores, 12.0 GB RAM
17/09/08 16:15:48 INFO NettyBlockTransferService: Server created on 169.235.27.134:40862
17/09/08 16:15:48 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 16:15:48 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 40862, None)
17/09/08 16:15:48 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:40862 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 40862, None)
17/09/08 16:15:48 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 40862, None)
17/09/08 16:15:48 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 40862, None)
17/09/08 16:15:48 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908161548-0015
17/09/08 16:15:48 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 16:15:48 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908161548-0015 on 5 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	45.864	13.789	5	16:16:49.273
PFlock	20.0	160K	370689	35.342	19.578	5	16:17:44.278
PFlock	30.0	160K	703992	40.59	27.047	5	16:18:51.971
PFlock	40.0	160K	1106670	46.31	35.959	5	16:20:14.283
PFlock	50.0	160K	1554128	52.853	46.106	5	16:21:53.281
PFlock	60.0	160K	2045910	59.814	57.27	5	16:23:50.406
PFlock	70.0	160K	2582295	67.961	72.779	5	16:26:11.181
PFlock	80.0	160K	3165396	77.459	85.887	5	16:28:54.567
PFlock	90.0	160K	3800068	86.487	103.965	5	16:32:05.056
PFlock	100.0	160K	4498819	99.65	128.415	5	16:35:53.152
Done!!!
Running in 5 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 16:35:55 INFO SparkContext: Running Spark version 2.1.0
17/09/08 16:35:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 16:35:55 INFO SecurityManager: Changing view acls to: acald013
17/09/08 16:35:55 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 16:35:55 INFO SecurityManager: Changing view acls groups to: 
17/09/08 16:35:55 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 16:35:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 16:35:56 INFO Utils: Successfully started service 'sparkDriver' on port 45512.
17/09/08 16:35:56 INFO SparkEnv: Registering MapOutputTracker
17/09/08 16:35:56 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 16:35:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 16:35:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 16:35:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8ffb305e-6f1a-4d0b-adea-b628a0cfa171
17/09/08 16:35:56 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 16:35:56 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 16:35:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 16:35:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 16:35:56 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:45512/jars/pflock_2.11-1.0.jar with timestamp 1504913756453
17/09/08 16:35:56 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:45512/files/metrics.properties with timestamp 1504913756597
17/09/08 16:35:56 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-03b5049c-29c3-48a4-bedb-4ad277337bee/userFiles-fce44c82-3936-4160-8513-16dad9e56765/metrics.properties
17/09/08 16:35:56 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 16:35:56 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 19 ms (0 ms spent in bootstraps)
17/09/08 16:35:56 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908163556-0016
17/09/08 16:35:56 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908163556-0016/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 5 cores
17/09/08 16:35:56 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908163556-0016/0 on hostPort 169.235.27.135:41135 with 5 cores, 12.0 GB RAM
17/09/08 16:35:56 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33318.
17/09/08 16:35:56 INFO NettyBlockTransferService: Server created on 169.235.27.134:33318
17/09/08 16:35:56 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 16:35:56 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 33318, None)
17/09/08 16:35:56 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:33318 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 33318, None)
17/09/08 16:35:56 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 33318, None)
17/09/08 16:35:56 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 33318, None)
17/09/08 16:35:56 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908163556-0016
17/09/08 16:35:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 16:35:57 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908163556-0016 on 5 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	41.437	14.129	5	16:36:53.275
PFlock	20.0	160K	370689	36.968	19.186	5	16:37:49.501
PFlock	30.0	160K	703992	42.43	27.209	5	16:38:59.193
PFlock	40.0	160K	1106670	48.834	36.474	5	16:40:24.538
PFlock	50.0	160K	1554128	55.576	46.037	5	16:42:06.189
PFlock	60.0	160K	2045910	63.593	57.671	5	16:44:07.486
PFlock	70.0	160K	2582295	72.14	72.315	5	16:46:31.978
PFlock	80.0	160K	3165396	79.468	88.697	5	16:49:20.177
PFlock	90.0	160K	3800068	90.47	104.431	5	16:52:35.109
PFlock	100.0	160K	4498819	102.457	127.114	5	16:56:24.710
Done!!!
Running in 5 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 16:56:27 INFO SparkContext: Running Spark version 2.1.0
17/09/08 16:56:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 16:56:27 INFO SecurityManager: Changing view acls to: acald013
17/09/08 16:56:27 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 16:56:27 INFO SecurityManager: Changing view acls groups to: 
17/09/08 16:56:27 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 16:56:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 16:56:27 INFO Utils: Successfully started service 'sparkDriver' on port 37993.
17/09/08 16:56:27 INFO SparkEnv: Registering MapOutputTracker
17/09/08 16:56:27 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 16:56:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 16:56:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 16:56:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e8ce5cbc-f5bb-4395-82f7-8ff919c5c58c
17/09/08 16:56:27 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 16:56:27 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 16:56:27 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 16:56:27 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 16:56:27 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:37993/jars/pflock_2.11-1.0.jar with timestamp 1504914987963
17/09/08 16:56:28 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:37993/files/metrics.properties with timestamp 1504914988110
17/09/08 16:56:28 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-498dbcc8-2c8a-4b7a-98f6-151ded54bed0/userFiles-15cca5b9-a731-4b71-930b-f26f3e033c9d/metrics.properties
17/09/08 16:56:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 16:56:28 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 22 ms (0 ms spent in bootstraps)
17/09/08 16:56:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908165628-0017
17/09/08 16:56:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908165628-0017/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 5 cores
17/09/08 16:56:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908165628-0017/0 on hostPort 169.235.27.135:41135 with 5 cores, 12.0 GB RAM
17/09/08 16:56:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36057.
17/09/08 16:56:28 INFO NettyBlockTransferService: Server created on 169.235.27.134:36057
17/09/08 16:56:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 16:56:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 36057, None)
17/09/08 16:56:28 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:36057 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 36057, None)
17/09/08 16:56:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 36057, None)
17/09/08 16:56:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 36057, None)
17/09/08 16:56:28 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908165628-0017
17/09/08 16:56:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 16:56:28 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
17/09/08 16:56:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170908165628-0017/0 is now RUNNING
Running app-20170908165628-0017 on 5 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	40.85	13.459	5	16:57:23.361
PFlock	20.0	160K	370689	35.634	18.757	5	16:58:17.827
PFlock	30.0	160K	703992	41.261	26.239	5	16:59:25.381
PFlock	40.0	160K	1106670	47.171	35.382	5	17:00:47.980
PFlock	50.0	160K	1554128	53.763	46.039	5	17:02:27.825
PFlock	60.0	160K	2045910	61.843	57.621	5	17:04:27.332
PFlock	70.0	160K	2582295	68.918	69.819	5	17:06:46.114
PFlock	80.0	160K	3165396	77.906	85.518	5	17:09:29.573
PFlock	90.0	160K	3800068	87.172	103.119	5	17:12:39.896
PFlock	100.0	160K	4498819	100.108	125.175	5	17:16:25.208
Done!!!
Running in 6 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 17:16:27 INFO SparkContext: Running Spark version 2.1.0
17/09/08 17:16:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 17:16:27 INFO SecurityManager: Changing view acls to: acald013
17/09/08 17:16:27 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 17:16:27 INFO SecurityManager: Changing view acls groups to: 
17/09/08 17:16:27 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 17:16:27 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 17:16:28 INFO Utils: Successfully started service 'sparkDriver' on port 46577.
17/09/08 17:16:28 INFO SparkEnv: Registering MapOutputTracker
17/09/08 17:16:28 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 17:16:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 17:16:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 17:16:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c0334b92-9241-4298-9bbd-24c4a3c7aa63
17/09/08 17:16:28 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 17:16:28 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 17:16:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 17:16:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 17:16:28 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:46577/jars/pflock_2.11-1.0.jar with timestamp 1504916188428
17/09/08 17:16:28 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:46577/files/metrics.properties with timestamp 1504916188572
17/09/08 17:16:28 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-d047ad8d-423c-4e82-83d3-8d57c06f41b5/userFiles-07fa09d9-77ac-4d67-8550-e2359a78ee12/metrics.properties
17/09/08 17:16:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 17:16:28 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 22 ms (0 ms spent in bootstraps)
17/09/08 17:16:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908171628-0018
17/09/08 17:16:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908171628-0018/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 6 cores
17/09/08 17:16:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908171628-0018/0 on hostPort 169.235.27.135:41135 with 6 cores, 12.0 GB RAM
17/09/08 17:16:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37760.
17/09/08 17:16:28 INFO NettyBlockTransferService: Server created on 169.235.27.134:37760
17/09/08 17:16:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 17:16:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 37760, None)
17/09/08 17:16:28 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:37760 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 37760, None)
17/09/08 17:16:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 37760, None)
17/09/08 17:16:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 37760, None)
17/09/08 17:16:28 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908171628-0018
17/09/08 17:16:28 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 17:16:28 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908171628-0018 on 6 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	40.715	13.335	6	17:17:23.526
PFlock	20.0	160K	370689	34.448	18.129	6	17:18:16.170
PFlock	30.0	160K	703992	39.893	25.459	6	17:19:21.569
PFlock	40.0	160K	1106670	45.82	33.83	6	17:20:41.259
PFlock	50.0	160K	1554128	53.043	43.075	6	17:22:17.424
PFlock	60.0	160K	2045910	59.461	54.845	6	17:24:11.772
PFlock	70.0	160K	2582295	68.256	66.89	6	17:26:26.952
PFlock	80.0	160K	3165396	76.99	83.615	6	17:29:07.590
PFlock	90.0	160K	3800068	85.647	98.837	6	17:32:12.106
PFlock	100.0	160K	4498819	97.886	121.356	6	17:35:51.378
Done!!!
Running in 6 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 17:35:54 INFO SparkContext: Running Spark version 2.1.0
17/09/08 17:35:54 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 17:35:55 INFO SecurityManager: Changing view acls to: acald013
17/09/08 17:35:55 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 17:35:55 INFO SecurityManager: Changing view acls groups to: 
17/09/08 17:35:55 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 17:35:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 17:35:55 INFO Utils: Successfully started service 'sparkDriver' on port 33864.
17/09/08 17:35:55 INFO SparkEnv: Registering MapOutputTracker
17/09/08 17:35:55 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 17:35:55 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 17:35:55 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 17:35:55 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a02aa966-5c2c-40c7-a060-9d4e7d5662c6
17/09/08 17:35:55 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 17:35:55 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 17:35:55 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 17:35:55 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 17:35:55 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:33864/jars/pflock_2.11-1.0.jar with timestamp 1504917355572
17/09/08 17:35:55 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:33864/files/metrics.properties with timestamp 1504917355718
17/09/08 17:35:55 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-b7a7f74c-7ef0-4e79-89ef-353088b16e59/userFiles-4900c76e-ecdd-4faa-904c-a7bae7c7d399/metrics.properties
17/09/08 17:35:55 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 17:35:55 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 20 ms (0 ms spent in bootstraps)
17/09/08 17:35:55 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908173555-0019
17/09/08 17:35:55 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908173555-0019/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 6 cores
17/09/08 17:35:55 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908173555-0019/0 on hostPort 169.235.27.135:41135 with 6 cores, 12.0 GB RAM
17/09/08 17:35:55 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 38496.
17/09/08 17:35:55 INFO NettyBlockTransferService: Server created on 169.235.27.134:38496
17/09/08 17:35:55 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 17:35:55 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 38496, None)
17/09/08 17:35:55 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:38496 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 38496, None)
17/09/08 17:35:55 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 38496, None)
17/09/08 17:35:55 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 38496, None)
17/09/08 17:35:55 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170908173555-0019/0 is now RUNNING
17/09/08 17:35:56 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908173555-0019
17/09/08 17:35:56 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 17:35:56 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908173555-0019 on 6 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	39.558	13.385	6	17:36:49.515
PFlock	20.0	160K	370689	35.265	18.79	6	17:37:43.647
PFlock	30.0	160K	703992	40.885	26.96	6	17:38:51.540
PFlock	40.0	160K	1106670	47.163	36.09	6	17:40:14.831
PFlock	50.0	160K	1554128	54.599	46.015	6	17:41:55.490
PFlock	60.0	160K	2045910	62.441	57.295	6	17:43:55.260
PFlock	70.0	160K	2582295	72.112	71.505	6	17:46:18.910
PFlock	80.0	160K	3165396	79.803	88.595	6	17:49:07.341
PFlock	90.0	160K	3800068	89.248	105.543	6	17:52:22.162
PFlock	100.0	160K	4498819	101.913	128.123	6	17:56:12.227
Done!!!
Running in 6 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 17:56:14 INFO SparkContext: Running Spark version 2.1.0
17/09/08 17:56:14 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 17:56:14 INFO SecurityManager: Changing view acls to: acald013
17/09/08 17:56:14 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 17:56:14 INFO SecurityManager: Changing view acls groups to: 
17/09/08 17:56:14 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 17:56:14 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 17:56:15 INFO Utils: Successfully started service 'sparkDriver' on port 36457.
17/09/08 17:56:15 INFO SparkEnv: Registering MapOutputTracker
17/09/08 17:56:15 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 17:56:15 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 17:56:15 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 17:56:15 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8b59f3cc-3ffe-4e99-91f4-15986ce32f71
17/09/08 17:56:15 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 17:56:15 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 17:56:15 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 17:56:15 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 17:56:15 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:36457/jars/pflock_2.11-1.0.jar with timestamp 1504918575466
17/09/08 17:56:15 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:36457/files/metrics.properties with timestamp 1504918575609
17/09/08 17:56:15 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-94c31b29-5516-4bd1-b90b-8c1b04af01af/userFiles-36fe8caa-2051-419f-bb53-e8e1b00db0a7/metrics.properties
17/09/08 17:56:15 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 17:56:15 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 28 ms (0 ms spent in bootstraps)
17/09/08 17:56:15 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908175615-0020
17/09/08 17:56:15 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908175615-0020/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 6 cores
17/09/08 17:56:15 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908175615-0020/0 on hostPort 169.235.27.135:41135 with 6 cores, 12.0 GB RAM
17/09/08 17:56:15 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 46805.
17/09/08 17:56:15 INFO NettyBlockTransferService: Server created on 169.235.27.134:46805
17/09/08 17:56:15 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 17:56:15 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 46805, None)
17/09/08 17:56:15 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:46805 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 46805, None)
17/09/08 17:56:15 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 46805, None)
17/09/08 17:56:15 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 46805, None)
17/09/08 17:56:16 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908175615-0020
17/09/08 17:56:16 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 17:56:16 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908175615-0020 on 6 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	39.492	13.44	6	17:57:09.626
PFlock	20.0	160K	370689	33.659	18.202	6	17:58:01.553
PFlock	30.0	160K	703992	39.997	26.366	6	17:59:07.962
PFlock	40.0	160K	1106670	45.165	35.603	6	18:00:28.771
PFlock	50.0	160K	1554128	52.835	44.237	6	18:02:05.882
PFlock	60.0	160K	2045910	57.92	55.021	6	18:03:58.869
PFlock	70.0	160K	2582295	66.359	68.394	6	18:06:13.663
PFlock	80.0	160K	3165396	74.31	85.781	6	18:08:53.789
PFlock	90.0	160K	3800068	83.796	101.896	6	18:11:59.513
PFlock	100.0	160K	4498819	94.394	122.425	6	18:15:36.365
Done!!!
Running in 7 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 18:15:38 INFO SparkContext: Running Spark version 2.1.0
17/09/08 18:15:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 18:15:39 INFO SecurityManager: Changing view acls to: acald013
17/09/08 18:15:39 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 18:15:39 INFO SecurityManager: Changing view acls groups to: 
17/09/08 18:15:39 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 18:15:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 18:15:39 INFO Utils: Successfully started service 'sparkDriver' on port 34050.
17/09/08 18:15:39 INFO SparkEnv: Registering MapOutputTracker
17/09/08 18:15:39 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 18:15:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 18:15:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 18:15:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-4b6941d4-3e8d-4efa-9d89-801156f55070
17/09/08 18:15:39 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 18:15:39 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 18:15:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 18:15:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 18:15:39 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:34050/jars/pflock_2.11-1.0.jar with timestamp 1504919739692
17/09/08 18:15:39 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:34050/files/metrics.properties with timestamp 1504919739836
17/09/08 18:15:39 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-33778599-8f65-4a7d-b8d1-1a12584fe5fe/userFiles-3f19d4fb-d7d0-4da3-a566-9c43fe2cdec9/metrics.properties
17/09/08 18:15:39 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 18:15:39 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 29 ms (0 ms spent in bootstraps)
17/09/08 18:15:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908181539-0021
17/09/08 18:15:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908181539-0021/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 7 cores
17/09/08 18:15:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40596.
17/09/08 18:15:40 INFO NettyBlockTransferService: Server created on 169.235.27.134:40596
17/09/08 18:15:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908181539-0021/0 on hostPort 169.235.27.135:41135 with 7 cores, 12.0 GB RAM
17/09/08 18:15:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 18:15:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 40596, None)
17/09/08 18:15:40 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:40596 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 40596, None)
17/09/08 18:15:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 40596, None)
17/09/08 18:15:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 40596, None)
17/09/08 18:15:40 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908181539-0021
17/09/08 18:15:40 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 18:15:40 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908181539-0021 on 7 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	40.748	12.705	7	18:16:34.238
PFlock	20.0	160K	370689	33.221	17.867	7	18:17:25.398
PFlock	30.0	160K	703992	38.811	25.145	7	18:18:29.407
PFlock	40.0	160K	1106670	44.866	32.733	7	18:19:47.050
PFlock	50.0	160K	1554128	50.823	41.981	7	18:21:19.895
PFlock	60.0	160K	2045910	59.113	52.982	7	18:23:12.036
PFlock	70.0	160K	2582295	66.632	65.804	7	18:25:24.511
PFlock	80.0	160K	3165396	75.386	78.427	7	18:27:58.359
PFlock	90.0	160K	3800068	85.173	97.712	7	18:31:01.275
PFlock	100.0	160K	4498819	95.678	117.921	7	18:34:34.905
17/09/08 18:34:34 ERROR TransportResponseHandler: Still have 2 requests outstanding when connection from /169.235.27.135:37772 is closed
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@5a1e6339 rejected from java.util.concurrent.ThreadPoolExecutor@3a7ecb9a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:75)
	at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:110)
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:128)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:109)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:376)
	at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:680)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:465)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
java.util.concurrent.RejectedExecutionException: Task scala.concurrent.impl.CallbackRunnable@43b6f88 rejected from java.util.concurrent.ThreadPoolExecutor@3a7ecb9a[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 1946]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(ThreadPoolExecutor.java:2047)
	at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:823)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1369)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.Future$$anonfun$recover$1.apply(Future.scala:326)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at org.spark_project.guava.util.concurrent.MoreExecutors$SameThreadExecutorService.execute(MoreExecutors.java:293)
	at scala.concurrent.impl.ExecutionContextImpl$$anon$1.execute(ExecutionContextImpl.scala:136)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.complete(Promise.scala:55)
	at scala.concurrent.impl.Promise$DefaultPromise.complete(Promise.scala:153)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.Future$$anonfun$map$1.apply(Future.scala:237)
	at scala.concurrent.impl.CallbackRunnable.run(Promise.scala:32)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.processBatch$1(BatchingExecutor.scala:63)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply$mcV$sp(BatchingExecutor.scala:78)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BatchingExecutor$Batch$$anonfun$run$1.apply(BatchingExecutor.scala:55)
	at scala.concurrent.BlockContext$.withBlockContext(BlockContext.scala:72)
	at scala.concurrent.BatchingExecutor$Batch.run(BatchingExecutor.scala:54)
	at scala.concurrent.Future$InternalCallbackExecutor$.unbatchedExecute(Future.scala:601)
	at scala.concurrent.BatchingExecutor$class.execute(BatchingExecutor.scala:106)
	at scala.concurrent.Future$InternalCallbackExecutor$.execute(Future.scala:599)
	at scala.concurrent.impl.CallbackRunnable.executeWithValue(Promise.scala:40)
	at scala.concurrent.impl.Promise$DefaultPromise.tryComplete(Promise.scala:248)
	at scala.concurrent.Promise$class.tryFailure(Promise.scala:112)
	at scala.concurrent.impl.Promise$DefaultPromise.tryFailure(Promise.scala:153)
	at org.apache.spark.rpc.netty.NettyRpcEnv.org$apache$spark$rpc$netty$NettyRpcEnv$$onFailure$1(NettyRpcEnv.scala:205)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.NettyRpcEnv$$anonfun$2.apply(NettyRpcEnv.scala:228)
	at org.apache.spark.rpc.netty.RpcOutboxMessage.onFailure(Outbox.scala:75)
	at org.apache.spark.network.client.TransportResponseHandler.failOutstandingRequests(TransportResponseHandler.java:110)
	at org.apache.spark.network.client.TransportResponseHandler.channelInactive(TransportResponseHandler.java:128)
	at org.apache.spark.network.server.TransportChannelHandler.channelInactive(TransportChannelHandler.java:109)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.handler.timeout.IdleStateHandler.channelInactive(IdleStateHandler.java:257)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.ChannelInboundHandlerAdapter.channelInactive(ChannelInboundHandlerAdapter.java:75)
	at org.apache.spark.network.util.TransportFrameDecoder.channelInactive(TransportFrameDecoder.java:182)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelInactive(AbstractChannelHandlerContext.java:230)
	at io.netty.channel.DefaultChannelPipeline$HeadContext.channelInactive(DefaultChannelPipeline.java:1289)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:251)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelInactive(AbstractChannelHandlerContext.java:237)
	at io.netty.channel.DefaultChannelPipeline.fireChannelInactive(DefaultChannelPipeline.java:893)
	at io.netty.channel.AbstractChannel$AbstractUnsafe$7.run(AbstractChannel.java:691)
	at io.netty.util.concurrent.SingleThreadEventExecutor.runAllTasks(SingleThreadEventExecutor.java:376)
	at io.netty.util.concurrent.SingleThreadEventExecutor.confirmShutdown(SingleThreadEventExecutor.java:680)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:465)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:140)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:144)
	at java.lang.Thread.run(Thread.java:745)
Done!!!
Running in 7 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 18:34:37 INFO SparkContext: Running Spark version 2.1.0
17/09/08 18:34:37 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 18:34:37 INFO SecurityManager: Changing view acls to: acald013
17/09/08 18:34:37 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 18:34:37 INFO SecurityManager: Changing view acls groups to: 
17/09/08 18:34:37 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 18:34:37 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 18:34:37 INFO Utils: Successfully started service 'sparkDriver' on port 42227.
17/09/08 18:34:37 INFO SparkEnv: Registering MapOutputTracker
17/09/08 18:34:37 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 18:34:37 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 18:34:37 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 18:34:37 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e0d5b07d-74bc-429f-94cb-6e019a094da9
17/09/08 18:34:37 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 18:34:37 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 18:34:38 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 18:34:38 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 18:34:38 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:42227/jars/pflock_2.11-1.0.jar with timestamp 1504920878164
17/09/08 18:34:38 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:42227/files/metrics.properties with timestamp 1504920878309
17/09/08 18:34:38 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-a5b06627-fe42-4ac8-91a5-33a87eaa5837/userFiles-6c6a695c-fd8c-4a21-baa1-854e6b6d228a/metrics.properties
17/09/08 18:34:38 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 18:34:38 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 20 ms (0 ms spent in bootstraps)
17/09/08 18:34:38 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908183438-0022
17/09/08 18:34:38 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908183438-0022/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 7 cores
17/09/08 18:34:38 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908183438-0022/0 on hostPort 169.235.27.135:41135 with 7 cores, 12.0 GB RAM
17/09/08 18:34:38 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 33640.
17/09/08 18:34:38 INFO NettyBlockTransferService: Server created on 169.235.27.134:33640
17/09/08 18:34:38 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 18:34:38 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 33640, None)
17/09/08 18:34:38 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:33640 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 33640, None)
17/09/08 18:34:38 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 33640, None)
17/09/08 18:34:38 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 33640, None)
17/09/08 18:34:38 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908183438-0022
17/09/08 18:34:38 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 18:34:38 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908183438-0022 on 7 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	45.955	13.348	7	18:35:38.543
PFlock	20.0	160K	370689	33.829	18.068	7	18:36:30.518
PFlock	30.0	160K	703992	38.656	24.49	7	18:37:33.717
PFlock	40.0	160K	1106670	44.925	33.453	7	18:38:52.135
PFlock	50.0	160K	1554128	50.974	43.697	7	18:40:26.844
PFlock	60.0	160K	2045910	58.66	54.391	7	18:42:19.928
PFlock	70.0	160K	2582295	67.372	66.747	7	18:44:34.079
PFlock	80.0	160K	3165396	75.055	82.133	7	18:47:11.303
PFlock	90.0	160K	3800068	83.957	100.25	7	18:50:15.539
PFlock	100.0	160K	4498819	97.361	118.024	7	18:53:50.952
Done!!!
Running in 7 cores...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/08 18:53:53 INFO SparkContext: Running Spark version 2.1.0
17/09/08 18:53:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/08 18:53:53 INFO SecurityManager: Changing view acls to: acald013
17/09/08 18:53:53 INFO SecurityManager: Changing modify acls to: acald013
17/09/08 18:53:53 INFO SecurityManager: Changing view acls groups to: 
17/09/08 18:53:53 INFO SecurityManager: Changing modify acls groups to: 
17/09/08 18:53:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/08 18:53:53 INFO Utils: Successfully started service 'sparkDriver' on port 46464.
17/09/08 18:53:53 INFO SparkEnv: Registering MapOutputTracker
17/09/08 18:53:53 INFO SparkEnv: Registering BlockManagerMaster
17/09/08 18:53:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/08 18:53:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/08 18:53:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-1b4185a4-6d50-43f2-8fb2-152d01d387c8
17/09/08 18:53:53 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/08 18:53:54 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/08 18:53:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/08 18:53:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.134:4040
17/09/08 18:53:54 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.134:46464/jars/pflock_2.11-1.0.jar with timestamp 1504922034226
17/09/08 18:53:54 INFO SparkContext: Added file file:/home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties at spark://169.235.27.134:46464/files/metrics.properties with timestamp 1504922034373
17/09/08 18:53:54 INFO Utils: Copying /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/conf/metrics.properties to /tmp/spark-66516c20-3890-4ac0-870f-48268c2e491d/userFiles-4caad28d-48a0-40c5-9c90-8ee3104b93d2/metrics.properties
17/09/08 18:53:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.134:7077...
17/09/08 18:53:54 INFO TransportClientFactory: Successfully created connection to /169.235.27.134:7077 after 27 ms (0 ms spent in bootstraps)
17/09/08 18:53:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170908185354-0023
17/09/08 18:53:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170908185354-0023/0 on worker-20170908115448-169.235.27.135-41135 (169.235.27.135:41135) with 7 cores
17/09/08 18:53:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20170908185354-0023/0 on hostPort 169.235.27.135:41135 with 7 cores, 12.0 GB RAM
17/09/08 18:53:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43257.
17/09/08 18:53:54 INFO NettyBlockTransferService: Server created on 169.235.27.134:43257
17/09/08 18:53:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/08 18:53:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.134, 43257, None)
17/09/08 18:53:54 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.134:43257 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.134, 43257, None)
17/09/08 18:53:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.134, 43257, None)
17/09/08 18:53:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.134, 43257, None)
17/09/08 18:53:54 INFO EventLoggingListener: Logging events to file:///home/acald013/Spark/Logs/app-20170908185354-0023
17/09/08 18:53:54 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/08 18:53:54 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/spark-warehouse/'.
Running app-20170908185354-0023 on 7 cores...
Tag  	Epsilon	Dataset	N	TimeD	TimeM	Cores	Timestamp
PFlock	10.0	160K	117196	41.601	12.897	7	18:54:49.767
PFlock	20.0	160K	370689	32.263	17.682	7	18:55:39.786
PFlock	30.0	160K	703992	37.936	24.387	7	18:56:42.155
PFlock	40.0	160K	1106670	43.3	32.978	7	18:57:58.478
PFlock	50.0	160K	1554128	48.775	42.404	7	18:59:29.703
PFlock	60.0	160K	2045910	56.5	51.605	7	19:01:17.849
PFlock	70.0	160K	2582295	65.166	64.11	7	19:03:27.157
PFlock	80.0	160K	3165396	71.884	79.79	7	19:05:58.865
PFlock	90.0	160K	3800068	80.925	96.166	7	19:08:55.989
PFlock	100.0	160K	4498819	92.695	115.508	7	19:12:24.222
Done!!!
