acald013@dblab-rack15: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running in 7 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:02:38 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:02:38 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:02:38 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:02:38 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:02:38 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:02:38 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:02:38 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:02:39 INFO Utils: Successfully started service 'sparkDriver' on port 32787.
17/09/20 18:02:39 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:02:39 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:02:39 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:02:39 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:02:39 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-aa09c547-7ab0-49b5-88b0-68fb85780689
17/09/20 18:02:39 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:02:39 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:02:39 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:02:39 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:02:39 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:32787/jars/pflock_2.11-1.0.jar with timestamp 1505955759921
17/09/20 18:02:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:02:40 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 47 ms (0 ms spent in bootstraps)
17/09/20 18:02:40 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920180240-0000
17/09/20 18:02:40 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45722.
17/09/20 18:02:40 INFO NettyBlockTransferService: Server created on 169.235.27.138:45722
17/09/20 18:02:40 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:02:40 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 45722, None)
17/09/20 18:02:40 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:45722 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 45722, None)
17/09/20 18:02:40 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 45722, None)
17/09/20 18:02:40 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 45722, None)
17/09/20 18:02:40 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920180240-0000/0 on worker-20170920180234-169.235.27.134-46720 (169.235.27.134:46720) with 7 cores
17/09/20 18:02:40 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920180240-0000/0 on hostPort 169.235.27.134:46720 with 7 cores, 12.0 GB RAM
17/09/20 18:02:40 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920180240-0000/0 is now RUNNING
17/09/20 18:02:41 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920180240-0000
17/09/20 18:02:41 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:02:41 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920180240-0000 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/20 18:03:04 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
Running in 7 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:04:02 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:04:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:04:03 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:04:03 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:04:03 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:04:03 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:04:03 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:04:03 INFO Utils: Successfully started service 'sparkDriver' on port 44001.
17/09/20 18:04:03 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:04:03 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:04:03 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:04:03 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:04:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-54b950bf-cec0-45f6-8b4a-0b8e8c840bb5
17/09/20 18:04:03 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:04:03 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:04:04 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:04:04 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:04:04 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:44001/jars/pflock_2.11-1.0.jar with timestamp 1505955844322
17/09/20 18:04:04 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:04:04 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/20 18:04:04 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920180404-0001
17/09/20 18:04:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920180404-0001/0 on worker-20170920180234-169.235.27.134-46720 (169.235.27.134:46720) with 7 cores
17/09/20 18:04:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920180404-0001/0 on hostPort 169.235.27.134:46720 with 7 cores, 12.0 GB RAM
17/09/20 18:04:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42550.
17/09/20 18:04:04 INFO NettyBlockTransferService: Server created on 169.235.27.138:42550
17/09/20 18:04:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:04:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 42550, None)
17/09/20 18:04:04 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:42550 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 42550, None)
17/09/20 18:04:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 42550, None)
17/09/20 18:04:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 42550, None)
17/09/20 18:04:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920180404-0001/0 is now RUNNING
17/09/20 18:04:05 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920180404-0001
17/09/20 18:04:05 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:04:05 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920180404-0001 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
Running in 7 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:05:26 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:05:26 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:05:26 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:05:26 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:05:26 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:05:26 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:05:26 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:05:27 INFO Utils: Successfully started service 'sparkDriver' on port 36866.
17/09/20 18:05:27 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:05:27 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:05:27 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:05:27 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:05:27 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-f1eee96e-6481-4a2f-8bbb-3988d5f9c0fe
17/09/20 18:05:27 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:05:27 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:05:28 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:05:28 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:05:28 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:36866/jars/pflock_2.11-1.0.jar with timestamp 1505955928066
17/09/20 18:05:28 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:05:28 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/20 18:05:28 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920180528-0002
17/09/20 18:05:28 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920180528-0002/0 on worker-20170920180234-169.235.27.134-46720 (169.235.27.134:46720) with 7 cores
17/09/20 18:05:28 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920180528-0002/0 on hostPort 169.235.27.134:46720 with 7 cores, 12.0 GB RAM
17/09/20 18:05:28 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44747.
17/09/20 18:05:28 INFO NettyBlockTransferService: Server created on 169.235.27.138:44747
17/09/20 18:05:28 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:05:28 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 44747, None)
17/09/20 18:05:28 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920180528-0002/0 is now RUNNING
17/09/20 18:05:28 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:44747 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 44747, None)
17/09/20 18:05:28 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 44747, None)
17/09/20 18:05:28 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 44747, None)
17/09/20 18:05:29 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920180528-0002
17/09/20 18:05:29 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:05:29 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920180528-0002 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running in 14 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:06:55 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:06:55 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:06:55 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:06:55 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:06:55 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:06:55 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:06:55 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:06:56 INFO Utils: Successfully started service 'sparkDriver' on port 42985.
17/09/20 18:06:56 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:06:56 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:06:56 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:06:56 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:06:56 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-b31bad92-f5f9-48c0-97b5-f7add624f80d
17/09/20 18:06:56 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:06:56 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:06:56 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:06:56 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:06:57 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:42985/jars/pflock_2.11-1.0.jar with timestamp 1505956017003
17/09/20 18:06:57 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:06:57 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/20 18:06:57 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920180657-0000
17/09/20 18:06:57 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35375.
17/09/20 18:06:57 INFO NettyBlockTransferService: Server created on 169.235.27.138:35375
17/09/20 18:06:57 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:06:57 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 35375, None)
17/09/20 18:06:57 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:35375 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 35375, None)
17/09/20 18:06:57 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 35375, None)
17/09/20 18:06:57 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 35375, None)
17/09/20 18:06:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920180657-0000/0 on worker-20170920180649-169.235.27.134-42567 (169.235.27.134:42567) with 7 cores
17/09/20 18:06:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920180657-0000/0 on hostPort 169.235.27.134:42567 with 7 cores, 12.0 GB RAM
17/09/20 18:06:57 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920180657-0000/1 on worker-20170920180649-169.235.27.135-40192 (169.235.27.135:40192) with 7 cores
17/09/20 18:06:57 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920180657-0000/1 on hostPort 169.235.27.135:40192 with 7 cores, 12.0 GB RAM
17/09/20 18:06:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920180657-0000/1 is now RUNNING
17/09/20 18:06:57 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920180657-0000/0 is now RUNNING
17/09/20 18:06:58 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920180657-0000
17/09/20 18:06:58 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:06:58 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920180657-0000 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/20 18:07:17 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     51.786     23.862     75.648     144582         11         14       1024    18:08:15.140
    PFlock       60.0        40K     37.309     20.055     57.364     175982         38         14       1024    18:09:12.658
    PFlock       70.0        40K     37.999     21.829     59.828     209268         95         14       1024    18:10:12.577
    PFlock       80.0        40K     39.972     24.265     64.237     244670        216         14       1024    18:11:16.900
    PFlock       90.0        40K     42.651     27.893     70.544     283164        426         14       1024    18:12:27.528
    PFlock      100.0        40K     44.385     30.280     74.665     325850        610         14       1024    18:13:42.282
Done!!!
Running in 14 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:13:48 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:13:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:13:48 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:13:48 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:13:48 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:13:48 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:13:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:13:49 INFO Utils: Successfully started service 'sparkDriver' on port 42827.
17/09/20 18:13:49 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:13:49 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:13:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:13:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:13:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-eb990269-6e42-4de4-b4c6-542c6c23a740
17/09/20 18:13:49 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:13:49 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:13:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:13:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:13:50 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:42827/jars/pflock_2.11-1.0.jar with timestamp 1505956430132
17/09/20 18:13:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:13:50 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/20 18:13:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920181350-0001
17/09/20 18:13:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920181350-0001/0 on worker-20170920180649-169.235.27.134-42567 (169.235.27.134:42567) with 7 cores
17/09/20 18:13:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920181350-0001/0 on hostPort 169.235.27.134:42567 with 7 cores, 12.0 GB RAM
17/09/20 18:13:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920181350-0001/1 on worker-20170920180649-169.235.27.135-40192 (169.235.27.135:40192) with 7 cores
17/09/20 18:13:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920181350-0001/1 on hostPort 169.235.27.135:40192 with 7 cores, 12.0 GB RAM
17/09/20 18:13:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 41115.
17/09/20 18:13:50 INFO NettyBlockTransferService: Server created on 169.235.27.138:41115
17/09/20 18:13:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:13:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 41115, None)
17/09/20 18:13:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920181350-0001/1 is now RUNNING
17/09/20 18:13:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920181350-0001/0 is now RUNNING
17/09/20 18:13:50 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:41115 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 41115, None)
17/09/20 18:13:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 41115, None)
17/09/20 18:13:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 41115, None)
17/09/20 18:13:51 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920181350-0001
17/09/20 18:13:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:13:51 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920181350-0001 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/20 18:14:11 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     53.503     22.679     76.182     144582         11         14       1024    18:15:08.841
    PFlock       60.0        40K     37.215     19.652     56.867     175982         38         14       1024    18:16:05.861
    PFlock       70.0        40K     38.759     22.620     61.379     209268         95         14       1024    18:17:07.329
    PFlock       80.0        40K     40.975     24.528     65.503     244670        216         14       1024    18:18:12.915
    PFlock       90.0        40K     43.035     27.290     70.325     283164        426         14       1024    18:19:23.325
    PFlock      100.0        40K     43.595     30.921     74.516     325850        610         14       1024    18:20:37.927
Done!!!
Running in 14 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:20:43 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:20:44 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:20:44 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:20:44 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:20:44 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:20:44 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:20:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:20:44 INFO Utils: Successfully started service 'sparkDriver' on port 34352.
17/09/20 18:20:44 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:20:44 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:20:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:20:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:20:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8d7843a6-e46d-4690-9a1f-60b7b7f0c29b
17/09/20 18:20:44 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:20:44 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:20:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:20:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:20:45 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:34352/jars/pflock_2.11-1.0.jar with timestamp 1505956845407
17/09/20 18:20:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:20:45 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/20 18:20:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920182045-0002
17/09/20 18:20:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920182045-0002/0 on worker-20170920180649-169.235.27.134-42567 (169.235.27.134:42567) with 7 cores
17/09/20 18:20:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920182045-0002/0 on hostPort 169.235.27.134:42567 with 7 cores, 12.0 GB RAM
17/09/20 18:20:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920182045-0002/1 on worker-20170920180649-169.235.27.135-40192 (169.235.27.135:40192) with 7 cores
17/09/20 18:20:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920182045-0002/1 on hostPort 169.235.27.135:40192 with 7 cores, 12.0 GB RAM
17/09/20 18:20:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45553.
17/09/20 18:20:45 INFO NettyBlockTransferService: Server created on 169.235.27.138:45553
17/09/20 18:20:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:20:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 45553, None)
17/09/20 18:20:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920182045-0002/1 is now RUNNING
17/09/20 18:20:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920182045-0002/0 is now RUNNING
17/09/20 18:20:45 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:45553 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 45553, None)
17/09/20 18:20:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 45553, None)
17/09/20 18:20:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 45553, None)
17/09/20 18:20:46 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920182045-0002
17/09/20 18:20:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:20:46 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920182045-0002 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/20 18:21:07 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     51.756     20.400     72.156     144582         11         14       1024    18:22:00.002
    PFlock       60.0        40K     37.115     21.099     58.214     175982         38         14       1024    18:22:58.370
    PFlock       70.0        40K     38.888     21.946     60.834     209268         95         14       1024    18:23:59.297
    PFlock       80.0        40K     39.942     23.337     63.279     244670        216         14       1024    18:25:02.657
    PFlock       90.0        40K     42.807     28.550     71.357     283164        426         14       1024    18:26:14.095
    PFlock      100.0        40K     43.704     30.035     73.739     325850        610         14       1024    18:27:27.925
Done!!!
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running in 21 cores and 1024 partitions.  Setting mu = 40 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/20 18:27:38 INFO SparkContext: Running Spark version 2.1.0
17/09/20 18:27:39 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/20 18:27:39 INFO SecurityManager: Changing view acls to: acald013
17/09/20 18:27:39 INFO SecurityManager: Changing modify acls to: acald013
17/09/20 18:27:39 INFO SecurityManager: Changing view acls groups to: 
17/09/20 18:27:39 INFO SecurityManager: Changing modify acls groups to: 
17/09/20 18:27:39 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/20 18:27:40 INFO Utils: Successfully started service 'sparkDriver' on port 42138.
17/09/20 18:27:40 INFO SparkEnv: Registering MapOutputTracker
17/09/20 18:27:40 INFO SparkEnv: Registering BlockManagerMaster
17/09/20 18:27:40 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/20 18:27:40 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/20 18:27:40 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-60684705-73ce-429a-936b-345669bc4fb6
17/09/20 18:27:40 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/20 18:27:40 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/20 18:27:40 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/20 18:27:40 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/20 18:27:40 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:42138/jars/pflock_2.11-1.0.jar with timestamp 1505957260826
17/09/20 18:27:40 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/20 18:27:41 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 48 ms (0 ms spent in bootstraps)
17/09/20 18:27:41 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170920182741-0000
17/09/20 18:27:41 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37157.
17/09/20 18:27:41 INFO NettyBlockTransferService: Server created on 169.235.27.138:37157
17/09/20 18:27:41 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/20 18:27:41 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 37157, None)
17/09/20 18:27:41 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:37157 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 37157, None)
17/09/20 18:27:41 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 37157, None)
17/09/20 18:27:41 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 37157, None)
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920182741-0000/0 on worker-20170920182733-169.235.27.134-32827 (169.235.27.134:32827) with 7 cores
17/09/20 18:27:41 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920182741-0000/0 on hostPort 169.235.27.134:32827 with 7 cores, 12.0 GB RAM
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920182741-0000/1 on worker-20170920182733-169.235.27.135-37957 (169.235.27.135:37957) with 7 cores
17/09/20 18:27:41 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920182741-0000/1 on hostPort 169.235.27.135:37957 with 7 cores, 12.0 GB RAM
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170920182741-0000/2 on worker-20170920182733-169.235.27.137-33060 (169.235.27.137:33060) with 7 cores
17/09/20 18:27:41 INFO StandaloneSchedulerBackend: Granted executor ID app-20170920182741-0000/2 on hostPort 169.235.27.137:33060 with 7 cores, 12.0 GB RAM
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920182741-0000/1 is now RUNNING
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920182741-0000/0 is now RUNNING
17/09/20 18:27:41 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170920182741-0000/2 is now RUNNING
17/09/20 18:27:42 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170920182741-0000
17/09/20 18:27:42 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/20 18:27:42 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170920182741-0000 on 21 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/20 18:28:01 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        60K     52.408     22.541     74.949     322132        214         21       1024    18:28:58.281
    PFlock       60.0        60K     35.483     23.627     59.110     392242        557         21       1024    18:29:57.546
