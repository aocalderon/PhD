acald013@dblab-rack11: no org.apache.spark.deploy.worker.Worker to stop
no org.apache.spark.deploy.master.Master to stop
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
Running in 7 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:21:43 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:21:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:21:43 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:21:43 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:21:43 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:21:43 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:21:43 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:21:44 INFO Utils: Successfully started service 'sparkDriver' on port 36475.
17/09/16 21:21:44 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:21:44 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:21:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:21:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:21:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6cf2cac5-6d03-4b28-aded-b7ee2d0ab73b
17/09/16 21:21:44 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:21:44 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:21:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:21:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:21:44 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:36475/jars/pflock_2.11-1.0.jar with timestamp 1505622104972
17/09/16 21:21:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:21:45 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 21:21:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916212145-0000
17/09/16 21:21:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 34507.
17/09/16 21:21:45 INFO NettyBlockTransferService: Server created on 169.235.27.138:34507
17/09/16 21:21:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:21:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 34507, None)
17/09/16 21:21:45 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:34507 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 34507, None)
17/09/16 21:21:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 34507, None)
17/09/16 21:21:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 34507, None)
17/09/16 21:21:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916212145-0000/0 on worker-20170916212138-169.235.27.138-35600 (169.235.27.138:35600) with 7 cores
17/09/16 21:21:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916212145-0000/0 on hostPort 169.235.27.138:35600 with 7 cores, 12.0 GB RAM
17/09/16 21:21:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916212145-0000/0 is now RUNNING
17/09/16 21:21:46 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916212145-0000
17/09/16 21:21:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:21:46 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse'.
Running app-20170916212145-0000 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
Running in 7 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:23:48 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:23:49 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:23:49 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:23:49 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:23:49 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:23:49 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:23:49 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:23:49 INFO Utils: Successfully started service 'sparkDriver' on port 39548.
17/09/16 21:23:49 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:23:49 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:23:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:23:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:23:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-6cfbe55d-2e9a-4233-af28-93b8a7835b78
17/09/16 21:23:49 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:23:49 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:23:50 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:23:50 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:23:50 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:39548/jars/pflock_2.11-1.0.jar with timestamp 1505622230392
17/09/16 21:23:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:23:50 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 50 ms (0 ms spent in bootstraps)
17/09/16 21:23:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916212350-0001
17/09/16 21:23:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916212350-0001/0 on worker-20170916212138-169.235.27.138-35600 (169.235.27.138:35600) with 7 cores
17/09/16 21:23:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916212350-0001/0 on hostPort 169.235.27.138:35600 with 7 cores, 12.0 GB RAM
17/09/16 21:23:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45234.
17/09/16 21:23:50 INFO NettyBlockTransferService: Server created on 169.235.27.138:45234
17/09/16 21:23:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:23:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 45234, None)
17/09/16 21:23:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916212350-0001/0 is now RUNNING
17/09/16 21:23:50 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:45234 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 45234, None)
17/09/16 21:23:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 45234, None)
17/09/16 21:23:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 45234, None)
17/09/16 21:23:51 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916212350-0001
17/09/16 21:23:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:23:51 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916212350-0001 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
Running in 7 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:26:01 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:26:02 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:26:02 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:26:02 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:26:02 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:26:02 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:26:02 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:26:02 INFO Utils: Successfully started service 'sparkDriver' on port 41015.
17/09/16 21:26:02 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:26:02 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:26:02 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:26:02 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:26:03 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-e2571820-df45-4450-86f6-c345f230ab23
17/09/16 21:26:03 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:26:03 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:26:03 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:26:03 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:26:03 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:41015/jars/pflock_2.11-1.0.jar with timestamp 1505622363620
17/09/16 21:26:03 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:26:03 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 48 ms (0 ms spent in bootstraps)
17/09/16 21:26:03 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916212603-0002
17/09/16 21:26:04 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916212603-0002/0 on worker-20170916212138-169.235.27.138-35600 (169.235.27.138:35600) with 7 cores
17/09/16 21:26:04 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916212603-0002/0 on hostPort 169.235.27.138:35600 with 7 cores, 12.0 GB RAM
17/09/16 21:26:04 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 43374.
17/09/16 21:26:04 INFO NettyBlockTransferService: Server created on 169.235.27.138:43374
17/09/16 21:26:04 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:26:04 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 43374, None)
17/09/16 21:26:04 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916212603-0002/0 is now RUNNING
17/09/16 21:26:04 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:43374 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 43374, None)
17/09/16 21:26:04 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 43374, None)
17/09/16 21:26:04 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 43374, None)
17/09/16 21:26:04 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916212603-0002
17/09/16 21:26:04 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:26:04 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916212603-0002 on 7 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
Exception in thread "main" java.lang.UnsupportedOperationException: empty collection
	at org.apache.spark.rdd.RDD$$anonfun$first$1.apply(RDD.scala:1369)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:151)
	at org.apache.spark.rdd.RDDOperationScope$.withScope(RDDOperationScope.scala:112)
	at org.apache.spark.rdd.RDD.withScope(RDD.scala:362)
	at org.apache.spark.rdd.RDD.first(RDD.scala:1366)
	at org.apache.spark.sql.simba.index.RTreeIndexedRelation.<init>(RTreeIndexedRelation.scala:55)
	at org.apache.spark.sql.simba.index.IndexedRelation$.apply(IndexedRelation.scala:41)
	at org.apache.spark.sql.simba.IndexManager$$anonfun$createIndexQuery$1.apply(IndexManager.scala:201)
	at org.apache.spark.sql.simba.IndexManager.writeLock(IndexManager.scala:64)
	at org.apache.spark.sql.simba.IndexManager.createIndexQuery(IndexManager.scala:192)
	at org.apache.spark.sql.simba.Dataset.index(DataSet.scala:179)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:98)
	at PFlock$$anonfun$main$1$$anonfun$apply$mcVI$sp$1.apply(PFlock.scala:53)
	at scala.collection.immutable.NumericRange.foreach(NumericRange.scala:73)
	at PFlock$$anonfun$main$1.apply$mcVI$sp(PFlock.scala:53)
	at scala.collection.immutable.Range.foreach$mVc$sp(Range.scala:160)
	at PFlock$.main(PFlock.scala:52)
	at PFlock.main(PFlock.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:498)
	at org.apache.spark.deploy.SparkSubmit$.org$apache$spark$deploy$SparkSubmit$$runMain(SparkSubmit.scala:738)
	at org.apache.spark.deploy.SparkSubmit$.doRunMain$1(SparkSubmit.scala:187)
	at org.apache.spark.deploy.SparkSubmit$.submit(SparkSubmit.scala:212)
	at org.apache.spark.deploy.SparkSubmit$.main(SparkSubmit.scala:126)
	at org.apache.spark.deploy.SparkSubmit.main(SparkSubmit.scala)
Done!!!
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running in 14 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:28:18 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:28:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:28:18 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:28:18 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:28:18 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:28:18 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:28:18 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:28:19 INFO Utils: Successfully started service 'sparkDriver' on port 44801.
17/09/16 21:28:19 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:28:19 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:28:19 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:28:19 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:28:19 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-10e1220d-3fff-4386-ad68-42ebde2e218f
17/09/16 21:28:19 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:28:19 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:28:19 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:28:19 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:28:20 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:44801/jars/pflock_2.11-1.0.jar with timestamp 1505622500044
17/09/16 21:28:20 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:28:20 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 21:28:20 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916212820-0000
17/09/16 21:28:20 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 44271.
17/09/16 21:28:20 INFO NettyBlockTransferService: Server created on 169.235.27.138:44271
17/09/16 21:28:20 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:28:20 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 44271, None)
17/09/16 21:28:20 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:44271 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 44271, None)
17/09/16 21:28:20 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 44271, None)
17/09/16 21:28:20 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 44271, None)
17/09/16 21:28:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916212820-0000/0 on worker-20170916212814-169.235.27.138-32945 (169.235.27.138:32945) with 7 cores
17/09/16 21:28:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916212820-0000/0 on hostPort 169.235.27.138:32945 with 7 cores, 12.0 GB RAM
17/09/16 21:28:20 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916212820-0000/1 on worker-20170916212812-169.235.27.137-38349 (169.235.27.137:38349) with 7 cores
17/09/16 21:28:20 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916212820-0000/1 on hostPort 169.235.27.137:38349 with 7 cores, 12.0 GB RAM
17/09/16 21:28:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916212820-0000/1 is now RUNNING
17/09/16 21:28:20 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916212820-0000/0 is now RUNNING
17/09/16 21:28:21 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916212820-0000
17/09/16 21:28:21 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:28:21 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916212820-0000 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 21:28:44 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     67.519     32.076     99.595     260494        456         14       1024    21:30:02.140
    PFlock       60.0        40K     52.603     33.236     85.839     299480        695         14       1024    21:31:28.138
    PFlock       70.0        40K     56.133     45.334    101.467     340518       1030         14       1024    21:33:09.696
    PFlock       80.0        40K     55.550     38.448     93.998     383160       1107         14       1024    21:34:43.791
    PFlock       90.0        40K     63.426     48.300    111.726     429422       1034         14       1024    21:36:35.609
    PFlock      100.0        40K     60.869     46.006    106.875     481096       1044         14       1024    21:38:22.574
Done!!!
Running in 14 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:38:27 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:38:27 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:38:28 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:38:28 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:38:28 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:38:28 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:38:28 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:38:28 INFO Utils: Successfully started service 'sparkDriver' on port 34860.
17/09/16 21:38:28 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:38:28 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:38:28 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:38:28 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:38:28 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-27580315-f4cf-4995-acdd-e4d9baa90502
17/09/16 21:38:28 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:38:28 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:38:29 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:38:29 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:38:29 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:34860/jars/pflock_2.11-1.0.jar with timestamp 1505623109322
17/09/16 21:38:29 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:38:29 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 51 ms (0 ms spent in bootstraps)
17/09/16 21:38:29 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916213829-0001
17/09/16 21:38:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916213829-0001/0 on worker-20170916212814-169.235.27.138-32945 (169.235.27.138:32945) with 7 cores
17/09/16 21:38:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916213829-0001/0 on hostPort 169.235.27.138:32945 with 7 cores, 12.0 GB RAM
17/09/16 21:38:29 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916213829-0001/1 on worker-20170916212812-169.235.27.137-38349 (169.235.27.137:38349) with 7 cores
17/09/16 21:38:29 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916213829-0001/1 on hostPort 169.235.27.137:38349 with 7 cores, 12.0 GB RAM
17/09/16 21:38:29 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45960.
17/09/16 21:38:29 INFO NettyBlockTransferService: Server created on 169.235.27.138:45960
17/09/16 21:38:29 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:38:29 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 45960, None)
17/09/16 21:38:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916213829-0001/0 is now RUNNING
17/09/16 21:38:29 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:45960 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 45960, None)
17/09/16 21:38:29 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916213829-0001/1 is now RUNNING
17/09/16 21:38:29 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 45960, None)
17/09/16 21:38:29 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 45960, None)
17/09/16 21:38:30 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916213829-0001
17/09/16 21:38:30 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:38:30 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916213829-0001 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 21:38:55 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     66.694     31.125     97.819     260494        456         14       1024    21:40:09.592
    PFlock       60.0        40K     51.905     32.596     84.501     299480        695         14       1024    21:41:34.252
    PFlock       70.0        40K     55.026     35.616     90.642     340518       1030         14       1024    21:43:04.992
    PFlock       80.0        40K     56.306     37.936     94.242     383160       1107         14       1024    21:44:39.331
    PFlock       90.0        40K     59.131     49.208    108.339     429422       1034         14       1024    21:46:27.761
    PFlock      100.0        40K     61.121     49.650    110.771     481096       1044         14       1024    21:48:18.627
Done!!!
Running in 14 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:48:23 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:48:23 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:48:24 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:48:24 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:48:24 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:48:24 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:48:24 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:48:24 INFO Utils: Successfully started service 'sparkDriver' on port 43090.
17/09/16 21:48:24 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:48:24 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:48:24 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:48:24 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:48:24 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-9b482431-766a-41e9-860b-540542d8714f
17/09/16 21:48:24 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:48:25 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:48:25 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:48:25 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:48:25 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:43090/jars/pflock_2.11-1.0.jar with timestamp 1505623705529
17/09/16 21:48:25 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:48:25 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 47 ms (0 ms spent in bootstraps)
17/09/16 21:48:25 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916214825-0002
17/09/16 21:48:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916214825-0002/0 on worker-20170916212814-169.235.27.138-32945 (169.235.27.138:32945) with 7 cores
17/09/16 21:48:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916214825-0002/0 on hostPort 169.235.27.138:32945 with 7 cores, 12.0 GB RAM
17/09/16 21:48:25 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916214825-0002/1 on worker-20170916212812-169.235.27.137-38349 (169.235.27.137:38349) with 7 cores
17/09/16 21:48:25 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 36787.
17/09/16 21:48:25 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916214825-0002/1 on hostPort 169.235.27.137:38349 with 7 cores, 12.0 GB RAM
17/09/16 21:48:25 INFO NettyBlockTransferService: Server created on 169.235.27.138:36787
17/09/16 21:48:25 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:48:25 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 36787, None)
17/09/16 21:48:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916214825-0002/0 is now RUNNING
17/09/16 21:48:25 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916214825-0002/1 is now RUNNING
17/09/16 21:48:25 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:36787 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 36787, None)
17/09/16 21:48:25 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 36787, None)
17/09/16 21:48:25 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 36787, None)
17/09/16 21:48:26 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916214825-0002
17/09/16 21:48:26 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:48:26 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916214825-0002 on 14 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 21:48:51 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        40K     65.181     31.482     96.663     260494        456         14       1024    21:50:04.629
    PFlock       60.0        40K     52.396     39.006     91.402     299480        695         14       1024    21:51:36.203
    PFlock       70.0        40K     53.069     35.556     88.625     340518       1030         14       1024    21:53:04.922
    PFlock       80.0        40K     55.703     36.871     92.574     383160       1107         14       1024    21:54:37.589
    PFlock       90.0        40K     57.351     37.032     94.383     429422       1034         14       1024    21:56:12.063
    PFlock      100.0        40K     62.874     40.530    103.404     481096       1044         14       1024    21:57:55.559
Done!!!
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
Running in 21 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 21:58:05 INFO SparkContext: Running Spark version 2.1.0
17/09/16 21:58:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 21:58:06 INFO SecurityManager: Changing view acls to: acald013
17/09/16 21:58:06 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 21:58:06 INFO SecurityManager: Changing view acls groups to: 
17/09/16 21:58:06 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 21:58:06 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 21:58:07 INFO Utils: Successfully started service 'sparkDriver' on port 33538.
17/09/16 21:58:07 INFO SparkEnv: Registering MapOutputTracker
17/09/16 21:58:07 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 21:58:07 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 21:58:07 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 21:58:07 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d812a9c9-88a8-4035-9ef3-074dc857c535
17/09/16 21:58:07 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 21:58:07 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 21:58:07 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 21:58:07 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 21:58:07 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:33538/jars/pflock_2.11-1.0.jar with timestamp 1505624287750
17/09/16 21:58:07 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 21:58:07 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 21:58:08 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916215808-0000
17/09/16 21:58:08 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42120.
17/09/16 21:58:08 INFO NettyBlockTransferService: Server created on 169.235.27.138:42120
17/09/16 21:58:08 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 21:58:08 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 42120, None)
17/09/16 21:58:08 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:42120 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 42120, None)
17/09/16 21:58:08 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 42120, None)
17/09/16 21:58:08 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 42120, None)
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916215808-0000/0 on worker-20170916215801-169.235.27.138-46006 (169.235.27.138:46006) with 7 cores
17/09/16 21:58:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916215808-0000/0 on hostPort 169.235.27.138:46006 with 7 cores, 12.0 GB RAM
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916215808-0000/1 on worker-20170916215800-169.235.27.135-37933 (169.235.27.135:37933) with 7 cores
17/09/16 21:58:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916215808-0000/1 on hostPort 169.235.27.135:37933 with 7 cores, 12.0 GB RAM
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916215808-0000/2 on worker-20170916215800-169.235.27.137-43799 (169.235.27.137:43799) with 7 cores
17/09/16 21:58:08 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916215808-0000/2 on hostPort 169.235.27.137:43799 with 7 cores, 12.0 GB RAM
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916215808-0000/1 is now RUNNING
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916215808-0000/2 is now RUNNING
17/09/16 21:58:08 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916215808-0000/0 is now RUNNING
17/09/16 21:58:09 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916215808-0000
17/09/16 21:58:09 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 21:58:09 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916215808-0000 on 21 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 21:58:33 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        60K     62.345     26.896     89.241     418042        492         21       1024    21:59:39.538
    PFlock       60.0        60K     44.291     30.179     74.470     497100        756         21       1024    22:00:54.172
    PFlock       70.0        60K     48.452     28.899     77.351     580400        923         21       1024    22:02:11.621
    PFlock       80.0        60K     51.437     32.449     83.886     668720        984         21       1024    22:03:35.601
    PFlock       90.0        60K     54.914     34.641     89.555     765034       1164         21       1024    22:05:05.245
    PFlock      100.0        60K     59.183     42.227    101.410     871886       1353         21       1024    22:06:46.749
Done!!!
Running in 21 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 22:06:52 INFO SparkContext: Running Spark version 2.1.0
17/09/16 22:06:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 22:06:53 INFO SecurityManager: Changing view acls to: acald013
17/09/16 22:06:53 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 22:06:53 INFO SecurityManager: Changing view acls groups to: 
17/09/16 22:06:53 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 22:06:53 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 22:06:53 INFO Utils: Successfully started service 'sparkDriver' on port 35547.
17/09/16 22:06:53 INFO SparkEnv: Registering MapOutputTracker
17/09/16 22:06:53 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 22:06:53 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 22:06:53 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 22:06:53 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-d184ba01-941c-4820-a8d0-ff197151cf16
17/09/16 22:06:53 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 22:06:53 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 22:06:54 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 22:06:54 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 22:06:54 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:35547/jars/pflock_2.11-1.0.jar with timestamp 1505624814476
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 22:06:54 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 22:06:54 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916220654-0001
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916220654-0001/0 on worker-20170916215801-169.235.27.138-46006 (169.235.27.138:46006) with 7 cores
17/09/16 22:06:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916220654-0001/0 on hostPort 169.235.27.138:46006 with 7 cores, 12.0 GB RAM
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916220654-0001/1 on worker-20170916215800-169.235.27.135-37933 (169.235.27.135:37933) with 7 cores
17/09/16 22:06:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916220654-0001/1 on hostPort 169.235.27.135:37933 with 7 cores, 12.0 GB RAM
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916220654-0001/2 on worker-20170916215800-169.235.27.137-43799 (169.235.27.137:43799) with 7 cores
17/09/16 22:06:54 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916220654-0001/2 on hostPort 169.235.27.137:43799 with 7 cores, 12.0 GB RAM
17/09/16 22:06:54 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 45684.
17/09/16 22:06:54 INFO NettyBlockTransferService: Server created on 169.235.27.138:45684
17/09/16 22:06:54 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 22:06:54 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 45684, None)
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916220654-0001/1 is now RUNNING
17/09/16 22:06:54 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:45684 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 45684, None)
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916220654-0001/0 is now RUNNING
17/09/16 22:06:54 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916220654-0001/2 is now RUNNING
17/09/16 22:06:54 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 45684, None)
17/09/16 22:06:54 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 45684, None)
17/09/16 22:06:55 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916220654-0001
17/09/16 22:06:55 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 22:06:55 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916220654-0001 on 21 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 22:07:20 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        60K     62.469     28.027     90.496     418042        492         21       1024    22:08:27.490
    PFlock       60.0        60K     44.105     31.921     76.026     497100        756         21       1024    22:09:43.676
    PFlock       70.0        60K     50.598     30.370     80.968     580400        923         21       1024    22:11:04.737
    PFlock       80.0        60K     47.809     35.324     83.133     668720        984         21       1024    22:12:27.958
    PFlock       90.0        60K     55.850     36.516     92.366     765034       1164         21       1024    22:14:00.413
    PFlock      100.0        60K     60.066     41.468    101.534     871886       1353         21       1024    22:15:42.036
Done!!!
Running in 21 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 22:15:48 INFO SparkContext: Running Spark version 2.1.0
17/09/16 22:15:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 22:15:48 INFO SecurityManager: Changing view acls to: acald013
17/09/16 22:15:48 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 22:15:48 INFO SecurityManager: Changing view acls groups to: 
17/09/16 22:15:48 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 22:15:48 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 22:15:49 INFO Utils: Successfully started service 'sparkDriver' on port 43077.
17/09/16 22:15:49 INFO SparkEnv: Registering MapOutputTracker
17/09/16 22:15:49 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 22:15:49 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 22:15:49 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 22:15:49 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-c3d797c5-862d-4264-8390-9c445ca057c4
17/09/16 22:15:49 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 22:15:49 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 22:15:49 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 22:15:49 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 22:15:49 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:43077/jars/pflock_2.11-1.0.jar with timestamp 1505625349992
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 22:15:50 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 46 ms (0 ms spent in bootstraps)
17/09/16 22:15:50 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916221550-0002
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916221550-0002/0 on worker-20170916215801-169.235.27.138-46006 (169.235.27.138:46006) with 7 cores
17/09/16 22:15:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916221550-0002/0 on hostPort 169.235.27.138:46006 with 7 cores, 12.0 GB RAM
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916221550-0002/1 on worker-20170916215800-169.235.27.135-37933 (169.235.27.135:37933) with 7 cores
17/09/16 22:15:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916221550-0002/1 on hostPort 169.235.27.135:37933 with 7 cores, 12.0 GB RAM
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916221550-0002/2 on worker-20170916215800-169.235.27.137-43799 (169.235.27.137:43799) with 7 cores
17/09/16 22:15:50 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916221550-0002/2 on hostPort 169.235.27.137:43799 with 7 cores, 12.0 GB RAM
17/09/16 22:15:50 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 40112.
17/09/16 22:15:50 INFO NettyBlockTransferService: Server created on 169.235.27.138:40112
17/09/16 22:15:50 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 22:15:50 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 40112, None)
17/09/16 22:15:50 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:40112 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 40112, None)
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916221550-0002/1 is now RUNNING
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916221550-0002/0 is now RUNNING
17/09/16 22:15:50 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916221550-0002/2 is now RUNNING
17/09/16 22:15:50 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 40112, None)
17/09/16 22:15:50 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 40112, None)
17/09/16 22:15:51 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916221550-0002
17/09/16 22:15:51 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 22:15:51 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916221550-0002 on 21 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 22:16:15 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        60K     61.084     28.714     89.798     418042        492         21       1024    22:17:22.249
    PFlock       60.0        60K     44.853     35.594     80.447     497100        756         21       1024    22:18:42.858
    PFlock       70.0        60K     48.052     29.169     77.221     580400        923         21       1024    22:20:00.174
    PFlock       80.0        60K     49.020     31.843     80.863     668720        984         21       1024    22:21:21.126
    PFlock       90.0        60K     50.339     36.980     87.319     765034       1164         21       1024    22:22:48.538
    PFlock      100.0        60K     54.464     42.085     96.549     871886       1353         21       1024    22:24:25.181
Done!!!
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
starting org.apache.spark.deploy.master.Master, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.master.Master-1-dblab-rack15.out
acald013@dblab-rack12: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack12.out
acald013@dblab-rack15: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack15.out
acald013@dblab-rack14: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack14.cs.ucr.edu.out
acald013@dblab-rack11: starting org.apache.spark.deploy.worker.Worker, logging to /home/acald013/Spark/spark-2.1.0-bin-hadoop2.7/logs/spark-acald013-org.apache.spark.deploy.worker.Worker-1-dblab-rack11.cs.ucr.edu.out
Running in 28 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 22:24:42 INFO SparkContext: Running Spark version 2.1.0
17/09/16 22:24:42 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 22:24:42 INFO SecurityManager: Changing view acls to: acald013
17/09/16 22:24:42 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 22:24:42 INFO SecurityManager: Changing view acls groups to: 
17/09/16 22:24:42 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 22:24:42 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 22:24:43 INFO Utils: Successfully started service 'sparkDriver' on port 44743.
17/09/16 22:24:43 INFO SparkEnv: Registering MapOutputTracker
17/09/16 22:24:43 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 22:24:43 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 22:24:43 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 22:24:43 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a2806772-145a-404f-aede-bda5251adfe9
17/09/16 22:24:43 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 22:24:43 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 22:24:44 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 22:24:44 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 22:24:44 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:44743/jars/pflock_2.11-1.0.jar with timestamp 1505625884104
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 22:24:44 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 47 ms (0 ms spent in bootstraps)
17/09/16 22:24:44 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916222444-0000
17/09/16 22:24:44 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 37013.
17/09/16 22:24:44 INFO NettyBlockTransferService: Server created on 169.235.27.138:37013
17/09/16 22:24:44 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 22:24:44 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 37013, None)
17/09/16 22:24:44 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:37013 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 37013, None)
17/09/16 22:24:44 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 37013, None)
17/09/16 22:24:44 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 37013, None)
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916222444-0000/0 on worker-20170916222432-169.235.27.138-45478 (169.235.27.138:45478) with 7 cores
17/09/16 22:24:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916222444-0000/0 on hostPort 169.235.27.138:45478 with 7 cores, 12.0 GB RAM
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916222444-0000/1 on worker-20170916222431-169.235.27.135-34860 (169.235.27.135:34860) with 7 cores
17/09/16 22:24:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916222444-0000/1 on hostPort 169.235.27.135:34860 with 7 cores, 12.0 GB RAM
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916222444-0000/2 on worker-20170916222436-169.235.27.134-44815 (169.235.27.134:44815) with 7 cores
17/09/16 22:24:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916222444-0000/2 on hostPort 169.235.27.134:44815 with 7 cores, 12.0 GB RAM
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916222444-0000/3 on worker-20170916222431-169.235.27.137-42211 (169.235.27.137:42211) with 7 cores
17/09/16 22:24:44 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916222444-0000/3 on hostPort 169.235.27.137:42211 with 7 cores, 12.0 GB RAM
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916222444-0000/1 is now RUNNING
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916222444-0000/2 is now RUNNING
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916222444-0000/3 is now RUNNING
17/09/16 22:24:44 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916222444-0000/0 is now RUNNING
17/09/16 22:24:45 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916222444-0000
17/09/16 22:24:45 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 22:24:45 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916222444-0000 on 28 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 22:25:10 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        80K     61.298     27.011     88.309     575930        482         28       1024    22:26:14.964
    PFlock       60.0        80K     40.231     27.323     67.554     700950        751         28       1024    22:27:22.677
    PFlock       70.0        80K     46.447     30.112     76.559     833016        904         28       1024    22:28:39.338
    PFlock       80.0        80K     51.499     32.311     83.810     974432       1065         28       1024    22:30:03.239
    PFlock       90.0        80K     48.933     35.785     84.718    1130136       1287         28       1024    22:31:28.048
    PFlock      100.0        80K     56.085     39.096     95.181    1302882       1563         28       1024    22:33:03.321
Done!!!
Running in 28 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 22:33:10 INFO SparkContext: Running Spark version 2.1.0
17/09/16 22:33:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 22:33:10 INFO SecurityManager: Changing view acls to: acald013
17/09/16 22:33:10 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 22:33:10 INFO SecurityManager: Changing view acls groups to: 
17/09/16 22:33:10 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 22:33:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 22:33:11 INFO Utils: Successfully started service 'sparkDriver' on port 43353.
17/09/16 22:33:11 INFO SparkEnv: Registering MapOutputTracker
17/09/16 22:33:11 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 22:33:11 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 22:33:11 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 22:33:11 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-a8940679-7ebc-4843-8000-c682376a9650
17/09/16 22:33:11 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 22:33:11 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 22:33:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 22:33:11 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 22:33:11 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:43353/jars/pflock_2.11-1.0.jar with timestamp 1505626391924
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 22:33:12 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 22:33:12 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916223312-0001
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916223312-0001/0 on worker-20170916222432-169.235.27.138-45478 (169.235.27.138:45478) with 7 cores
17/09/16 22:33:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916223312-0001/0 on hostPort 169.235.27.138:45478 with 7 cores, 12.0 GB RAM
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916223312-0001/1 on worker-20170916222431-169.235.27.135-34860 (169.235.27.135:34860) with 7 cores
17/09/16 22:33:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916223312-0001/1 on hostPort 169.235.27.135:34860 with 7 cores, 12.0 GB RAM
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916223312-0001/2 on worker-20170916222436-169.235.27.134-44815 (169.235.27.134:44815) with 7 cores
17/09/16 22:33:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916223312-0001/2 on hostPort 169.235.27.134:44815 with 7 cores, 12.0 GB RAM
17/09/16 22:33:12 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 42775.
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916223312-0001/3 on worker-20170916222431-169.235.27.137-42211 (169.235.27.137:42211) with 7 cores
17/09/16 22:33:12 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916223312-0001/3 on hostPort 169.235.27.137:42211 with 7 cores, 12.0 GB RAM
17/09/16 22:33:12 INFO NettyBlockTransferService: Server created on 169.235.27.138:42775
17/09/16 22:33:12 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 22:33:12 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 42775, None)
17/09/16 22:33:12 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:42775 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 42775, None)
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916223312-0001/1 is now RUNNING
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916223312-0001/0 is now RUNNING
17/09/16 22:33:12 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 42775, None)
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916223312-0001/2 is now RUNNING
17/09/16 22:33:12 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 42775, None)
17/09/16 22:33:12 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916223312-0001/3 is now RUNNING
17/09/16 22:33:13 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916223312-0001
17/09/16 22:33:13 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 22:33:13 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916223312-0001 on 28 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 22:33:36 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        80K     58.219     27.250     85.469     575930        482         28       1024    22:34:39.893
    PFlock       60.0        80K     41.550     27.334     68.884     700950        751         28       1024    22:35:48.940
    PFlock       70.0        80K     49.199     30.182     79.381     833016        904         28       1024    22:37:08.419
    PFlock       80.0        80K     47.173     32.777     79.950     974432       1065         28       1024    22:38:28.465
    PFlock       90.0        80K     51.569     36.354     87.923    1130136       1287         28       1024    22:39:56.480
    PFlock      100.0        80K     58.921     40.226     99.147    1302882       1563         28       1024    22:41:35.718
17/09/16 22:41:36 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
Done!!!
Running in 28 cores and 1024 partitions.  Setting mu = 50 ...
Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties
17/09/16 22:41:43 INFO SparkContext: Running Spark version 2.1.0
17/09/16 22:41:43 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/09/16 22:41:44 INFO SecurityManager: Changing view acls to: acald013
17/09/16 22:41:44 INFO SecurityManager: Changing modify acls to: acald013
17/09/16 22:41:44 INFO SecurityManager: Changing view acls groups to: 
17/09/16 22:41:44 INFO SecurityManager: Changing modify acls groups to: 
17/09/16 22:41:44 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(acald013); groups with view permissions: Set(); users  with modify permissions: Set(acald013); groups with modify permissions: Set()
17/09/16 22:41:44 INFO Utils: Successfully started service 'sparkDriver' on port 42541.
17/09/16 22:41:44 INFO SparkEnv: Registering MapOutputTracker
17/09/16 22:41:44 INFO SparkEnv: Registering BlockManagerMaster
17/09/16 22:41:44 INFO BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information
17/09/16 22:41:44 INFO BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up
17/09/16 22:41:44 INFO DiskBlockManager: Created local directory at /tmp/blockmgr-8268ce0e-bd38-4464-8a4e-6053c40a7f94
17/09/16 22:41:44 INFO MemoryStore: MemoryStore started with capacity 6.2 GB
17/09/16 22:41:44 INFO SparkEnv: Registering OutputCommitCoordinator
17/09/16 22:41:45 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/09/16 22:41:45 INFO SparkUI: Bound SparkUI to 0.0.0.0, and started at http://169.235.27.138:4040
17/09/16 22:41:45 INFO SparkContext: Added JAR file:/home/acald013/PhD/Y2Q4/PFlock/target/scala-2.11/pflock_2.11-1.0.jar at spark://169.235.27.138:42541/jars/pflock_2.11-1.0.jar with timestamp 1505626905274
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Connecting to master spark://169.235.27.138:7077...
17/09/16 22:41:45 INFO TransportClientFactory: Successfully created connection to /169.235.27.138:7077 after 49 ms (0 ms spent in bootstraps)
17/09/16 22:41:45 INFO StandaloneSchedulerBackend: Connected to Spark cluster with app ID app-20170916224145-0002
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916224145-0002/0 on worker-20170916222432-169.235.27.138-45478 (169.235.27.138:45478) with 7 cores
17/09/16 22:41:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916224145-0002/0 on hostPort 169.235.27.138:45478 with 7 cores, 12.0 GB RAM
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916224145-0002/1 on worker-20170916222431-169.235.27.135-34860 (169.235.27.135:34860) with 7 cores
17/09/16 22:41:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916224145-0002/1 on hostPort 169.235.27.135:34860 with 7 cores, 12.0 GB RAM
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916224145-0002/2 on worker-20170916222436-169.235.27.134-44815 (169.235.27.134:44815) with 7 cores
17/09/16 22:41:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916224145-0002/2 on hostPort 169.235.27.134:44815 with 7 cores, 12.0 GB RAM
17/09/16 22:41:45 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35894.
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor added: app-20170916224145-0002/3 on worker-20170916222431-169.235.27.137-42211 (169.235.27.137:42211) with 7 cores
17/09/16 22:41:45 INFO StandaloneSchedulerBackend: Granted executor ID app-20170916224145-0002/3 on hostPort 169.235.27.137:42211 with 7 cores, 12.0 GB RAM
17/09/16 22:41:45 INFO NettyBlockTransferService: Server created on 169.235.27.138:35894
17/09/16 22:41:45 INFO BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy
17/09/16 22:41:45 INFO BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 169.235.27.138, 35894, None)
17/09/16 22:41:45 INFO BlockManagerMasterEndpoint: Registering block manager 169.235.27.138:35894 with 6.2 GB RAM, BlockManagerId(driver, 169.235.27.138, 35894, None)
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916224145-0002/1 is now RUNNING
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916224145-0002/0 is now RUNNING
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916224145-0002/2 is now RUNNING
17/09/16 22:41:45 INFO BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 169.235.27.138, 35894, None)
17/09/16 22:41:45 INFO StandaloneAppClient$ClientEndpoint: Executor updated: app-20170916224145-0002/3 is now RUNNING
17/09/16 22:41:45 INFO BlockManager: Initialized BlockManager: BlockManagerId(driver, 169.235.27.138, 35894, None)
17/09/16 22:41:46 INFO EventLoggingListener: Logging events to file:///home/acald013/Logs/app-20170916224145-0002
17/09/16 22:41:46 INFO StandaloneSchedulerBackend: SchedulerBackend is ready for scheduling beginning after reached minRegisteredResourcesRatio: 0.0
17/09/16 22:41:46 INFO SharedState: Warehouse path is 'file:/home/acald013/PhD/Y2Q4/Scripts/Scaleup/spark-warehouse/'.
Running app-20170916224145-0002 on 28 cores and 1024 partitions...
       Tag    Epsilon    Dataset      TimeD      TimeM  TotalTime NCandidate   NMaximal      Cores Partitions       Timestamp
17/09/16 22:42:09 ERROR LiveListenerBus: Dropping SparkListenerEvent because no remaining room in event queue. This likely means one of the SparkListeners is too slow and cannot keep up with the rate at which tasks are being started by the scheduler.
    PFlock       50.0        80K     59.203     27.167     86.370     575930        482         28       1024    22:43:14.193
    PFlock       60.0        80K     43.922     27.857     71.779     700950        751         28       1024    22:44:26.132
    PFlock       70.0        80K     43.912     31.273     75.185     833016        904         28       1024    22:45:41.418
    PFlock       80.0        80K     51.514     33.613     85.127     974432       1065         28       1024    22:47:06.635
    PFlock       90.0        80K     51.491     36.145     87.636    1130136       1287         28       1024    22:48:34.362
    PFlock      100.0        80K     55.751     40.241     95.992    1302882       1563         28       1024    22:50:10.447
17/09/16 22:50:12 ERROR LiveListenerBus: SparkListenerBus has already stopped! Dropping event SparkListenerExecutorMetricsUpdate(1,WrappedArray())
Done!!!
acald013@dblab-rack14: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack11: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack15: stopping org.apache.spark.deploy.worker.Worker
acald013@dblab-rack12: stopping org.apache.spark.deploy.worker.Worker
stopping org.apache.spark.deploy.master.Master
Done!!!
