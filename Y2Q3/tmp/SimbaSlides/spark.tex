%% LyX 2.1.4 created this file.  For more info, see http://www.lyx.org/.
%% Do not edit unless you really know what you are doing.
\documentclass{beamer}
\usepackage{minted}
\usepackage{animate}
\usepackage{graphicx}
\def\Put(#1,#2)#3{\leavevmode\makebox(0,0){\put(#1,#2){#3}}}
\usepackage{color}
\usepackage{tikz}
\usepackage{amssymb}

\newcommand\blfootnote[1]{%
  \begingroup
  \renewcommand\thefootnote{}\footnote{#1}%
  \addtocounter{footnote}{-1}%
  \endgroup
}

\definecolor{LightGray}{gray}{0.9}

\ifx\hypersetup\undefined
  \AtBeginDocument{%
    \hypersetup{unicode=true,
 bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},colorlinks=false}
  }
\else
  \hypersetup{unicode=true,
 bookmarksnumbered=false,bookmarksopen=false,
 breaklinks=false,pdfborder={0 0 0},colorlinks=false}
\fi

\makeatletter
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Textclass specific LaTeX commands.
 % this default might be overridden by plain title style
 \newcommand\makebeamertitle{\frame{\maketitle}}%
 % (ERT) argument for the TOC
 \AtBeginDocument{%
   \let\origtableofcontents=\tableofcontents
   \def\tableofcontents{\@ifnextchar[{\origtableofcontents}{\gobbletableofcontents}}
   \def\gobbletableofcontents#1{\origtableofcontents}
 }

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% User specified LaTeX commands.
\usetheme{Warsaw}
% or ...
\useoutertheme{infolines}
\addtobeamertemplate{headline}{}{\vskip2pt}

\setbeamercovered{transparent}
% or whatever (possibly just delete it)



\setbeamertemplate{footline}
{
  \leavevmode%
  \hbox{%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,center]{title in head/foot}%
    \usebeamerfont{title in head/foot}\insertshorttitle
  \end{beamercolorbox}%
  \begin{beamercolorbox}[wd=.5\paperwidth,ht=2.25ex,dp=1ex,right]{date in head/foot}%
    \usebeamerfont{date in head/foot}\insertshortdate{}\hspace*{2em}
    \insertframenumber{} / \inserttotalframenumber\hspace*{2ex} 
  \end{beamercolorbox}}%
  \vskip0pt%
}

\makeatother

\begin{document}

\title[Spark]{A Gentle Introduction to Spark 2.0.}
\subtitle{Based on Madhukara Phatak posts at \url{http://blog.madhukaraphatak.com/categories/spark-two/}.}
\author{Andres Calderon}
%\institute{University of California, Riverside}

\makebeamertitle

% \AtBeginSection[]{
%   \frame<beamer>{ 
%     \frametitle{Agenda}   
%     \tableofcontents[currentsubsection] 
%   }
% }

\newif\iflattersubsect

\AtBeginSection[] {
    \begin{frame}<beamer>
    \frametitle{Outline} %
    \tableofcontents[currentsection]  
    \end{frame}
    \lattersubsectfalse
}

\AtBeginSubsection[] {
    % \iflattersubsect
    \begin{frame}<beamer>
    \frametitle{Outline} %
    \tableofcontents[currentsubsection]  
    \end{frame}
    % \fi
    % \lattersubsecttrue
}

\section{Spark Overview}

\begin{frame}{Overview}
  \begin{itemize}
    \item Apache Spark provides an API centered on a data structure called the resilient distributed dataset (RDD).
	\item RDD: a read-only multiset of data items distributed over a cluster of machines, that is maintained in a fault-tolerant way.
  \end{itemize} 
\end{frame}

\begin{frame}{Overview}
  \begin{itemize}
    \item Response to limitations in the MapReduce cluster computing paradigm, which forces a linear dataflow structure ...
	\item Read from disk $\longrightarrow$ Map across the data $\longrightarrow$ Reduce results $\longrightarrow$ Store to disk..
	\item Spark's RDDs function as a working set for distributed programs that offers a form of distributed shared memory.
  \end{itemize} 
\end{frame}

\begin{frame}{Overview}
	\centering
	\includegraphics[height=0.8\textheight]{./Figures/RDD2.png}
\end{frame}

\begin{frame}{Overview}
	\centering
	\includegraphics[width=\textwidth]{./Figures/RDD1.png}
\end{frame}

\begin{frame}{Spark APIs}
  \begin{itemize}
    \item APIs in different languages:
    \begin{itemize}
    	\item Scala
    	\item Python
    	\item R
    	\item Java
    \end{itemize}
  \end{itemize} 
\end{frame}

\begin{frame}[fragile]{Spark APIs}
  \centering
  \begin{minted}[fontsize=\footnotesize,tabsize=8,breaklines,framesep=10pt,frame=single]{scala}
// create a spark config object
val conf = new SparkConf().setAppName("wiki_test") 
// Create a spark context
val sc = new SparkContext(conf) 
// Read files from "somedir" into an RDD 
// of (filename, content) pairs.
val data = sc.textFile("/path/to/somedir")
// Split each file into a list of tokens (words).
val tokens = data.flatMap(_.split(" ")) 
// Add a count of one to each token, 
// then sum the counts per word type.
val wordFreq = tokens.map((_, 1)).reduceByKey(_ + _) 
// Get the top 10 words. Swap word and count to sort by count.
wordFreq.sortBy(s => -s._2).map(x => (x._2, x._1)).top(10) 
  \end{minted}
\end{frame}

\begin{frame}[fragile]{Spark APIs}
  \centering
  \begin{minted}[fontsize=\footnotesize,tabsize=8,breaklines,framesep=10pt,frame=single]{scala}
import org.apache.spark.sql.SQLContext

// URL for your database server.
val url = "jdbc:mysql://IP:Port/db?user=username;password=passwd" 
// Create a sql context object
val sqlContext = new org.apache.spark.sql.SQLContext(sc)

val df = sqlContext
  .read
  .format("jdbc")
  .option("url", url)
  .option("dbtable", "people")
  .load()

// Looks the schema of this DataFrame.
df.printSchema() 
// Counts people by age
val countsByAge = df.groupBy("age").count() 
  \end{minted}
\end{frame}

\begin{frame}{Spark APIs}
  \begin{itemize}
    \item Other Spark's Frameworks:
    \begin{itemize}
    	\item Spark Streaming
    	\item MLlib Machine Learning Library
    	\item GraphX
    \end{itemize}
  \end{itemize} 
\end{frame}

\section{Spark Session API}

\begin{frame}{Datasets}
  \begin{itemize}
    \item \texttt{Dataset} - the new abstraction of Spark.
    \begin{itemize}
     \item Replace \texttt{RDD} as standard abstraction layer.
     \item \texttt{Dataframe API} becomes its subset.
     \item $[Low Level]$ \texttt{RDD API} $\longrightarrow$ \texttt{Dataframe API} $\longrightarrow$ \texttt{Dataset} $[High Level]$
    \end{itemize}
  \end{itemize} 
\end{frame}

\begin{frame}{SparkSession}
  \begin{itemize}
    \item \texttt{SparkSession} - New entry point of Spark
    \begin{itemize}
     \item Replace \texttt{SparkContext} as standard entry point.
     \item Combine \texttt{SQLContext}, \texttt{HiveContext} and future \texttt{StreamingContext}.
    \end{itemize}
  \end{itemize} 
\end{frame}

\section{Wordcount in Dataset API}
\begin{frame}{Introduction to \texttt{Dataset}}
  \begin{itemize}
    \item A \texttt{Dataset} is a \textbf{strongly typed collection of domain-specific objects} that can be transformed in parallel using functional or relational operations.
    \item Each \texttt{Dataset} also has an untyped view called a \texttt{DataFrame}, which is a \texttt{Dataset} of \texttt{Row}.
  \end{itemize} 
\end{frame}

\begin{frame}{Introduction to \texttt{Dataset}}
  \begin{itemize}
    \item \texttt{RDD} represents an immutable,partitioned collection of elements that can be operated on in parallel
    \item The major difference is, \texttt{Dataset} is collection of domain specific objects where as \texttt{RDD} is collection of any object.
  \end{itemize} 
\end{frame}

\begin{frame}[fragile]{Creating SparkSession}
  \centering
  Demo at \url{https://tinyurl.com/demospark}
\end{frame}

\end{document}
