{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import Row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points = sc.textFile(\"/opt/GISData/Geolife_Trajectories_1.3/beijing.csv\")\\\n",
    ".map(lambda line: line.split(\",\"))\\\n",
    ".map(lambda p: Row(x=float(p[0]), y=float(p[1])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(x=1.044024, y=103.5263),\n",
       " Row(x=1.044028, y=103.524919),\n",
       " Row(x=1.04413, y=103.523515),\n",
       " Row(x=1.044208, y=103.780915),\n",
       " Row(x=1.044237, y=103.522101)]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "points.take(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+----------+\n",
      "|       x|         y|\n",
      "+--------+----------+\n",
      "|1.044024|  103.5263|\n",
      "|1.044028|103.524919|\n",
      "| 1.04413|103.523515|\n",
      "|1.044208|103.780915|\n",
      "|1.044237|103.522101|\n",
      "|1.044306|103.527684|\n",
      "|1.044355|103.520695|\n",
      "| 1.04449|103.519302|\n",
      "|1.044508|103.529049|\n",
      "|1.044685|103.530384|\n",
      "|1.044709|103.517923|\n",
      "| 1.04478|103.779929|\n",
      "|1.044874|103.531761|\n",
      "|1.044995|103.516561|\n",
      "|1.045075|103.533127|\n",
      "|1.045309|103.515218|\n",
      "|1.045314|103.534466|\n",
      "|1.045346|103.778954|\n",
      "|1.045535|103.535827|\n",
      "|1.045645|103.513892|\n",
      "+--------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "points = points.toDF()\n",
    "points.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "points.registerTempTable(\"points\")\n",
    "#points.cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "18015599\n"
     ]
    }
   ],
   "source": [
    "sql = \"SELECT * FROM points WHERE POINT(x, y) IN CIRCLERANGE(POINT(39.9042, 116.4074), 1)\"\n",
    "buffer = sqlContext.sql(sql).cache()\n",
    "print(buffer.count())\n",
    "\n",
    "# 18015599"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#buffer_sample = buffer.sample(False, 0.06, 42)\n",
    "#buffer_sample.toPandas().to_csv('P1M.csv')\n",
    "#print(buffer_sample.count())\n",
    "\n",
    "# 1083011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#buffer_sample = buffer.sample(False, 0.112, 42)\n",
    "#buffer_sample.toPandas().to_csv('P2M.csv')\n",
    "#print(buffer_sample.count())\n",
    "\n",
    "# 2018904"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#buffer_sample = buffer.sample(False, 0.225, 42)\n",
    "#buffer_sample.toPandas().to_csv('P4M.csv')\n",
    "#print(buffer_sample.count())\n",
    "\n",
    "# 4052491"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#buffer_sample = buffer.sample(False, 0.445, 42)\n",
    "#buffer_sample.toPandas().to_csv('P8M.csv')\n",
    "#print(buffer_sample.count())\n",
    "\n",
    "# 8015746"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#buffer_sample = buffer.sample(False, 0.889, 42)\n",
    "#buffer_sample.write.format('com.databricks.spark.csv').save('P16M.csv')\n",
    "#print(buffer_sample.count())\n",
    "\n",
    "# 16015244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0006: 10830\n",
      "0.0011: 20065\n",
      "0.00115: 20967\n",
      "0.0017: 30882\n",
      "0.00225: 40865\n",
      "0.0028: 50838\n",
      "0.00335: 60670\n",
      "0.0039: 70473\n",
      "0.00445: 80379\n",
      "0.005: 90294\n",
      "0.00555: 100149\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for i in np.arange(0.0005,0.00555,0.00005):\n",
    "    buffer_sample = buffer.sample(False, i, 42)\n",
    "    c = buffer_sample.count()\n",
    "    if (c / 1000) % 10 == 0:\n",
    "        print(\"{0}: {1}\".format(i,c))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from pyproj import Proj, transform\n",
    "\n",
    "def tranformCoords(row):\n",
    "    inProj = Proj(init='epsg:4326')\n",
    "    outProj = Proj(init='epsg:4799')\n",
    "    x2,y2 = transform(inProj,outProj, row.y, row.x)\n",
    "    return Row(x=float(x2), y=float(y2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P10K.csv\n",
      "P20K.csv\n",
      "P30K.csv\n",
      "P40K.csv\n",
      "P50K.csv\n",
      "P60K.csv\n",
      "P70K.csv\n",
      "P80K.csv\n",
      "P90K.csv\n",
      "P100K.csv\n"
     ]
    }
   ],
   "source": [
    "for i in [0.0006,0.0011,0.0017,0.00225,0.0028,0.00335,0.0039,0.00445,0.005,0.00555]:\n",
    "    buffer_sample = buffer.sample(False, i, 42)\n",
    "    c = buffer_sample.count()\n",
    "    name = \"P{1}K.csv\".format(i,c/1000)\n",
    "    \n",
    "    # buffer_sample.write.format('com.databricks.spark.csv').save(name)\n",
    "    buffer_sample.map(tranformCoords).toDF().toPandas().to_csv(name, header=None)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PySpark",
   "language": "python",
   "name": "pyspark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
